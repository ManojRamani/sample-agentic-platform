{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Thought Reasoning\n",
    "\n",
    "Chain of Thought (CoT) is a prompting technique that asks language models to show their step-by-step thinking before giving a final answer. This approach:\n",
    "\n",
    "- **Improves accuracy** on complex reasoning tasks\n",
    "- **Provides transparency** into the model's thinking process\n",
    "- **Works best for**: math problems, logic puzzles, and multi-step reasoning\n",
    "\n",
    "However, CoT increases token usage and response time, so it's less suitable for simple tasks or latency-sensitive applications.\n",
    "\n",
    "In this notebook, we'll explore how to implement CoT prompting and see its impact on response quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Chain of Thought\n",
    "\n",
    "Let's start with a simple example to see the difference between regular prompting and chain of thought prompting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "REGION = 'us-west-2'\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "session = boto3.Session()\n",
    "bedrock = session.client(service_name='bedrock-runtime', region_name=REGION)\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Templates\n",
    "\n",
    "The code below defines reusable components for our Chain of Thought examples:\n",
    "- A system prompt that instructs Claude to show its reasoning process\n",
    "- A prompt template for structuring questions\n",
    "- A function to call Claude via Bedrock\n",
    "- A utility to extract content from XML tags using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "# Define a system prompt that encourages Chain of Thought.\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that can solve math word problems. \n",
    "<instruction>\n",
    "You think through problems step by step and provide answers.\n",
    "You place your thinking process in <thinking> tags and your final answer in <response> tags.\n",
    "</instruction>\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Using the context provided, solve the question.\n",
    "\n",
    "<question>{question}</question>\n",
    "\n",
    "Think through the problem step by step and place your reasoning in <thinking> tags. When you have a final answer, place it in <response> tags.\n",
    "\"\"\"\n",
    "\n",
    "MODEL_ID: str = \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "\n",
    "# Helper function to call bedrock\n",
    "def call_bedrock(prompt: str) -> str:\n",
    "\n",
    "    # Create the message in Bedrock's required format\n",
    "    user_message: Dict[str, Any] = { \"role\": \"user\",\"content\": [{ \"text\": prompt}] }\n",
    "\n",
    "    # Configure model parameters\n",
    "    inference_config: Dict[str, Any] = {\n",
    "        \"temperature\": .4,\n",
    "        \"maxTokens\": 1000\n",
    "    }\n",
    "\n",
    "    # Send request to Claude Haiku 3.5 via Bedrock\n",
    "    response: Dict[str, Any] = bedrock.converse(\n",
    "        modelId=MODEL_ID,  # Using Sonnet 3.5 \n",
    "        messages=[user_message],\n",
    "        system=[{\"text\": SYSTEM_PROMPT}],\n",
    "        inferenceConfig=inference_config\n",
    "    )\n",
    "\n",
    "    # Get the model's text response\n",
    "    return response['output']['message']['content'][0]['text']\n",
    "\n",
    "# Define a function to extract content from XML tags\n",
    "def extract_tag_content(text: str, tag_name: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract content between specified XML tags from text.\n",
    "    \"\"\"\n",
    "    pattern = f'<{tag_name}>(.*?)</{tag_name}>'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "# Define the math problem as a variable to make it reusable\n",
    "math_question = \"If John has 5 apples and gives 2 to Mary, who then gives 1 to Tom, how many apples does Mary have?\"\n",
    "\n",
    "# For reusing prompts, it's useful to have a dictionary of inputs.\n",
    "inputs = {\n",
    "    'question': math_question,\n",
    "}\n",
    "\n",
    "# Format the prompt by filling in the variable.\n",
    "prompt: str = PROMPT_TEMPLATE.format(**inputs)\n",
    "\n",
    "# Call the model.\n",
    "model_text: str = call_bedrock(prompt)\n",
    "\n",
    "# Extract the thinking and response sections\n",
    "thinking: Optional[str] = extract_tag_content(model_text, \"thinking\")\n",
    "final_answer: Optional[str] = extract_tag_content(model_text, \"response\")\n",
    "\n",
    "# Print the extracted sections\n",
    "print(\"\\n===== CHAIN OF THOUGHT REASONING =====\")\n",
    "print(thinking if thinking else \"No thinking section found\")\n",
    "\n",
    "print(\"\\n===== FINAL ANSWER =====\")\n",
    "print(final_answer if final_answer else \"No response section found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how in the Chain of Thought example, we get to see the model's reasoning process! This is incredibly valuable when:\n",
    "1. Debugging incorrect responses\n",
    "2. Verifying the model's logic\n",
    "3. Building trust in the model's outputs\n",
    "\n",
    "## More Complex Example\n",
    "\n",
    "Let's try a more complex problem that really shows the power of chain of thought reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_question = \"A store has a 20% off sale. Alice buys a shirt for $40 and pants for $60. The store has a policy that the discount applies to the more expensive item first. What is her total after the discount?\"\n",
    "\n",
    "# For reusing prompts, it's useful to have a dictionary of inputs.\n",
    "inputs = {\n",
    "    'question': math_question,\n",
    "}\n",
    "\n",
    "# Format the prompt by filling in the variable.\n",
    "prompt: str = PROMPT_TEMPLATE.format(**inputs)\n",
    "\n",
    "# Call the model.\n",
    "model_text: str = call_bedrock(prompt)\n",
    "\n",
    "# Extract the thinking and response sections\n",
    "thinking: Optional[str] = extract_tag_content(model_text, \"thinking\")\n",
    "final_answer: Optional[str] = extract_tag_content(model_text, \"response\")\n",
    "\n",
    "# Print the extracted sections\n",
    "print(\"\\n===== CHAIN OF THOUGHT REASONING =====\")\n",
    "print(thinking if thinking else \"No thinking section found\")\n",
    "\n",
    "print(\"\\n===== FINAL ANSWER =====\")\n",
    "print(final_answer if final_answer else \"No response section found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Now it's your turn! Try writing a chain of thought prompt for this problem:\n",
    "\n",
    "\"In a game, players score points based on collecting gems. Red gems are worth 5 points, blue gems are worth 3 points, and green gems are worth 2 points. If a player has 4 red gems, 3 blue gems, and 6 green gems, what is their total score?\"\n",
    "\n",
    "Remember to encourage the model to show its work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
