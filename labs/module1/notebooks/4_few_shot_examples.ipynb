{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Learning with Amazon Bedrock\n",
    "\n",
    "Welcome to our exploration of few-shot learning! In this notebook, we'll learn how to use examples to help our LLM understand exactly what we want. Think of it like teaching by showing examples - it's a powerful way to get more reliable and consistent results.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Let's start with our usual imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "session = boto3.Session()\n",
    "bedrock = session.client(service_name='bedrock-runtime')\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "We'll reuse the same helpers from the COT examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "MODEL_ID: str = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "SYSTEM_PROMPT: str = \"You are a helpful assistant\"\n",
    "\n",
    "# Helper function to call bedrock\n",
    "def call_bedrock(prompt: str) -> str:\n",
    "\n",
    "    # Create the message in Bedrock's required format\n",
    "    user_message: Dict[str, Any] = { \"role\": \"user\",\"content\": [{ \"text\": prompt}] }\n",
    "\n",
    "    # Configure model parameters\n",
    "    inference_config: Dict[str, Any] = {\n",
    "        \"temperature\": .4,\n",
    "        \"maxTokens\": 1000\n",
    "    }\n",
    "\n",
    "    # Send request to Claude Haiku 3.5 via Bedrock\n",
    "    response: Dict[str, Any] = bedrock.converse(\n",
    "        modelId=MODEL_ID,  # Using Sonnet 3.5 \n",
    "        messages=[user_message],\n",
    "        system=[{\"text\": SYSTEM_PROMPT}],\n",
    "        inferenceConfig=inference_config\n",
    "    )\n",
    "\n",
    "    # Get the model's text response\n",
    "    return response['output']['message']['content'][0]['text']\n",
    "\n",
    "# Define a function to extract content from XML tags\n",
    "def extract_tag_content(text: str, tag_name: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract content between specified XML tags from text.\n",
    "    \"\"\"\n",
    "    pattern = f'<{tag_name}>(.*?)</{tag_name}>'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Few-Shot Example\n",
    "\n",
    "Let's start by comparing zero-shot (no examples) vs few-shot (with examples) prompting. We'll try to get the model to generate product descriptions in a specific format:\n",
    "\n",
    "```\n",
    "Product: <name>\n",
    "Key Features: <numbered list of features>\n",
    "Ideal For: <what it's ideal for>\n",
    "Price Range: <price range>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt text using XML tags for better Claude interaction\n",
    "ZERO_SHOT_PROMPT = \"\"\"\n",
    "Generate a product description for a coffee maker. Describe the product name, key features, ideal for, and price range.\n",
    "\"\"\"\n",
    "\n",
    "# Send request to Claude\n",
    "response: str = call_bedrock(ZERO_SHOT_PROMPT)\n",
    "\n",
    "# Print the response\n",
    "print(\"Zero-shot response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "The results are good, but not quite what we were looking for format wise. It's also a bit verbose. Lets provide some examples of what we're looking for to improve the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot prompt with examples\n",
    "FEW_SHOT_PROMPT = \"\"\"\n",
    "Here are some examples of product descriptions in our preferred format:\n",
    "\n",
    "<examples>\n",
    "<response>\n",
    "Product: Blender\n",
    "Key Features:\n",
    "1. 1000W motor\n",
    "2. 6 speed settings\n",
    "3. 64 oz capacity\n",
    "Ideal For: Smoothies, soups, sauces\n",
    "Price Range: $$$\n",
    "</response>\n",
    "\n",
    "<response>\n",
    "Product: Toaster\n",
    "Key Features:\n",
    "1. 4 slice capacity\n",
    "2. Digital controls\n",
    "3. 7 browning levels\n",
    "Ideal For: Breakfast, quick meals\n",
    "Price Range: $$\n",
    "</response>\n",
    "</examples>\n",
    "\n",
    "Now, generate a product description for a coffee maker in the same format and place your response between the <response> tags.\"\n",
    "\"\"\"\n",
    "\n",
    "model_response: str = call_bedrock(FEW_SHOT_PROMPT)\n",
    "\n",
    "response: str = extract_tag_content(model_response, \"response\")\n",
    "\n",
    "print(\"Few-shot response:\\n\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the few-shot example produced a much more structured and consistent format! This is because we showed the model exactly what we wanted through examples. The models are also pretty chatty, so it's still useful to get them to respond in response tags that we can regex out. \n",
    "\n",
    "## More Complex Few-Shot Learning\n",
    "\n",
    "Now let's try something more complex - teaching the model to classify customer feedback into specific categories and in a specific format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIZATION_FEW_SHOT_PROMPT_TEMPLATE = \"\"\"\n",
    "Here are some examples of categorized customer feedback in the preferred response format:\n",
    "\n",
    "<examples>\n",
    "Feedback: The website kept crashing while I tried to check out\n",
    "Category: Technical Issue\n",
    "Priority: High\n",
    "\n",
    "Feedback: I love the new design, it's so much easier to find things\n",
    "Category: UI/UX\n",
    "Priority: Low\n",
    "\n",
    "Feedback: My order arrived damaged and customer service hasn't responded in 3 days\n",
    "Category: Customer Service\n",
    "Priority: High\n",
    "</examples>\n",
    "\n",
    "Now, classify the following customer feedback into a category and priority:\n",
    "<feedback>\n",
    "{feedback}\n",
    "</feedback>\n",
    "\n",
    "When responding, place your response between the <response> tags in that format.\n",
    "\"\"\"\n",
    "\n",
    "# Create inputs for our template\n",
    "inputs: Dict[str, str] = { \"feedback\": \"I've been waiting for a refund for two weeks now and can't get anyone to help me.\" }\n",
    "\n",
    "# Format the prompt with our inputs\n",
    "prompt: str = CATEGORIZATION_FEW_SHOT_PROMPT_TEMPLATE.format(**inputs)\n",
    "\n",
    "model_response: str = call_bedrock(prompt)\n",
    "\n",
    "response: str = extract_tag_content(model_response, \"response\")\n",
    "\n",
    "print(\"Few-shot response:\\n\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Effective Few-Shot Learning\n",
    "\n",
    "1. Use more than 1 example for best results. You start seeing diminishing returns (and increased costs after 10)\n",
    "2. Make your examples diverse but representative\n",
    "3. Keep the format consistent across examples\n",
    "5. Use realistic examples that match your use case\n",
    "\n",
    "## Advanced\n",
    "The examples don't have to be static. You can dynamically pass in the most relevant examples into the prompt through prompt variables.\n",
    "\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Now it's your turn! Try writing a few-shot prompt to teach the model to generate recipe instructions in this format:\n",
    "\n",
    "```\n",
    "Recipe: [name]\n",
    "Prep Time: [time]\n",
    "Difficulty: [easy/medium/hard]\n",
    "Steps:\n",
    "1. [step]\n",
    "2. [step]\n",
    "...\n",
    "```\n",
    "\n",
    "Create 2-3 example recipes, then ask for a new recipe. How well does the model follow your format?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
