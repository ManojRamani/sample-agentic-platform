{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) with Amazon Bedrock\n",
    "\n",
    "Retrieval Augmented Generation (RAG) combines the power of large language models with information retrieval. Instead of relying solely on a model's internal knowledge, RAG fetches relevant information from external sources and uses it to generate more accurate, up-to-date responses.\n",
    "\n",
    "RAG doesn't have to include a vectorDB, it can pull from various sources. However, for unstructured text, vectorDBs and semantic search is the most common approach. \n",
    "\n",
    "In this lab, we'll implement RAG using a local in-memory vector database called ChromaDB and use Llama index for taking unstructured text and ingesting it into our vectorDB. \n",
    "\n",
    "#### About ChromaDB\n",
    "\n",
    "ChromaDB is a lightweight, in-memory vector database that makes it easy to store and query embeddings. We'll use it to: (1) Store document embeddings, (2) Perform semantic searches, (3) Retrieve relevant context for our LLM. We will discuss VectorDBs further down the lab.\n",
    "\n",
    "**Will Local DBs Scale?** Running ChromaDB locally is perfect for learning and testing: For production applications, consider using: Amazon Bedrock Knowledge Bases, Pinecone, other managed vector database services. These solutions offer persistence, scalability, and performance features necessary for real-world applications.\n",
    "\n",
    "#### About Llama Index\n",
    "For chunking, we'll use LlamaIndex. There are many tools/frameworks for ingesting documents and implementing chunking strategies. We chose LlamaIndex because it offers a lot of advanced chunking options. It creates \"nodes\" that can be converted into different formats for ingestion.\n",
    "\n",
    "When using these GenAI frameworks, you want to abstract the details of the framework away from your core code. This makes mixing and matching much easier. In the example below, we'll use LlamaIndex but we'll wrap the chunking logic in a class and normalize the output to a class that we create named RAGChunk. This way, we aren't too reliant on the framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's stand up our local Chroma DB and initialize Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import boto3\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize Chroma client from our persisted store\n",
    "chroma_client = chromadb.PersistentClient(path=\"../../data/chroma\")\n",
    "\n",
    "session = boto3.Session()\n",
    "bedrock = session.client(service_name='bedrock-runtime')\n",
    "\n",
    "print(\"✅ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting text\n",
    "To make text easier to search, we want to split it up so we're only retrieving the most relevant information. To do this, we need to create a RAG pipeline\n",
    "\n",
    "#### Setup RAG Pipeline\n",
    "We'll wrap our LlamaIndex and Chroma code in our own class definitions to abstract away the framework from our core logic. That way we can easily change our mind and pick a different DB, framework, etc.. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import BaseNode\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "import uuid\n",
    "\n",
    "# Create a class to use instead of LlamaIndex Nodes. This way we decouple our chroma collections from LlamaIndexes\n",
    "class RAGChunk(BaseModel):\n",
    "    id_: str\n",
    "    text: str\n",
    "    metadata: Dict[str, Any] = {}\n",
    "\n",
    "\n",
    "# This is a simple chunker that uses LlamaIndex's SentenceSplitter to chunk raw text\n",
    "# It can easily be extended to support files or other data sources.\n",
    "class TextChunker:\n",
    "    def __init__(self, chunk_size: int = 256, chunk_overlap: int = 20):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    \n",
    "    def chunk_text(self, text: str, metadata: Dict = None) -> List[RAGChunk]:\n",
    "        \"\"\"Chunk raw text directly\"\"\"\n",
    "        metadata = metadata if metadata else {'source': 'raw_text'}\n",
    "        # Create a document from the text\n",
    "        document: Document = Document(text=text, metadata=metadata)\n",
    "        # Split into chunks\n",
    "        nodes: List[BaseNode] = self.splitter.get_nodes_from_documents([document])\n",
    "        # Create a unique id for the chunk\n",
    "        unique_id: str = str(uuid.uuid4())\n",
    "        \n",
    "        # Convert to RAGChunk objects\n",
    "        chunks: List[RAGChunk] = []\n",
    "\n",
    "        for i, node in enumerate(nodes):\n",
    "            rag_chunk: RAGChunk = RAGChunk(\n",
    "                id_=node.node_id or f\"text_chunk_{i}_{unique_id}\",\n",
    "                text=node.text,\n",
    "                metadata={ **node.metadata, **metadata }\n",
    "            )\n",
    "\n",
    "            chunks.append(rag_chunk)\n",
    "        \n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup VectorDB Wrapper\n",
    "Next we'll create a wrapper around chromaDB and our retrieval results so that we can easily swap out different DBs later. To understand what's happening, we need to introduce a couple concepts\n",
    "\n",
    "##### Embeddings\n",
    "Embeddings are essentially a numerical representation of a chunk of texts meaning. Imagine you have a 2D space with (X,Y) coordinates. A question like \"What is espresso?\" might be represented in that graph as X,Y coordinate of (1,1). \n",
    "\n",
    "You also have two chunks of text, one relevant one not:\n",
    "1. \"concentrated coffee beverage made by forcing hot water under high pressure through ground coffee beans\"\n",
    "2. \"Opensearch is an opensource database that can perform semantic search\"\n",
    "\n",
    "Chunk text one might be represented on that 2D space as (1.5, 1.5) while chunk two might be represented as (4,4). Using these embeddings, we can find semantically similar chunks of information by finding the nearest neighbors.\n",
    "\n",
    "Find a diagram below to illustrate the concept:\n",
    "\n",
    "<img src=\"../assets/semanticsearch.png\" width=\"50%\" height=\"auto\">\n",
    "\n",
    "##### Embedding Models\n",
    "To get these embeddings, we use an embedding model. It consumes information (in our case text) and outputs the embedding (vector). In the example above, 2 dimensions (X,Y) is not enough to capture enough information to do anything useful. These embedding models usually output vectors in 512 dimensions or above and are commonly in the 1024+ range. \n",
    "\n",
    "**Note**: embeddings are often called vectors because they have magnitude and direction from the origin. In our 2D example, that means those embeddings have direction and magnitude from (0,0)\n",
    "\n",
    "##### VectorDBs\n",
    "VectorDBs work by finding the closest neighbors to your input. If we expanded out that example above to 100k+ documents, finding the nearest neighbors would be difficult. VectorDBs are optimized for finding nearest neighbors of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any, Optional\n",
    "from chromadb.api.types import EmbeddingFunction\n",
    "\n",
    "# Create abstraction around ChromaDB so that we can easily swap out the DB or embedding model\n",
    "class RetrievalResult(BaseModel):\n",
    "    id: str\n",
    "    document: str\n",
    "    embedding: List[float]\n",
    "    distance: float\n",
    "    metadata: Dict = {}\n",
    "\n",
    "class ChromaDBWrapperClient:\n",
    "    def __init__(self, chroma_client, collection_name: str, embedding_function: Optional[EmbeddingFunction] = None):\n",
    "        self.client = chroma_client\n",
    "        self.collection_name = collection_name\n",
    "        self.embedding_function = embedding_function\n",
    "        \n",
    "        # Create or get the collection\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=embedding_function\n",
    "        )\n",
    "\n",
    "    def add_chunks_to_collection(self, chunks: List[RAGChunk]):\n",
    "        # Add the chunks to the collection\n",
    "        self.collection.add(\n",
    "            ids=[chunk.id_ for chunk in chunks],\n",
    "            documents=[chunk.text for chunk in chunks],\n",
    "            metadatas=[chunk.metadata for chunk in chunks]\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Added {len(chunks)} chunks to collection {self.collection_name}\")\n",
    "        \n",
    "    def retrieve(self, query_text: str, n_results: int = 1) -> List[RetrievalResult]:\n",
    "        # Query the collection\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=n_results,\n",
    "            include=['embeddings', 'documents', 'metadatas', 'distances']\n",
    "        )\n",
    "\n",
    "        # Transform the results into RetrievalResult objects\n",
    "        retrieval_results = []\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            retrieval_results.append(RetrievalResult(\n",
    "                id=results['ids'][0][i],\n",
    "                document=results['documents'][0][i],\n",
    "                embedding=results['embeddings'][0][i],\n",
    "                distance=results['distances'][0][i],\n",
    "                metadata=results['metadatas'][0][i] if results['metadatas'][0] else {}\n",
    "            ))\n",
    "\n",
    "        return retrieval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Complete!\n",
    "We've set up everything we need to do RAG with a vectorDB. If you notice in the code above, we add an embedding model to the collection (think table) directly. It's valuable to tie the embedding model to the same place where you're doing your search. Different embedding models have different \"embedding spaces\" so they're not interchangable. Using Cohere models to create embeddings for your query and then doing a search on embeddings created with Amazon's Titan embedding model won't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Knowledge\n",
    "Next lets create some chunks and populate it in our vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COFFEE_KNOWLEDGE = \"\"\"\n",
    "Espresso is a concentrated form of coffee served in small, strong shots. It is made by forcing hot water under pressure through finely-ground coffee beans. Espresso forms the base for many coffee drinks.\n",
    "\n",
    "Cappuccino is an espresso-based drink that's traditionally prepared with steamed milk, and milk foam. A traditional Italian cappuccino is generally a single shot of espresso topped with equal parts steamed milk and milk foam.\n",
    "\n",
    "Latte is a coffee drink made with espresso and steamed milk. The word comes from the Italian 'caffè e latte' meaning 'coffee and milk'. A typical latte is made with one or two shots of espresso, steamed milk and a small layer of milk foam on top.\n",
    "\n",
    "Cold Brew is coffee made by steeping coarse coffee grounds in cold water for 12-24 hours. This method creates a smooth, less acidic taste compared to hot brewed coffee. Cold brew can be served over ice or heated up.\n",
    "\"\"\"\n",
    "\n",
    "# Create our chunker. We're making the chunk size extremely small here to demonstrate the point.\n",
    "text_chunker: TextChunker = TextChunker(chunk_size=128, chunk_overlap=5)\n",
    "\n",
    "# Create our rag chunks\n",
    "rag_chunks: List[RAGChunk] = text_chunker.chunk_text(COFFEE_KNOWLEDGE)\n",
    "\n",
    "# checkout our chunks\n",
    "print(f'We just created {len(rag_chunks)} chunks!\\n')\n",
    "\n",
    "for i, chunk in enumerate(rag_chunks):\n",
    "    print(f'Chunk {i}:\\n {chunk.text}\\n\\n')\n",
    "\n",
    "print(\"✅ Knowledge created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool! We just created 2 chunks that we can use for our knowledge base to lookup knowledge. \n",
    "\n",
    "Now we need to plug it into our vector DB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils.embedding_functions import AmazonBedrockEmbeddingFunction\n",
    "\n",
    "# Define our embedding model. In our case, we're using Titan Embed Text v2 embedding model from Bedrock\n",
    "TITAN_TEXT_EMBED_V2_ID: str = 'amazon.titan-embed-text-v2:0'\n",
    "\n",
    "# This is a handy function Chroma implemented for calling bedrock. Lets use it!\n",
    "# You need the session object to call bedrock and the model name to use.\n",
    "embedding_function: AmazonBedrockEmbeddingFunction = AmazonBedrockEmbeddingFunction(\n",
    "    session=session,\n",
    "    model_name=TITAN_TEXT_EMBED_V2_ID\n",
    ")\n",
    "\n",
    "# Initialize our vector store. We'll use the same client we created earlier.\n",
    "coffee_vector_store = ChromaDBWrapperClient(chroma_client, \"coffee_knowledge\", embedding_function)\n",
    "\n",
    "# Add our chunks to the vector store\n",
    "coffee_vector_store.add_chunks_to_collection(rag_chunks)\n",
    "\n",
    "print(\"✅ RAG retrieval ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the System\n",
    "To test the system, we just need to retrieve chunks from our vectorDB and use it as context when calling Bedrock. Let's create a RAG helper function and create a simple RAG prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our system prompt. This is the prompt that will be used to generate the response.\n",
    "SYSTEM_PROMPT: str = \"\"\"\n",
    "You are a coffee expert. You are given a question and a context. \n",
    "Your job is to answer the question based ONLY on the context provided. \n",
    "Just answer the question, avoid saying \"Based on the context provided\" before answering.\n",
    "If the context doesn't contain the answer, say \"I don't know\"\n",
    "\"\"\"\n",
    "\n",
    "# Define our RAG prompt template. This is the prompt that will be used to generate the response.\n",
    "RAG_PROMPT_TEMPLATE: str = \"\"\"\n",
    "Using the context below, answer the question.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Remember, if the context doesn't contain the answer, say \"I don't know\".\n",
    "\"\"\"\n",
    "\n",
    "MODEL_ID: str = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "\n",
    "def call_bedrock(prompt: str) -> str:\n",
    "    # Create the message in Bedrock's required format\n",
    "    user_message: Dict[str, Any] = { \"role\": \"user\",\"content\": [{ \"text\": prompt}] }\n",
    "    # Configure model parameters\n",
    "    inference_config: Dict[str, Any] = {\n",
    "        \"temperature\": .4,\n",
    "        \"maxTokens\": 1000\n",
    "    }\n",
    "\n",
    "    # Send request to Claude Haiku 3.5 via Bedrock\n",
    "    response: Dict[str, Any] = bedrock.converse(\n",
    "        modelId=MODEL_ID,  # Using Sonnet 3.5 \n",
    "        messages=[user_message],\n",
    "        system=[{\"text\": SYSTEM_PROMPT}],\n",
    "        inferenceConfig=inference_config\n",
    "    )\n",
    "\n",
    "    # Get the model's text response\n",
    "    return response['output']['message']['content'][0]['text']\n",
    "\n",
    "# Helper function to call bedrock\n",
    "def do_rag(input_question: str) -> str:\n",
    "    # Retrieve the context from the vector store\n",
    "    retrieval_results: List[RetrievalResult] = coffee_vector_store.retrieve(input_question)\n",
    "    # Format the context into a string\n",
    "    context: str = \"\\n\\n\".join([result.document for result in retrieval_results])\n",
    "    # Create the RAG prompt\n",
    "    rag_prompt: str = RAG_PROMPT_TEMPLATE.format(question=input_question, context=context)\n",
    "    # Call Bedrock with the RAG prompt\n",
    "    return call_bedrock(rag_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets ask questions using our knoweldge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions: List[str] = [\n",
    "    \"How is cold brew coffee made?\",\n",
    "    \"What is pour over coffee?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    # Retrieve the context from \n",
    "    response = do_rag(question)\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We were able to answer the first question. However, information about pour over coffee doesn't exist in our vectorDB so the model says it doesn't know. Let's add some knowledge to the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding New Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_knowledge = \"\"\"\n",
    "Pour Over is a method of brewing coffee where hot water is poured over ground coffee in a filter. This method gives the brewer complete control over brewing time and water temperature, leading to a clean, flavorful cup of coffee.\n",
    "\"\"\"\n",
    "\n",
    "# Reuse the same text splitter and collection wrapper to add new knowledge\n",
    "new_texts: List[RAGChunk] = text_chunker.chunk_text(new_knowledge)\n",
    "coffee_vector_store.add_chunks_to_collection(new_texts)\n",
    "\n",
    "# Test new knowledge\n",
    "question: str = \"How is pour over coffee made?\"\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "response = do_rag(question)\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Now it's your turn! Try creating a knowledge base about a topic you're interested in:\n",
    "\n",
    "1. Write 3-4 paragraphs about your chosen topic\n",
    "2. Process the text using the text splitter\n",
    "3. Add data user the chroma wrapper.\n",
    "4. Test with different questions\n",
    "\n",
    "Remember: The quality of the answers depends on the quality and relevance of the information in your knowledge base!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
