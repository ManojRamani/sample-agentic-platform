{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Evaluation is a crucial part of building effective AI applications! Without it, we can't understand if our models, prompts, and systems are working as intended. Evaluation is actually a huge topic with many approaches and methodologies - we have separate labs dedicated to exploring this in depth which can be found below: \n",
    "* [Evaluation blog](https://community.aws/content/2dHLkZmP7i7m0HjxinUoQPWlyyt/evaluating-llm-systems-best-practices)\n",
    "* [Hands on evaluation lab](https://github.com/aws-samples/genai-system-evaluation/tree/main)\n",
    "\n",
    "\n",
    "For this lab, we'll focus on just one exciting technique: LLM-As-A-Judge\n",
    "This approach uses another LLM with a evaluation rubric to evaluate the results of another model. It lets us automatically assess the quality of model responses based on criteria we define, providing quick feedback without always needing human reviewers.\n",
    "\n",
    "In the lab, you will:\n",
    "* Create a grading rubric for your evaluation\n",
    "* Modify an evaluation dataset\n",
    "* Run eval on prompts and try to improve them. \n",
    "\n",
    "\n",
    "Let's dive in and see how we can use this technique to improve our AI systems!\n",
    "\n",
    "## Setup\n",
    "\n",
    "Let's start with our usual imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "REGION = 'us-west-2'\n",
    "session = boto3.Session()\n",
    "bedrock = session.client(service_name='bedrock-runtime', region_name=REGION)\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an LLM Judge\n",
    "\n",
    "We're going to evaluate our RAG prompt from the previous workshop to see if it's giving answers we like. We'll run an eval dataset through Bedrock and capture the models response. We'll then run the responses through our evaluation prompt and get metrics we can use to evaluate how well the prompt is performing. \n",
    "\n",
    "First, lets start with our LLM-As-A-Judge evaluation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_SYSTEM_PROMPT = \"\"\"\n",
    "You are a fair but discerning evaluator who maintains high standards. \n",
    "You use a decimal scoring system (with one decimal place only, e.g., 3.7 or 4.2) to provide nuanced assessments.\n",
    "Your goal is to differentiate between varying levels of quality using the full range of the scale.\n",
    "\"\"\"\n",
    "\n",
    "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
    "Please evaluate the following response using a decimal scoring system.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<generated_response>\n",
    "{generated_response}\n",
    "</generated_response>\n",
    "\n",
    "<gold_standard>\n",
    "{gold_standard}\n",
    "</gold_standard>\n",
    "\n",
    "Evaluation Criteria:\n",
    "Each criterion is scored on a scale from 0.0 to 1.0 using ONE decimal place only (e.g., 0.7, not 0.75):\n",
    "\n",
    "1. Context Utilization (0.0-1.0): How well does the response use the information and follow constraints in the context?\n",
    "   - Compare how effectively the response addresses the key elements in the context\n",
    "   - Consider how well it follows any specific requirements mentioned\n",
    "\n",
    "2. Completeness (0.0-1.0): How thoroughly does the response address all aspects of the question?\n",
    "   - Identify important elements that are covered or missing compared to the gold standard\n",
    "   - Consider the depth and breadth of coverage\n",
    "\n",
    "3. Conciseness (0.0-1.0): Is the response appropriately concise without unnecessary information?\n",
    "   - Evaluate the balance between thoroughness and brevity\n",
    "   - Consider whether all included information is relevant and necessary\n",
    "\n",
    "4. Accuracy (0.0-1.0): How factually correct is the information in the response?\n",
    "   - Check for any factual errors, misconceptions, or imprecisions\n",
    "   - Consider the technical correctness relative to the gold standard\n",
    "\n",
    "5. Clarity (0.0-1.0): How clear, well-organized, and easy to understand is the response?\n",
    "   - Evaluate structure, flow, and accessibility for the intended audience\n",
    "   - Consider the quality of explanations and examples\n",
    "\n",
    "Scoring Guidelines:\n",
    "- Use ONE decimal place only (e.g., 0.7, not 0.75 or 0.67)\n",
    "- Use this scale as a general guideline:\n",
    "  * 0.0: Completely fails to meet expectations\n",
    "  * 0.3: Significantly below expectations\n",
    "  * 0.5: Partially meets expectations\n",
    "  * 0.7: Mostly meets expectations\n",
    "  * 0.9: Exceeds expectations\n",
    "  * 1.0: Outstanding, on par with gold standard\n",
    "- Consider scores between these values (0.2, 0.4, 0.6, 0.8) for more nuanced evaluation\n",
    "- Compare directly with the gold standard to calibrate your scores\n",
    "- Don't hesitate to use lower scores when warranted, or higher scores when deserved\n",
    "\n",
    "Evaluation Process:\n",
    "1. For each criterion, analyze both strengths and weaknesses\n",
    "2. Be specific in your analysis, referencing the text\n",
    "3. Assign a decimal score with ONE decimal place for each criterion\n",
    "4. Calculate the total by adding all five scores (result will be between 0.0-5.0)\n",
    "\n",
    "Provide your detailed analysis inside <thinking></thinking> tags.\n",
    "Then give the final score (with one decimal place, e.g., 3.7) inside <score></score> tags.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bedrock Helpers\n",
    "Let's copy paste our helper Bedrock function from the previous workshops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Optional\n",
    "import re\n",
    "\n",
    "HAIKU_MODEL_ID: str = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "SONNET_MODEL_ID: str = \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "def call_bedrock(prompt: str, system_prompt: str, model_id: str = HAIKU_MODEL_ID) -> str:\n",
    "    # Create the message in Bedrock's required format\n",
    "    user_message: Dict[str, Any] = { \"role\": \"user\",\"content\": [{ \"text\": prompt}] }\n",
    "    # Configure model parameters\n",
    "    inference_config: Dict[str, Any] = {\n",
    "        \"temperature\": .4,\n",
    "        \"maxTokens\": 1000\n",
    "    }\n",
    "\n",
    "    # Send request to Claude Haiku 3.5 via Bedrock\n",
    "    response: Dict[str, Any] = bedrock.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[user_message],\n",
    "        system=[{\"text\": system_prompt}],\n",
    "        inferenceConfig=inference_config\n",
    "    )\n",
    "\n",
    "    # Get the model's text response\n",
    "    return response['output']['message']['content'][0]['text']\n",
    "\n",
    "# Define a function to extract content from XML tags\n",
    "def extract_tag_content(text: str, tag_name: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract content between specified XML tags from text.\n",
    "    \"\"\"\n",
    "    pattern = f'<{tag_name}>(.*?)</{tag_name}>'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Do We Trust The Judge? \n",
    "A common question with LLM-As-A-Judge is \"how do we trust it?\". To answer that, we need a solution to evaluate the evaluator. This is where rubric alignment comes into play. We'll hand grade a subset of answers based on the same grading rubric the model will use. Using those human curated grades, we can run the same samples through our LLM-As-A-Judge prompt and see how closely they match. \n",
    "\n",
    "Lets get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation dataset\n",
    "rubric_alignment_df: pd.DataFrame = pd.read_csv('../data/eval-datasets/rubric_alignment.csv')\n",
    "\n",
    "rubric_alignment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets create our test runner to validate whether or judge prompt aligns to our human evaluators expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from typing import Dict, Tuple, Any\n",
    "\n",
    "def process_row(row_data: Tuple[int, pd.Series]) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single row for evaluation in parallel.\"\"\"\n",
    "    index, row = row_data\n",
    "    prompt = row['query_text']\n",
    "    context = row['context']\n",
    "    llm_response = row['llm_response']\n",
    "    \n",
    "    # Get the evaluation prompt response\n",
    "    evaluation_prompt: str = EVALUATION_PROMPT_TEMPLATE.format(\n",
    "        question=prompt,\n",
    "        generated_response=llm_response,\n",
    "        gold_standard=llm_response,\n",
    "        context=context\n",
    "    )\n",
    "\n",
    "    # Get the evaluation response. Note we're using the Sonnet model here.\n",
    "    evaluation_response: str = call_bedrock(evaluation_prompt, EVALUATION_SYSTEM_PROMPT)\n",
    "\n",
    "    thinking: str = extract_tag_content(evaluation_response, 'thinking')\n",
    "    score: str = extract_tag_content(evaluation_response, 'score')\n",
    "    \n",
    "    return {\n",
    "        'index': index,\n",
    "        'thinking': thinking,\n",
    "        'score': score\n",
    "    }\n",
    "\n",
    "def align_llm_judge_prompt_parallel(df: pd.DataFrame, max_workers: int = 4) -> pd.DataFrame:\n",
    "    \"\"\"Parallelize the evaluation process using ThreadPoolExecutor.\"\"\"\n",
    "    # make a copy of the dataframe so we don't destroy the existing one between eval runs\n",
    "    judge_alignment_df = df.copy(deep=True)\n",
    "    \n",
    "    # Create a list of row data to process\n",
    "    row_data = list(judge_alignment_df.iterrows())\n",
    "    \n",
    "    # Process rows in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = list(executor.map(process_row, row_data))\n",
    "    \n",
    "    # Update the dataframe with results\n",
    "    for result in results:\n",
    "        index = result['index']\n",
    "        judge_alignment_df.loc[index, 'ai_reasoning'] = result['thinking']\n",
    "        judge_alignment_df.loc[index, 'ai_grade'] = result['score']\n",
    "    \n",
    "    return judge_alignment_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_alignment_df = align_llm_judge_prompt_parallel(rubric_alignment_df)\n",
    "\n",
    "judge_alignment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have our answers. However, checking these 1 by 1 would be frustrating at scale. We can do better. Lets create another helper to compare grades and get a more wholistic view of the performance. To do this, we'll use some more standard ML metrics to check how well this aligns.\n",
    "\n",
    "**Note:** Alignment is hard. What we're looking for is whether the prompt more or less aligns with our expectations and there's no big outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "class RubricValidator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.human_scores = self.convert_to_float(df['human_grade'])\n",
    "        self.llm_scores = self.convert_to_float(df['ai_grade'])\n",
    "\n",
    "    def convert_to_float(self, series):\n",
    "        return series.astype(float)\n",
    "\n",
    "    def calculate_metrics(self):\n",
    "        mse = mean_squared_error(self.human_scores, self.llm_scores)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(self.human_scores, self.llm_scores)\n",
    "        r2 = r2_score(self.human_scores, self.llm_scores)\n",
    "        \n",
    "        exact_match = np.mean(self.human_scores == self.llm_scores)\n",
    "        within_1_point = np.mean(np.abs(self.human_scores - self.llm_scores) <= 1)\n",
    "\n",
    "        return {\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R-squared': r2,\n",
    "            'Exact Match Ratio': exact_match,\n",
    "            'Within 1 Point Ratio': within_1_point\n",
    "        }\n",
    "\n",
    "    def generate_report(self):\n",
    "        metrics = self.calculate_metrics()\n",
    "        report = \"LLM Rubric Validation Report\\n\"\n",
    "        report += \"===========================\\n\\n\"\n",
    "        \n",
    "        for metric, value in metrics.items():\n",
    "            if isinstance(value, str):\n",
    "                report += f\"{metric}: {value}\\n\"\n",
    "            else:\n",
    "                report += f\"{metric}: {value:.4f}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "    def analyze_discrepancies(self):\n",
    "        discrepancies = self.df[self.human_scores != self.llm_scores]\n",
    "        return discrepancies[['query_text', 'human_grade', 'ai_grade', 'human_reasoning', 'ai_reasoning']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the evaluator. \n",
    "validator = RubricValidator(judge_alignment_df)\n",
    "print(validator.generate_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "It looks like our LLM judge prompt roughly aligns with our expectations. It's okay if it's not exact. The important part is we believe we can rely on it.\n",
    "\n",
    "Here are examples of good results:\n",
    "\n",
    "**Mean Squared Error (MSE) & Root Mean Squared Error (RMSE):**\n",
    "\n",
    "These metrics measure the average squared difference between the LLM and human grades. They penalize larger errors more heavily, giving us insight into the magnitude of discrepancies.\n",
    "\n",
    "Interpretation: Lower is better. For a 5-point scale:\n",
    "* Excellent: MSE < 0.25 (RMSE < 0.5)\n",
    "* Good: 0.25 â‰¤ MSE < 1 (0.5 â‰¤ RMSE < 1)\n",
    "* Fair: 1 â‰¤ MSE < 2.25 (1 â‰¤ RMSE < 1.5)\n",
    "* Poor: MSE â‰¥ 2.25 (RMSE â‰¥ 1.5)\n",
    "\n",
    "**Within one point**\n",
    "This is very important. If we see anything below ~90% we need to go back and update our judge prompt. \n",
    "\n",
    "### Define our RAG prompt\n",
    "Next we'll copy paste the prompt from the previous workshop into this workshop. We'll run RAG on the context provided in the gold standard dataset and compare the models results to the gold standard using our rubric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our system prompt. This is the prompt that will be used to generate the response.\n",
    "RAG_SYSTEM_PROMPT: str = \"\"\"\n",
    "You are a coffee expert. You are given a question and a context. \n",
    "Your job is to answer the question based ONLY on the context provided. \n",
    "Just answer the question, avoid saying \"Based on the context provided\" before answering.\n",
    "If the context doesn't contain the answer, say \"I don't know\"\n",
    "\"\"\"\n",
    "\n",
    "# Define our RAG prompt template. This is the prompt that will be used to generate the response.\n",
    "RAG_PROMPT_TEMPLATE: str = \"\"\"\n",
    "Using the context below, answer the question.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Remember, if the context doesn't contain the answer, say \"I don't know\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running an Eval\n",
    "Now for the fun part. lets run our evaluation dataset and see how it performs! We have a dataset of 6 example questions to start for the workshop. In practice, you'll want at least 50 human curated eval datapoints. But 6 is easier to manage which is why we opted for less.\n",
    "\n",
    "**Note**: We aren't testing E2E. The dataset assumes the retrieval part of RAG is already done. We simply need to evaluate the prompt. Lets load the eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_df: pd.DataFrame = pd.read_csv('../data/eval-datasets/gold_standard.csv')\n",
    "\n",
    "eval_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from typing import Tuple\n",
    "\n",
    "def process_row(row: pd.Series) -> Tuple[int, str, str]:\n",
    "    \"\"\"Process a single row of the dataframe in parallel.\"\"\"\n",
    "    index = row.name\n",
    "    question = row['query_text']\n",
    "    context = row['context']\n",
    "    gold_standard = row['gold_standard']\n",
    "    \n",
    "    # Get the evaluation prompt response\n",
    "    rag_prompt: str = RAG_PROMPT_TEMPLATE.format(\n",
    "        question=question,\n",
    "        context=context,\n",
    "    )\n",
    "\n",
    "    # Get the evaluation response\n",
    "    rag_response: str = call_bedrock(rag_prompt, RAG_SYSTEM_PROMPT)\n",
    "\n",
    "    # Now lets evaluate the response against the gold standard.\n",
    "    evaluation_prompt: str = EVALUATION_PROMPT_TEMPLATE.format(\n",
    "        question=question,\n",
    "        generated_response=rag_response,\n",
    "        gold_standard=gold_standard,\n",
    "        context=context\n",
    "    )\n",
    "\n",
    "    # Get the evaluation response. Note we're using the Sonnet model here.\n",
    "    evaluation_response: str = call_bedrock(evaluation_prompt, EVALUATION_SYSTEM_PROMPT)\n",
    "\n",
    "    thinking: str = extract_tag_content(evaluation_response, 'thinking')\n",
    "    score: str = extract_tag_content(evaluation_response, 'score')\n",
    "    \n",
    "    return index, thinking, score\n",
    "\n",
    "def run_eval(df: pd.DataFrame, max_workers: int = 4) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run evaluation on all rows in parallel.\n",
    "    \"\"\"\n",
    "    # make a copy of the dataframe so we don't destroy the existing one between eval runs\n",
    "    judge_alignment_df = df.copy(deep=True)\n",
    "    \n",
    "    # Use ThreadPoolExecutor to process rows in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all jobs to the executor\n",
    "        future_to_row = {executor.submit(process_row, row): row for _, row in df.iterrows()}\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_row):\n",
    "            try:\n",
    "                index, thinking, score = future.result()\n",
    "                # Add the evaluation response to the dataframe\n",
    "                judge_alignment_df.loc[index, 'ai_reasoning'] = thinking\n",
    "                judge_alignment_df.loc[index, 'ai_grade'] = score\n",
    "                \n",
    "                # Print progress (optional)\n",
    "                print(f\"Completed evaluation {index+1}/{len(df)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {e}\")\n",
    "    \n",
    "    return judge_alignment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the eval\n",
    "eval_results_df: pd.DataFrame = run_eval(eval_data_df)\n",
    "\n",
    "eval_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to our rubric alignment, lets create a helper class to generate a report of how the prompt is behaving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textwrap import fill\n",
    "\n",
    "class PromptEvaluator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.grades = df['ai_grade'].astype(float)\n",
    "    \n",
    "    def calculate_metrics(self):\n",
    "        return {\n",
    "            'Mean': np.mean(self.grades),\n",
    "            'Median': np.median(self.grades),\n",
    "            'Standard Deviation': np.std(self.grades),\n",
    "            'Minimum Grade': np.min(self.grades),\n",
    "            'Maximum Grade': np.max(self.grades)\n",
    "        }\n",
    "    \n",
    "    def generate_report(self):\n",
    "        metrics = self.calculate_metrics()\n",
    "        report = \"Prompt Evaluation Report\\n\"\n",
    "        report += \"========================\\n\\n\"\n",
    "        \n",
    "        for metric, value in metrics.items():\n",
    "            report += f\"{metric}: {value:.2f}\\n\"\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def analyze_grade_distribution(self):\n",
    "        # Create bins for grades (1-2, 2-3, 3-4, 4-5)\n",
    "        bins = [0, 1, 2, 3, 4, 5]\n",
    "        labels = ['0-1', '1-2', '2-3', '3-4', '4-5']\n",
    "        \n",
    "        # Create a new column with binned grades\n",
    "        binned_grades = pd.cut(self.df['ai_grade'].astype(float), \n",
    "                            bins=bins, \n",
    "                            labels=labels, \n",
    "                            include_lowest=True,\n",
    "                            right=True)\n",
    "        \n",
    "        # Count occurrences in each bin\n",
    "        distribution = binned_grades.value_counts().sort_index()\n",
    "        \n",
    "        return distribution\n",
    "\n",
    "    def pretty_print_lowest_results(self, n=3, width=80):\n",
    "        lowest_results = self.df.nsmallest(n, 'grade')\n",
    "        for index, row in lowest_results.iterrows():\n",
    "            print(f\"{'='*width}\\n\")\n",
    "            print(f\"Grade: {row['ai_grade']}\\n\")\n",
    "            print(\"Query Text:\")\n",
    "            print(fill(row['query_text'], width=width))\n",
    "            print(\"\\nLLM Response:\")\n",
    "            print(fill(row['llm_response'], width=width))\n",
    "            print(\"\\nReasoning:\")\n",
    "            print(fill(row['reasoning'], width=width))\n",
    "            print(f\"\\n{'='*width}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluator\n",
    "evaluator = PromptEvaluator(eval_results_df)\n",
    "\n",
    "# Generate and print the report\n",
    "print(evaluator.generate_report())\n",
    "\n",
    "# Analyze grade distribution\n",
    "print(evaluator.analyze_grade_distribution())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "The results don't look great. That's to be expected though, this is not a very good RAG prompt ðŸ˜Š. Most of the scores are falling below what we would consider acceptable quality.\n",
    "\n",
    "Now it's your turn! Your goal is to improve the RAG prompt to get better evaluation scores. Here's what to do:\n",
    "\n",
    "1. Analyze the current RAG prompt to identify its weaknesses.\n",
    "\n",
    "2. Create an improved version of the RAG prompt that addresses these weaknesses. Consider:\n",
    "   - Adding more specific instructions\n",
    "   - Providing better context utilization guidance\n",
    "   - Incorporating structure for more complete answers\n",
    "   - Specifying the desired tone and style\n",
    "\n",
    "3. Test your improved prompt with the same set of questions.\n",
    "\n",
    "4. Use the evaluation system to measure your improvements.\n",
    "\n",
    "Your target is to get all responses scoring between 3-5 on our evaluation scale, with the majority falling in the 4-5 range.\n",
    "\n",
    "Remember: Good prompt engineering is iterative. Analyze the results, identify patterns in lower-scoring responses, and continue refining your prompt until you consistently achieve high-quality outputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This concludes module 1. Next lets get into some more advanced concepts and introduce our first framework (LangGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
