{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Building Effective Workflow Agents\n",
    "\n",
    "Welcome to our workflow agents lab! In this module, we'll learn how to build sophisticated workflow agents using several advanced patterns:\n",
    "\n",
    "- **Prompt chaining**: Breaking tasks into sequential steps where each output feeds into the next\n",
    "- **Routing**: Directing requests to different specialized agents based on query type\n",
    "- **Parallelization**: Running multiple agent tasks simultaneously for efficiency\n",
    "- **Orchestrators**: Coordinating complex workflows between multiple agents\n",
    "- **Evaluator-optimizer patterns**: Continuously improving agent performance\n",
    "\n",
    "In the next few notebooks, we'll apply these techniques to build a \"deep research\" OpenSearch Question Answering workflow agent that leverages OpenSearch's public documentation.\n",
    "\n",
    "### ðŸ’¾ Setup: ChromaDB Vector Store\n",
    "\n",
    "In this notebook, we'll set up our ChromaDB vector store for use across these different patterns. This involves:\n",
    "\n",
    "1. Downloading OpenSearch documentation\n",
    "2. Processing and chunking the text\n",
    "3. Creating embeddings\n",
    "4. Storing them in ChromaDB\n",
    "\n",
    "**Note**: Building the ChromaDB cache from the public documentation takes about 2 minutes. If doing the lab on your own, you'll need to run this section. If participating in an AWS lab environment, we've pre-created the ChromaDB store for you, and you can skip this section.\n",
    "\n",
    "Let's begin! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Download our source data\n",
    "First we'll need to clone the OpenSearch documentation from the website which is licensed under Apache 2.0. This will be our \"base knowledge\". we use bash cell magic %%bash to run the entire cell as a bash script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ../../data/opensearch-docs\n",
    "cd ../../data/opensearch-docs\n",
    "git clone https://github.com/opensearch-project/documentation-website.git ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create RAG Pipeline\n",
    "We'll modify the helper functions from module 1 to ingest markdown documents. To speed things up we'll also add some threading to the local chromaDB to create the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import boto3\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize Chroma client from our persisted store\n",
    "chroma_client = chromadb.PersistentClient(path=\"../../data/chroma\")\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "session = boto3.Session()\n",
    "bedrock = session.client(service_name='bedrock-runtime')\n",
    "\n",
    "print(\"âœ… Client setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll modify the LlamaIndex ingestion pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import Node\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "import re\n",
    "\n",
    "# Create a class to use instead of LlamaIndex Nodes. This way we decouple our chroma collections from LlamaIndexes\n",
    "class RAGChunk(BaseModel):\n",
    "    id_: str\n",
    "    text: str\n",
    "    metadata: Dict[str, Any] = {}\n",
    "\n",
    "\n",
    "class SentenceSplitterChunkingStrategy:\n",
    "    def __init__(self, input_dir: str, chunk_size: int = 256, chunk_overlap: int = 128):\n",
    "        self.input_dir = input_dir\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.pipeline = self._create_pipeline()\n",
    "\n",
    "        # Helper to get regex pattern for normalizing relative file paths.\n",
    "        self.relative_path_pattern = rf\"{re.escape(input_dir)}(/.*)\"\n",
    "\n",
    "    def _extract_relative_path(self, full_path):\n",
    "        # Get Regex pattern\n",
    "        pattern = self.relative_path_pattern\n",
    "        match = re.search(pattern, full_path)\n",
    "        if match:\n",
    "            return match.group(1).lstrip('/')\n",
    "        return None\n",
    "\n",
    "    def _create_pipeline(self) -> IngestionPipeline:\n",
    "        transformations = [\n",
    "            SentenceSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap),\n",
    "        ]\n",
    "        return IngestionPipeline(transformations=transformations)\n",
    "\n",
    "    def load_documents(self) -> List[Document]:\n",
    "        # If you're using a different type of file besides md, you'll want to change this. \n",
    "        return SimpleDirectoryReader(\n",
    "            input_dir=self.input_dir, \n",
    "            recursive=True,\n",
    "            required_exts=['.md']\n",
    "        ).load_data()\n",
    "\n",
    "    def to_ragchunks(self, nodes: List[Node]) -> List[RAGChunk]:\n",
    "        return [\n",
    "            RAGChunk(\n",
    "                id_=node.node_id,\n",
    "                text=node.text,\n",
    "                metadata={\n",
    "                    **node.metadata,\n",
    "                    'relative_path': self._extract_relative_path(node.metadata['file_path'])\n",
    "                }\n",
    "            )\n",
    "            for node in nodes\n",
    "        ]\n",
    "\n",
    "    def process(self) -> List[RAGChunk]:\n",
    "        documents = self.load_documents()\n",
    "        nodes = self.pipeline.run(documents=documents)\n",
    "        rag_chunks = self.to_ragchunks(nodes)\n",
    "        \n",
    "        print(f\"Processing complete. Created {len(rag_chunks)} chunks.\")\n",
    "        return rag_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These values were evaluated using the outputs of this notebook. https://github.com/aws-samples/genai-system-evaluation/blob/main/example-notebooks/1_Embedding_And_Chunking_Validation.ipynb\n",
    "chunking_strategy = SentenceSplitterChunkingStrategy(\n",
    "    input_dir=\"../../data/opensearch-docs\",\n",
    "    chunk_size=2048,\n",
    "    chunk_overlap=128\n",
    ")\n",
    "\n",
    "# Get the nodes from the chunker.\n",
    "chunks: RAGChunk = chunking_strategy.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a wrapper around ChromaDB like we did in module 1. But this time we'll add some parallelization to speed things up b/c we have a lot of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Dict\n",
    "from abc import ABC, abstractmethod\n",
    "import chromadb\n",
    "from chromadb.api.types import EmbeddingFunction\n",
    "from typing import List, Dict, Any\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from chromadb.utils.embedding_functions import AmazonBedrockEmbeddingFunction\n",
    "\n",
    "\n",
    "class RetrievalResult(BaseModel):\n",
    "    id: str\n",
    "    document: str\n",
    "    embedding: List[float]\n",
    "    distance: float\n",
    "    metadata: Dict = {}\n",
    "\n",
    "\n",
    "# Example of a concrete implementation\n",
    "class ChromaDBWrapperClient:\n",
    "\n",
    "    def __init__(self, chroma_client, collection_name: str, embedding_function: AmazonBedrockEmbeddingFunction):\n",
    "        self.client = chroma_client\n",
    "        self.collection_name = collection_name\n",
    "        self.embedding_function = embedding_function\n",
    "\n",
    "        # Create the collection\n",
    "        self.collection = self._create_collection()\n",
    "\n",
    "    def _create_collection(self):\n",
    "        return self.client.get_or_create_collection(\n",
    "            name=self.collection_name,\n",
    "            embedding_function=self.embedding_function\n",
    "        )\n",
    "\n",
    "    def add_chunks_to_collection(self, chunks: List[RAGChunk], batch_size: int = 20, num_workers: int = 10):\n",
    "        batches = [chunks[i:i + batch_size] for i in range(0, len(chunks), batch_size)]\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            futures = [executor.submit(self._add_batch, batch) for batch in batches]\n",
    "            for future in as_completed(futures):\n",
    "                future.result()  # This will raise an exception if one occurred during the execution\n",
    "        print('Finished Ingesting Chunks Into Collection')\n",
    "\n",
    "    def _add_batch(self, batch: List[RAGChunk]):\n",
    "        self.collection.add(\n",
    "            ids=[chunk.id_ for chunk in batch],\n",
    "            documents=[chunk.text for chunk in batch],\n",
    "            metadatas=[chunk.metadata for chunk in batch]\n",
    "        )\n",
    "\n",
    "    def retrieve(self, query_text: str, n_results: int = 5) -> List[RetrievalResult]:\n",
    "        # Query the collection\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=n_results,\n",
    "            include=['embeddings', 'documents', 'metadatas', 'distances']\n",
    "        )\n",
    "\n",
    "        # Transform the results into RetrievalResult objects\n",
    "        retrieval_results = []\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            retrieval_results.append(RetrievalResult(\n",
    "                id=results['ids'][0][i],\n",
    "                document=results['documents'][0][i],\n",
    "                embedding=results['embeddings'][0][i],\n",
    "                distance=results['distances'][0][i],\n",
    "                metadata=results['metadatas'][0][i] if results['metadatas'][0] else {}\n",
    "            ))\n",
    "\n",
    "        return retrieval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use our LlamaIndex ingestion pipeline and bulk upload the data to chroma. This will create a persistant DB file under data/chroma. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils.embedding_functions import AmazonBedrockEmbeddingFunction\n",
    "\n",
    "# Define some experiment variables\n",
    "EMBEDDING_MODEL_ID: str = 'amazon.titan-embed-text-v2:0'\n",
    "COLLECTION_NAME: str = 'opensearch-docs-rag'\n",
    "\n",
    "# This is a handy function Chroma implemented for calling bedrock. Lets use it!\n",
    "embedding_function = AmazonBedrockEmbeddingFunction(\n",
    "    session=session,\n",
    "    model_name=EMBEDDING_MODEL_ID\n",
    ")\n",
    "\n",
    "# Create our retrieval task. All retrieval tasks in this tutorial implement BaseRetrievalTask which has the method retrieve()\n",
    "# If you'd like to extend this to a different retrieval configuration, all you have to do is create a class that that implements\n",
    "# this abstract class and the rest is the same!\n",
    "chroma_os_docs_collection: ChromaDBWrapperClient = ChromaDBWrapperClient(\n",
    "    chroma_client = chroma_client, \n",
    "    collection_name = COLLECTION_NAME,\n",
    "    embedding_function = embedding_function\n",
    ")\n",
    "\n",
    "chroma_os_docs_collection.add_chunks_to_collection(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run some queries and see what comes back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_os_docs_collection.retrieve(\"What is OpenSearch?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this setup, we downloaded the AOS documentation and ingested it into a local vector database that we'll be reusing across the next couple laps. Feel free to move to the next lab in the module."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
