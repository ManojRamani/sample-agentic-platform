{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”— Documentation Explainer: The Prompt Chaining Pattern\n",
    "\n",
    "Welcome to the prompt chaining lab! Prompt chaining (or prompt decomposition) involves breaking a task down into a series of steps. Each step calls an LLM and passes the output on to the next step. As part of each step, you can add error checking steps, or make calls to tools, knowledge bases, or other systems.\n",
    "\n",
    "This pattern works best when:\n",
    "- Tasks can be cleanly broken into fixed subtasks\n",
    "- Trading latency for higher accuracy makes sense\n",
    "- Each step builds on the previous one\n",
    "\n",
    "Perfect for breaking down complex OpenSearch documentation! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import boto3\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize Chroma client from our persisted store\n",
    "chroma_client = chromadb.PersistentClient(path=\"../../data/chroma\")\n",
    "\n",
    "session = boto3.Session()\n",
    "bedrock = session.client(service_name='bedrock-runtime')\n",
    "\n",
    "print(\"âœ… Client setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Helper Functions\n",
    "We'll set up some basic helper functions from our rag basics examples for doing rag, calling Bedrock, and the retrieval portion of our local ChromaDB from the setup step\n",
    "\n",
    "**Note on chroma:** Because we're using the same collection name and set a persistant client, it'll automatically load the dataset from disk so we don't need to reindex every time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Dict\n",
    "from chromadb.api.types import EmbeddingFunction\n",
    "from typing import List, Dict, Any\n",
    "from chromadb.utils.embedding_functions import AmazonBedrockEmbeddingFunction\n",
    "\n",
    "\n",
    "class RetrievalResult(BaseModel):\n",
    "    id: str\n",
    "    document: str\n",
    "    embedding: List[float]\n",
    "    distance: float\n",
    "    metadata: Dict = {}\n",
    "\n",
    "\n",
    "# Example of a concrete implementation\n",
    "class ChromaDBRetrievalClient:\n",
    "\n",
    "    def __init__(self, chroma_client, collection_name: str, embedding_function: AmazonBedrockEmbeddingFunction):\n",
    "        self.client = chroma_client\n",
    "        self.collection_name = collection_name\n",
    "        self.embedding_function = embedding_function\n",
    "\n",
    "        # Create the collection\n",
    "        self.collection = self._create_collection()\n",
    "\n",
    "    def _create_collection(self):\n",
    "        return self.client.get_or_create_collection(\n",
    "            name=self.collection_name,\n",
    "            embedding_function=self.embedding_function\n",
    "        )\n",
    "\n",
    "    def retrieve(self, query_text: str, n_results: int = 5) -> List[RetrievalResult]:\n",
    "        # Query the collection\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=n_results,\n",
    "            include=['embeddings', 'documents', 'metadatas', 'distances']\n",
    "        )\n",
    "\n",
    "        # Transform the results into RetrievalResult objects\n",
    "        retrieval_results = []\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            retrieval_results.append(RetrievalResult(\n",
    "                id=results['ids'][0][i],\n",
    "                document=results['documents'][0][i],\n",
    "                embedding=results['embeddings'][0][i],\n",
    "                distance=results['distances'][0][i],\n",
    "                metadata=results['metadatas'][0][i] if results['metadatas'][0] else {}\n",
    "            ))\n",
    "\n",
    "        return retrieval_results\n",
    "    \n",
    "from chromadb.utils.embedding_functions import AmazonBedrockEmbeddingFunction\n",
    "\n",
    "# Define some experiment variables\n",
    "EMBEDDING_MODEL_ID: str = 'amazon.titan-embed-text-v2:0'\n",
    "COLLECTION_NAME: str = 'opensearch-docs-rag'\n",
    "\n",
    "# This is a handy function Chroma implemented for calling bedrock. Lets use it!\n",
    "embedding_function = AmazonBedrockEmbeddingFunction(\n",
    "    session=session,\n",
    "    model_name=EMBEDDING_MODEL_ID\n",
    ")\n",
    "\n",
    "# Create our retrieval task. All retrieval tasks in this tutorial implement BaseRetrievalTask which has the method retrieve()\n",
    "# If you'd like to extend this to a different retrieval configuration, all you have to do is create a class that that implements\n",
    "# this abstract class and the rest is the same!\n",
    "chroma_os_docs_collection: ChromaDBRetrievalClient = ChromaDBRetrievalClient(\n",
    "    chroma_client = chroma_client, \n",
    "    collection_name = COLLECTION_NAME,\n",
    "    embedding_function = embedding_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that it works. \n",
    "len(chroma_os_docs_collection.retrieve(\"What is OpenSearch?\", n_results=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create some helper functions and classes\n",
    "\n",
    "## Prompt Template\n",
    "This is very useful for reusability. You can create your prompts in code or use a prompt management tool like Bedrock Prompt management to keep track of prompts. Even using these tools, it's important to create your own abstraction layer / implementation of the prompt so you can pass around typed objects.\n",
    "\n",
    "Below is a sample prompt template implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "\n",
    "HAIKU_MODEL_ID = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "class BasePrompt(BaseModel):\n",
    "    \"\"\"\n",
    "    A streamlined base class for creating prompts with system and user components.\n",
    "    \"\"\"\n",
    "    system_prompt: str\n",
    "    user_prompt: str    \n",
    "    inputs: Dict[str, Any] = Field(default_factory=dict)\n",
    "    model_id: str = HAIKU_MODEL_ID\n",
    "    hyperparams: Dict[str, Any] = Field(default_factory=lambda: {\n",
    "        \"temperature\": 0.5,\n",
    "        \"maxTokens\": 1000\n",
    "    })\n",
    "\n",
    "    # Just format the prompt if inputs were provided during initialization\n",
    "    def __init__(self, **data):\n",
    "        super().__init__(**data)\n",
    "        # Format prompts if inputs were provided during initialization\n",
    "        if self.inputs:\n",
    "            self.format()\n",
    "\n",
    "    def format(self, inputs: Dict[str, Any] = None) -> None:\n",
    "        \"\"\"Format system_prompt and user_prompt with inputs.\"\"\"\n",
    "\n",
    "        # Override the inputs if provided.\n",
    "        inputs_to_use = inputs if inputs else self.inputs\n",
    "\n",
    "        try:\n",
    "            self.system_prompt = self.system_prompt.format(**inputs_to_use)\n",
    "            self.user_prompt = self.user_prompt.format(**inputs_to_use)\n",
    "        except KeyError as e:\n",
    "            raise KeyError(f'Missing input value: {e}')\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Error formatting prompt: {e}')\n",
    "    \n",
    "\n",
    "    def to_bedrock_messages(self, conversation: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Convert to Bedrock messages format.\"\"\"\n",
    "        messages = conversation.copy() if conversation else []\n",
    "        messages.append({\"role\": \"user\", \"content\": [{\"text\": self.user_prompt}]})\n",
    "        return messages\n",
    "    \n",
    "    def to_bedrock_system(self, guard_content: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Convert to Bedrock system format.\"\"\"\n",
    "        system = [{\"text\": self.system_prompt}]\n",
    "        if guard_content:\n",
    "            system.append({\"guard\": guard_content})\n",
    "        return system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "Now lets create some helper functions for calling bedrock and doing RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "\n",
    "def call_bedrock(prompt: BasePrompt) -> str:\n",
    "    kwargs = {\n",
    "        \"modelId\": prompt.model_id,\n",
    "        \"inferenceConfig\": prompt.hyperparams,\n",
    "        \"messages\": prompt.to_bedrock_messages(),\n",
    "        \"system\": prompt.to_bedrock_system(),\n",
    "    }\n",
    "\n",
    "    # Call Bedrock\n",
    "    converse_response: Dict[str, Any] = bedrock.converse(**kwargs)\n",
    "    # Get the model's text response\n",
    "    return converse_response['output']['message']['content'][0]['text']\n",
    "\n",
    "# Helper function to call bedrock\n",
    "def do_rag(user_input: str, rag_prompt: Type[BasePrompt]) -> str:\n",
    "    # Retrieve the context from the vector store\n",
    "    retrieval_results: List[RetrievalResult] = chroma_os_docs_collection.retrieve(user_input, n_results=2)\n",
    "    # Format the context into a string\n",
    "    context: str = \"\\n\\n\".join([result.document for result in retrieval_results])\n",
    "\n",
    "    print(\"Retrieval done\")\n",
    "    # Create the RAG prompt\n",
    "    inputs: Dict[str, Any] = {\"question\": user_input, \"context\": context}\n",
    "    rag_prompt: BasePrompt = rag_prompt(inputs=inputs)\n",
    "    # Call Bedrock with the RAG prompt\n",
    "\n",
    "    print(\"Calling Bedrock\")\n",
    "    return call_bedrock(rag_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Our Documentation Explainer\n",
    "\n",
    "We'll build a workflow that:\n",
    "1. Extracts key concepts from OpenSearch documentation\n",
    "2. Explains those concepts in simpler terms\n",
    "3. Provides practical implementation examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets define our prompts using the BasePrompt class we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that can explain OpenSearch documentation in simple terms.\n",
    "\"\"\"\n",
    "\n",
    "# Define reusable prompt templates as constants\n",
    "EXTRACT_CONCEPTS_PROMPT_TEMPLATE = \"\"\"\n",
    "Using the users query, extract and list the key concepts from this OpenSearch documentation:\n",
    "\n",
    "<query>\n",
    "{question}\n",
    "</query>\n",
    "\n",
    "<documentation>\n",
    "{context}\n",
    "</documentation>\n",
    "\n",
    "Focus on core principles and important technical details.\n",
    "Format as a bulleted list.\n",
    "\"\"\"\n",
    "\n",
    "SIMPLIFY_EXPLANATION_PROMPT_TEMPLATE = \"\"\"\n",
    "Explain these OpenSearch concepts in simple, clear terms:\n",
    "{concepts}\n",
    "\n",
    "Write as if explaining to someone new to OpenSearch.\n",
    "Include analogies where helpful.\n",
    "\"\"\"\n",
    "\n",
    "GENERATE_EXAMPLES_PROMPT_TEMPLATE = \"\"\"\n",
    "Create practical examples for implementing these OpenSearch concepts:\n",
    "Concepts: {concepts}\n",
    "Explanation: {explanation}\n",
    "\n",
    "Include:\n",
    "- Code snippets where relevant\n",
    "- Step-by-step instructions\n",
    "- Common pitfalls to avoid\n",
    "\"\"\"\n",
    "\n",
    "FORMAT_OUTPUT_TEMPLATE = \"\"\"\n",
    "# OpenSearch Documentation Breakdown\n",
    "\n",
    "## Key Concepts\n",
    "{concepts}\n",
    "\n",
    "## Simple Explanation\n",
    "{explanation}\n",
    "\n",
    "## Implementation Examples\n",
    "{examples}\n",
    "\"\"\"\n",
    "\n",
    "class ExtractConceptPrompt(BasePrompt):\n",
    "    system_prompt: str = SYSTEM_PROMPT\n",
    "    user_prompt: str = EXTRACT_CONCEPTS_PROMPT_TEMPLATE\n",
    "\n",
    "class SimplifyExplanationPrompt(BasePrompt):\n",
    "    system_prompt: str = SYSTEM_PROMPT\n",
    "    user_prompt: str = SIMPLIFY_EXPLANATION_PROMPT_TEMPLATE\n",
    "\n",
    "class GenerateExamplesPrompt(BasePrompt):\n",
    "    system_prompt: str = SYSTEM_PROMPT\n",
    "    user_prompt: str = GENERATE_EXAMPLES_PROMPT_TEMPLATE\n",
    "\n",
    "class FormatOutputPrompt(BasePrompt):\n",
    "    system_prompt: str = SYSTEM_PROMPT\n",
    "    user_prompt: str = FORMAT_OUTPUT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets create our State dictionary. We're using Pydantic models here but will convert to a dictionary before passing to our LangGraph graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "\n",
    "# Define the WorkflowState using TypedDict\n",
    "class ExplainerState(TypedDict):\n",
    "    # Input\n",
    "    query: str = \"\"\n",
    "    # Extracted concepts\n",
    "    concepts: str = \"\"\n",
    "    # Simplified explanation\n",
    "    explanation: str = \"\"\n",
    "    # Generated examples\n",
    "    examples: str = \"\"\n",
    "    # Final formatted output\n",
    "    final_output: str = \"\"\n",
    "    # Track completion status\n",
    "    complete: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets create our nodes as functions passing in the state dict and outputting the state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_concepts(state: ExplainerState) -> ExplainerState:\n",
    "    \"\"\"Extracts key concepts from documentation\"\"\"\n",
    "    print(\"Starting concept extraction...\")\n",
    "    concepts: str = do_rag(state['query'], ExtractConceptPrompt)\n",
    "    state['concepts'] = concepts\n",
    "    print(f\"Concepts extracted\")\n",
    "    return state\n",
    "\n",
    "def simplify_explanation(state: ExplainerState) -> ExplainerState:\n",
    "    \"\"\"Explains concepts in simpler terms\"\"\"\n",
    "    print(\"Starting simplification of explanation...\")\n",
    "    inputs: Dict[str, Any] = {\"concepts\": state['concepts']}\n",
    "    prompt: BasePrompt = SimplifyExplanationPrompt(inputs=inputs)\n",
    "\n",
    "    explanation: str = call_bedrock(prompt)\n",
    "    state['explanation'] = explanation\n",
    "    print(f\"Simplified explanation\")\n",
    "    return state\n",
    "\n",
    "def generate_examples(state: ExplainerState) -> ExplainerState:\n",
    "    \"\"\"Provides practical implementation examples\"\"\"\n",
    "    print(\"Generating examples...\")\n",
    "    inputs: Dict[str, Any] = {\"concepts\": state['concepts'], \"explanation\": state['explanation']}\n",
    "    prompt: BasePrompt = GenerateExamplesPrompt(inputs=inputs)\n",
    "\n",
    "    examples: str = call_bedrock(prompt)\n",
    "    state['examples'] = examples\n",
    "    print(f\"Examples generated\")\n",
    "    return state\n",
    "\n",
    "def format_output(state: ExplainerState) -> ExplainerState:\n",
    "    \"\"\"Formats the final documentation breakdown\"\"\"\n",
    "    print(\"Formatting final output...\")\n",
    "    inputs: Dict[str, Any] = {\n",
    "        \"concepts\": state['concepts'], \n",
    "        \"explanation\": state['explanation'], \n",
    "        \"examples\": state['examples']\n",
    "    }\n",
    "    prompt: BasePrompt = FormatOutputPrompt(inputs=inputs)\n",
    "\n",
    "    final_output: str = call_bedrock(prompt)\n",
    "    state['final_output'] = final_output\n",
    "    print(\"Final output formatted.\")\n",
    "    return state\n",
    "    \n",
    "def init_state(query: str) -> ExplainerState:\n",
    "    \"\"\"Initialize the search state with a query.\"\"\"\n",
    "    return ExplainerState(\n",
    "        query=query,\n",
    "        concepts= \"\",\n",
    "        explanation= \"\",\n",
    "        examples= \"\",\n",
    "        final_output= \"\",\n",
    "        complete= False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, lets compile our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_chain_workflow() -> StateGraph:\n",
    "    # This is kind of frustrating. The StateGraph takes in a dict\n",
    "    # so you only find out if you have a typing error at runtime..\n",
    "    workflow = StateGraph(ExplainerState)\n",
    "\n",
    "     # Add nodes to the graph\n",
    "    workflow.add_node(\"extract_concepts\", extract_concepts)\n",
    "    workflow.add_node(\"simplify_explanation\", simplify_explanation)\n",
    "    workflow.add_node(\"generate_examples\", generate_examples)\n",
    "    workflow.add_node(\"format_output\", format_output)\n",
    "    \n",
    "    # Define the workflow edges. These are sequential.\n",
    "    workflow.add_edge(START, \"extract_concepts\")\n",
    "    workflow.add_edge(\"extract_concepts\", \"simplify_explanation\")\n",
    "    workflow.add_edge(\"simplify_explanation\", \"generate_examples\")\n",
    "    workflow.add_edge(\"generate_examples\", \"format_output\")\n",
    "    workflow.add_edge(\"format_output\", END)\n",
    "    \n",
    "    # Compile and return the workflow\n",
    "    return workflow.compile()\n",
    "\n",
    "graph: StateGraph = create_prompt_chain_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualize the graph to get a sense of what we're about to run\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run our Prompt Chain\n",
    "\n",
    "Let's test our documentation explainer with a question related to security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pose a question.\n",
    "question: str = \"Create you explain OpenSearch's security configuration?\"\n",
    "\n",
    "# Initialize the state.\n",
    "state: ExplainerState = init_state(query=question)\n",
    "\n",
    "# Invoke the graph.\n",
    "results: ExplainerState = graph.invoke(state)\n",
    "\n",
    "print(\"ðŸ“š OpenSearch Security Documentation Breakdown\")\n",
    "print(results['final_output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benefits of the Prompt Chaining Pattern\n",
    "\n",
    "Our documentation breakdown approach offers several advantages:\n",
    "\n",
    "âœ… Each step has a clear, focused purpose\n",
    "\n",
    "âœ… Better understanding through progressive refinement\n",
    "\n",
    "âœ… Easy to modify or extend the chain\n",
    "\n",
    "\n",
    "However, it's very slow because it's making lots of sequential calls with large amounts of outputs. In the next few sections, lets see how we can use some additional patterns to speed the workflow up!\n",
    "\n",
    "Next, we'll explore how to use the routing pattern to handle different types of OpenSearch questions! ðŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
