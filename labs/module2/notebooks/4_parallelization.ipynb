{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš¡ Multi-Aspect Solutions: The Parallelization Pattern\n",
    "\n",
    "Welcome to parallel processing! This pattern shines when:\n",
    "- Tasks can be divided into independent subtasks\n",
    "- Multiple perspectives improve the solution\n",
    "- Each aspect needs focused attention\n",
    "\n",
    "Perfect for generating different OpenSearch solution approaches! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "from utils.retrieval_utils import get_chroma_os_docs_collection, ChromaDBRetrievalClient\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "REGION = 'us-west-2'\n",
    "session = boto3.Session()\n",
    "bedrock = session.client(service_name='bedrock-runtime', region_name=REGION)\n",
    "\n",
    "# We've pushed the retrieval client from the prompt chaining notebook to the retrieval utils for simplicity\n",
    "chroma_os_docs_collection: ChromaDBRetrievalClient = get_chroma_os_docs_collection()\n",
    "\n",
    "print(\"âœ… Client setup and retrieval client complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers\n",
    "Import the same helpers for bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, Dict, Any, List\n",
    "\n",
    "# We pushed the base propmt from the previous lab to a a base prompt file.\n",
    "from utils.base_prompt import BasePrompt\n",
    "from utils.retrieval_utils import RetrievalResult\n",
    "\n",
    "def call_bedrock(prompt: BasePrompt) -> str:\n",
    "    kwargs = {\n",
    "        \"modelId\": 'us.amazon.nova-micro-v1:0', # Lets use nova micro because it's really fast\n",
    "        \"inferenceConfig\": prompt.hyperparams,\n",
    "        \"messages\": prompt.to_bedrock_messages(),\n",
    "        \"system\": prompt.to_bedrock_system(),\n",
    "    }\n",
    "\n",
    "    # Call Bedrock\n",
    "    converse_response: Dict[str, Any] = bedrock.converse(**kwargs)\n",
    "    # Get the model's text response\n",
    "    return converse_response['output']['message']['content'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Our Parallel Solution Generator\n",
    "\n",
    "We'll generate three different solutions simultaneously:\n",
    "1. Basic solution for beginners\n",
    "2. Advanced solution for experts\n",
    "3. Cost-optimized solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets create our prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Dict, Any, List, Annotated\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Define the system prompt\n",
    "SOLUTION_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant specializing in OpenSearch documentation and support.\n",
    "\"\"\"\n",
    "\n",
    "# Define reusable prompt templates as constants\n",
    "BEGINNER_PROMPT_TEMPLATE = \"\"\"\n",
    "Create a beginner-friendly solution for this OpenSearch question:\n",
    "{question}\n",
    "\n",
    "Focus on:\n",
    "- Simple, step-by-step instructions\n",
    "- Basic concepts and terminology\n",
    "- Common pitfalls to avoid\n",
    "- Default configurations\n",
    "\"\"\"\n",
    "\n",
    "EXPERT_PROMPT_TEMPLATE = \"\"\"\n",
    "Create an advanced, expert-level solution for this OpenSearch question:\n",
    "{question}\n",
    "\n",
    "Include:\n",
    "- Advanced configurations\n",
    "- Performance optimizations\n",
    "- Best practices\n",
    "- Edge cases and considerations\n",
    "\"\"\"\n",
    "\n",
    "COST_PROMPT_TEMPLATE = \"\"\"\n",
    "Create a cost-optimized solution for this OpenSearch question:\n",
    "{question}\n",
    "\n",
    "Focus on:\n",
    "- Resource efficiency\n",
    "- Infrastructure costs\n",
    "- Performance/cost tradeoffs\n",
    "- Cost monitoring and optimization\n",
    "\"\"\"\n",
    "\n",
    "# Define prompt classes that inherit from BasePrompt\n",
    "class BeginnerPrompt(BasePrompt):\n",
    "    system_prompt: str = SOLUTION_SYSTEM_PROMPT\n",
    "    user_prompt: str = BEGINNER_PROMPT_TEMPLATE\n",
    "\n",
    "class ExpertPrompt(BasePrompt):\n",
    "    system_prompt: str = SOLUTION_SYSTEM_PROMPT\n",
    "    user_prompt: str = EXPERT_PROMPT_TEMPLATE\n",
    "\n",
    "class CostPrompt(BasePrompt):\n",
    "    system_prompt: str = SOLUTION_SYSTEM_PROMPT\n",
    "    user_prompt: str = COST_PROMPT_TEMPLATE\n",
    "\n",
    "# Define the WorkflowState using TypedDict.\n",
    "class WorkflowState(TypedDict):\n",
    "    question: str\n",
    "    beginner_solution: str\n",
    "    expert_solution: str  \n",
    "    cost_solution: str\n",
    "    final_output: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets define our nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_start(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"Starts the parallel workflow\"\"\"\n",
    "    return state\n",
    "\n",
    "def generate_beginner_solution(state: WorkflowState) -> Dict:\n",
    "    \"\"\"Generates a beginner-friendly solution\"\"\"\n",
    "    beginner_prompt: BasePrompt = BeginnerPrompt(inputs={\"question\": state[\"question\"]})\n",
    "    solution: str = call_bedrock(beginner_prompt)\n",
    "    # Only return the field this function is responsible for\n",
    "    return {\"beginner_solution\": solution}\n",
    "\n",
    "def generate_expert_solution(state: WorkflowState) -> Dict:\n",
    "    \"\"\"Generates an expert-level solution\"\"\"\n",
    "    expert_prompt: BasePrompt = ExpertPrompt(inputs={\"question\": state[\"question\"]})\n",
    "    solution: str = call_bedrock(expert_prompt)\n",
    "    # Only return the field this function is responsible for\n",
    "    return {\"expert_solution\": solution}\n",
    "\n",
    "def generate_cost_solution(state: WorkflowState) -> Dict:\n",
    "    \"\"\"Generates a cost-optimized solution\"\"\"\n",
    "    cost_prompt: BasePrompt = CostPrompt(inputs={\"question\": state[\"question\"]})\n",
    "    solution: str = call_bedrock(cost_prompt)\n",
    "    # Only return the field this function is responsible for\n",
    "    return {\"cost_solution\": solution}\n",
    "\n",
    "def format_output(state: WorkflowState) -> WorkflowState:\n",
    "    \"\"\"Formats the parallel solutions into a clear response\"\"\"\n",
    "    state[\"final_output\"] = f\"\"\"\n",
    "    # OpenSearch Solution Approaches\n",
    "    \n",
    "    ## ðŸ“˜ Beginner-Friendly Solution\n",
    "    {state[\"beginner_solution\"]}\n",
    "    \n",
    "    ## ðŸŽ¯ Expert-Level Solution\n",
    "    {state[\"expert_solution\"]}\n",
    "    \n",
    "    ## ðŸ’° Cost-Optimized Solution\n",
    "    {state[\"cost_solution\"]}\n",
    "    \"\"\"\n",
    "    return state\n",
    "\n",
    "def init_state(question: str) -> WorkflowState:\n",
    "    \"\"\"Initialize the workflow state with a question.\"\"\"\n",
    "    return WorkflowState(\n",
    "        question=question,\n",
    "        beginner_solution=\"\",\n",
    "        expert_solution=\"\",\n",
    "        cost_solution=\"\",\n",
    "        final_output=\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly lets compile our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parallel_workflow() -> StateGraph:\n",
    "    \"\"\"Creates a workflow for generating parallel solutions\"\"\"\n",
    "    workflow = StateGraph(WorkflowState)\n",
    "    \n",
    "    # Add nodes to our graph\n",
    "    workflow.add_node(\"parallelizer\", parallel_start)\n",
    "    workflow.add_node(\"beginner\", generate_beginner_solution)\n",
    "    workflow.add_node(\"expert\", generate_expert_solution)\n",
    "    workflow.add_node(\"cost\", generate_cost_solution)\n",
    "    workflow.add_node(\"format\", format_output)\n",
    "    \n",
    "    # Create the parallel workflow\n",
    "    # From START, branch to all three solution generators\n",
    "    workflow.add_edge(START, \"parallelizer\")\n",
    "\n",
    "    # Each parallel node leads to the parallelizer\n",
    "    workflow.add_edge(\"parallelizer\", \"beginner\")\n",
    "    workflow.add_edge(\"parallelizer\", \"expert\")\n",
    "    workflow.add_edge(\"parallelizer\", \"cost\")\n",
    "    \n",
    "    # Each solution node leads to format when all are complete.\n",
    "    workflow.add_edge([\"beginner\", \"expert\", \"cost\"], \"format\")\n",
    "    \n",
    "    # Format leads to END\n",
    "    workflow.add_edge(\"format\", END)\n",
    "    \n",
    "    # Compile and return the workflow\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "graph: StateGraph = create_parallel_workflow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to scale OpenSearch clusters effectively?\"\n",
    "state: WorkflowState = init_state(question)\n",
    "result: WorkflowState = graph.invoke(state)\n",
    "\n",
    "print(\"âš¡ Multiple Solution Approaches Generated\")\n",
    "print(result['final_output'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benefits of the Parallelization Pattern\n",
    "\n",
    "Our parallel solution approach provides several advantages:\n",
    "\n",
    "âœ… Faster overall response time through parallelization\n",
    "\n",
    "âœ… Easy parallelization through LangGraph\n",
    "\n",
    "Next, we'll explore the orchestrator-workers pattern for complex troubleshooting! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
