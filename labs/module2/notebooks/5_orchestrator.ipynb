{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯 Complex Troubleshooting: The Orchestrator-Workers Pattern\n",
    "\n",
    "Welcome to orchestrator-workers! This pattern shines when:\n",
    "- Tasks need dynamic subtask planning\n",
    "- You can't predict all steps in advance\n",
    "- Work needs coordinated execution\n",
    "\n",
    "The main difference between an orchestrator and a parallelizer is that we don't know the steps to take ahead of time. Instead we'll use an LLM to dynamically assign tasks. \n",
    "\n",
    "Perfect for complex OpenSearch troubleshooting! 🚀\n",
    "\n",
    "Lets start with setting up our clients and retrieval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "from utils.retrieval_utils import get_chroma_os_docs_collection, ChromaDBRetrievalClient\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "REGION = 'us-west-2'\n",
    "session = boto3.Session()\n",
    "bedrock = session.client(service_name='bedrock-runtime', region_name=REGION)\n",
    "\n",
    "# We've pushed the retrieval client from the prompt chaining notebook to the retrieval utils for simplicity\n",
    "chroma_os_docs_collection: ChromaDBRetrievalClient = get_chroma_os_docs_collection()\n",
    "\n",
    "print(\"✅ Client setup and retrieval client complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Helpers\n",
    "Reuse the same helpers from our previous workshops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, Dict, Any, List\n",
    "\n",
    "# We pushed the base propmt from the previous lab to a a base prompt file.\n",
    "from utils.base_prompt import BasePrompt\n",
    "from utils.retrieval_utils import RetrievalResult\n",
    "\n",
    "def call_bedrock(prompt: BasePrompt) -> str:\n",
    "    kwargs = {\n",
    "        \"modelId\": prompt.model_id,\n",
    "        \"inferenceConfig\": prompt.hyperparams,\n",
    "        \"messages\": prompt.to_bedrock_messages(),\n",
    "        \"system\": prompt.to_bedrock_system(),\n",
    "    }\n",
    "\n",
    "    # Call Bedrock\n",
    "    converse_response: Dict[str, Any] = bedrock.converse(**kwargs)\n",
    "    # Get the model's text response\n",
    "    return converse_response['output']['message']['content'][0]['text']\n",
    "\n",
    "# Helper function to call bedrock\n",
    "def do_rag(user_input: str, rag_prompt: Type[BasePrompt]) -> str:\n",
    "    # Retrieve the context from the vector store\n",
    "    retrieval_results: List[RetrievalResult] = chroma_os_docs_collection.retrieve(user_input, n_results=2)\n",
    "    # Format the context into a string\n",
    "    context: str = \"\\n\\n\".join([result.document for result in retrieval_results])\n",
    "\n",
    "    print(\"Retrieval done\")\n",
    "    # Create the RAG prompt\n",
    "    inputs: Dict[str, Any] = {\"question\": user_input, \"context\": context}\n",
    "    rag_prompt: BasePrompt = rag_prompt(inputs=inputs)\n",
    "    # Call Bedrock with the RAG prompt\n",
    "\n",
    "    print(\"Calling Bedrock\")\n",
    "    return call_bedrock(rag_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Our Troubleshooting System\n",
    "\n",
    "We'll build a system with:\n",
    "1. An orchestrator that plans diagnostic steps\n",
    "2. Workers that handle specific tasks:\n",
    "   - Issue identification\n",
    "   - Test suggestion\n",
    "   - Resolution steps\n",
    "\n",
    "First lets create our prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "from utils.base_prompt import BasePrompt\n",
    "\n",
    "# Define troubleshooting prompts inheriting from BasePrompt\n",
    "class PlanningPrompt(BasePrompt):\n",
    "    system_prompt: str = \"You are an expert OpenSearch diagnostician. Your role is to identify potential causes for issues.\"\n",
    "    user_prompt: str = \"\"\"\n",
    "    Plan the diagnostic steps needed for this OpenSearch issue:\n",
    "    {problem}\n",
    "    \n",
    "    Analyze the issue and return a list of specific potential problems \n",
    "    that could be causing this issue. Be specific and thorough.\n",
    "    \n",
    "    For example, instead of just \"configuration issues\", specify \"incorrect \n",
    "    shard allocation settings\" or \"memory allocation problems\".\n",
    "    \n",
    "    Return each potential issue as a separate line. At most return 3 potential issues.\n",
    "    \"\"\"\n",
    "\n",
    "###########################################################################################\n",
    "# Notice how we're using Amazon Nova Micro on the investor tasks\n",
    "# Micro is extremely fast so we can generate lots of content in a short amount of time. \n",
    "###########################################################################################\n",
    "\n",
    "class InvestigationPrompt(BasePrompt):\n",
    "    model_id: str = \"us.amazon.nova-micro-v1:0\"\n",
    "    system_prompt: str = \"You are an expert OpenSearch troubleshooter. Provide detailed diagnostic and resolution information.\"\n",
    "    user_prompt: str = \"\"\"\n",
    "    Regarding OpenSearch problem: \n",
    "    {question}\n",
    "    \n",
    "    Here's the context provided for the problem.\n",
    "    {context}\n",
    "    \n",
    "    Explain how to diagnose if this is the actual problem and how to fix it.\n",
    "    Include:\n",
    "    1. Diagnostic commands or API calls\n",
    "    2. Expected symptoms if this is the issue\n",
    "    3. Step-by-step resolution steps\n",
    "    4. Preventive measures\n",
    "    \"\"\"\n",
    "\n",
    "class SynthesisPrompt(BasePrompt):\n",
    "    system_prompt: str = \"You are an expert OpenSearch engineer. Create a comprehensive, well-structured troubleshooting report.\"\n",
    "    user_prompt: str = \"\"\"\n",
    "    Create a comprehensive troubleshooting report for this OpenSearch issue:\n",
    "    {problem}\n",
    "    \n",
    "    Here are the findings from our investigation of potential causes:\n",
    "    \n",
    "    {issues_summary}\n",
    "    \n",
    "    Synthesize these findings into:\n",
    "    1. Most likely root causes (ranked)\n",
    "    2. Complete diagnostic steps\n",
    "    3. Recommended resolution approach\n",
    "    4. Verification steps to confirm the fix worked\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets define our states. LangGraph handles dynamic worker creation with two separate states. The planner can use a worker with a separate state and send N number of tasks to be executed. \n",
    "\n",
    "Lastly, it uses the operator.add. This creates an aggregate of the values from the state dict so that we don't have race conditions where different threads are trying to overwrite each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Annotated, Dict\n",
    "import operator\n",
    "\n",
    "# Define the state structure to track our workflow\n",
    "class TroubleshootingState(TypedDict):\n",
    "    problem: str\n",
    "    diagnostic_plan: List[str]\n",
    "    investigation_results: Annotated[List[Dict[str, str]], operator.add]  # For parallel workers to add results\n",
    "    final_report: str\n",
    "\n",
    "# Define worker state\n",
    "class WorkerState(TypedDict):\n",
    "    problem: str\n",
    "    issue: str\n",
    "    investigation_results: Annotated[List[Dict[str, str]], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.constants import Send\n",
    "\n",
    "def plan_diagnostics(state: TroubleshootingState) -> TroubleshootingState:\n",
    "    \"\"\"Orchestrator: Plans the troubleshooting steps needed\"\"\"\n",
    "    \n",
    "    # Create the planning prompt with proper inputs\n",
    "    planning_prompt = PlanningPrompt(inputs={\"problem\": state[\"problem\"]})\n",
    "    \n",
    "    # Call bedrock using the planning prompt\n",
    "    plan = call_bedrock(planning_prompt)\n",
    "    \n",
    "    # Extract each diagnostic step as a potential issue to investigate\n",
    "    steps = [step.strip() for step in plan.split('\\n') if step.strip()]\n",
    "    \n",
    "    # Return updated state with diagnostic plan\n",
    "    return {\n",
    "        **state,\n",
    "        \"diagnostic_plan\": steps,\n",
    "        \"investigation_results\": []  # Initialize empty list for results\n",
    "    }\n",
    "\n",
    "def investigate_issue(state: WorkerState) -> WorkerState:\n",
    "    \"\"\"Worker: Uses RAG to investigate a specific potential issue\"\"\"\n",
    "    \n",
    "    # Create a more focused query for the RAG system\n",
    "    search_query = f\"OpenSearch {state['problem']} {state['issue']}\"\n",
    "    \n",
    "    # Use the do_rag helper function with the proper prompt class and inputs\n",
    "    investigation_result = do_rag(\n",
    "        search_query,  # Use this as the search query for RAG\n",
    "        InvestigationPrompt\n",
    "    )\n",
    "    \n",
    "    # Return a list with a single result dictionary to be added to the main results\n",
    "    return {\n",
    "        \"investigation_results\": [{\"issue\": state[\"issue\"], \"result\": investigation_result}]\n",
    "    }\n",
    "\n",
    "def synthesize_findings(state: TroubleshootingState) -> TroubleshootingState:\n",
    "    \"\"\"Synthesizer: Creates a unified diagnostic response\"\"\"\n",
    "    \n",
    "    # Format all investigated issues into a structured summary\n",
    "    issues_summary = \"\"\n",
    "    for result in state[\"investigation_results\"]:\n",
    "        issues_summary += f\"\\n## Issue: {result['issue']}\\n{result['result']}\\n\"\n",
    "    \n",
    "    # Create the synthesis prompt with proper inputs\n",
    "    synthesis_prompt = SynthesisPrompt(inputs={\n",
    "        \"problem\": state[\"problem\"],\n",
    "        \"issues_summary\": issues_summary\n",
    "    })\n",
    "    \n",
    "    # Generate the unified response\n",
    "    final_report = call_bedrock(synthesis_prompt)\n",
    "    \n",
    "    # Return the updated state with the final report\n",
    "    return {\n",
    "        **state,\n",
    "        \"final_report\": final_report\n",
    "    }\n",
    "\n",
    "# Create worker assignments using the Send API\n",
    "def assign_workers(state: TroubleshootingState):\n",
    "    \"\"\"Assign a worker to each diagnostic issue in parallel\"\"\"\n",
    "    \n",
    "    # Create a Send message for each issue identified by the planner\n",
    "    return [\n",
    "        Send(\"investigate_issue\", { \"problem\": state[\"problem\"], \"issue\": issue }) \n",
    "        for issue in state[\"diagnostic_plan\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "def create_troubleshooting_workflow():\n",
    "    \"\"\"Creates a parallel workflow for orchestrated troubleshooting using RAG\"\"\"\n",
    "    # Initialize the state graph with our state structure\n",
    "    workflow = StateGraph(TroubleshootingState)\n",
    "    \n",
    "    # Add nodes to our graph\n",
    "    workflow.add_node(\"plan\", plan_diagnostics)\n",
    "    workflow.add_node(\"investigate_issue\", investigate_issue)\n",
    "    workflow.add_node(\"synthesize\", synthesize_findings)\n",
    "    \n",
    "    # Connect the workflow\n",
    "    workflow.add_edge(START, \"plan\")\n",
    "    workflow.add_conditional_edges(\"plan\", assign_workers, [\"investigate_issue\"])\n",
    "    workflow.add_edge(\"investigate_issue\", \"synthesize\")\n",
    "    workflow.add_edge(\"synthesize\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Create and use the workflow\n",
    "troubleshooter = create_troubleshooting_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute our troubleshooter\n",
    "Lastly, lets pick a question and run our orchestrator against it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with our test problem\n",
    "initial_state = {\n",
    "    \"problem\": \"OpenSearch cluster not responding to queries, showing red health status\",\n",
    "    \"diagnostic_plan\": [],\n",
    "    \"investigation_results\": [],\n",
    "    \"final_report\": \"\"\n",
    "}\n",
    "\n",
    "# Run the troubleshooting workflow\n",
    "result = troubleshooter.invoke(initial_state)\n",
    "\n",
    "print(\"🔍 Troubleshooting Report Generated\\n\")\n",
    "print(result[\"final_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benefits of the Orchestrator-Workers Pattern\n",
    "\n",
    "In this lab we created an orchestrator that can dynamically assign tasks and delegate to workers in parallel.\n",
    "\n",
    "Our troubleshooting system provides several advantages:\n",
    "\n",
    "✅ Dynamic planning based on the specific problem\n",
    "\n",
    "✅ Workers allocated for each task\n",
    "\n",
    "✅ Coordinated problem-solving\n",
    "\n",
    "✅ Comprehensive troubleshooting reports\n",
    "\n",
    "Next, we'll explore how to improve our answers using the evaluator-optimizer pattern! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
