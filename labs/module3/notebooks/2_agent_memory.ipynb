{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Building Autonomous Agents: Adding Memory\n",
    "\n",
    "Welcome to the first step in building autonomous agents! In this notebook, we'll focus on expanding an LLM's capabilities by giving it memory\n",
    "\n",
    "LLMs are stateless meaning they have no idea about previous interactions. That means we need to pass in the context to the LLM each time. This is what we refer to as \"memory\". APIs like the converse API allow us to pass messages as JSON objects. Under the hood, it's converting that JSON into a conversation history that the LLM can understand.\n",
    "\n",
    "## Objectives:\n",
    "- Understand memory\n",
    "- Add a simple sliding window memory implementation\n",
    "- Introduce long term memory\n",
    "- See how memory works in tools like LangChain.\n",
    "\n",
    "Throughout this notebook, we'll use a simple ChatBot to start (based on module 1 notebook 2) and extend it to have memory.\n",
    "\n",
    "Let's begin by setting up our environment and defining the chat bot ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies. \n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "session = boto3.Session()\n",
    "bedrock = session.client(service_name='bedrock-runtime')\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Case For Abstraction\n",
    "If you remember in module 1, we built a basic chat bot. Let's create a new chat bot with conversational memory in vanilla Python code to understand the concept at their core.\n",
    "\n",
    "An important design decision to make is how much you want to rely on a single GenAI framework. The more of the core types you own, the more flexible your system becomes. Want to switch from LangChain to Pydantic AI? Easy, just write code that converts their types into yours and the rest of the system doesn't care which framework you used. These types become abstraction layers between the framework and the rest of your code. (1) this is general coding best practice, but (2) it increases flexibility. The tradeoff is that you now own more code to maintain. If there's a major re-write between V1 and V2 of a framework and the types change, you'll spend a lot of time refactoring. But if it doesn't, then you've saved yourself some time and code. Because the space isn't mature, we suggest owning your own types for now.\n",
    "\n",
    "All the major frameworks have their own implementation of session context / messages. However, we want to own our own types to create 2 way door decisions down the road that are reversable. Writing a converter is much simpler than refactoring an entire code base if you want to change your mind later. \n",
    "\n",
    "Let's start with a chat bot but extend it to use our own memory. First up is creating types for our messages and conversations. These types live in the agentic platform code (src/agent_platform). Lets look at a couple of them to see how we structured them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_platform.core.models.memory_models import Message, SessionContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Message??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SessionContext??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the main types we'll be playing with in the memory section of this notebook. As you can see, we've adoped a message and session context format mostly similar to other providers. \n",
    "\n",
    "The ChatCompletion response type has become very popular over the last two years. \n",
    "\n",
    "In 2025, it appears that the industry is shifting more to Anthropic (and Bedrock's converse) style output. OpenAI has the Response object which looks a lot more similar to Anthropic and Bedrock's types where content is a list of objects each containing a \"type\". Using content objects (or content blocks) makes streaming a bit easier too.\n",
    "\n",
    "We've opted to make our object align more closely to where we \"think\" the industry is heading using the content list. However, we've added some helper functions like get_text_content() so we aren't iterating over a list every time we want to access the results of the message. This gives us the best of both worlds as we wait for specifications to solidify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract LLM Touchpoints\n",
    "We've typed everything up to this point with the exception of the LLM Request and LLM Response. Let's go ahead and output the response types we've created in the agentic platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_platform.core.models.llm_models import LLMRequest, LLMResponse\n",
    "\n",
    "LLMRequest??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMResponse??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Converters\n",
    "Owning your types often means you need to write converters. This is a small price to pay for flexibility. Fortunately with modern AI coding assistants, writing converts is pretty painless\n",
    "\n",
    "We've created a ConverseMessageConverter class you can use to do the conversion which we'll import below. This is pretty undifferentiated work so we left it out of the lab itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Converse API converter to convert the raw JSON into our own types.\n",
    "from agentic_platform.core.converter.llm_request_converters import ConverseRequestConverter\n",
    "from agentic_platform.core.converter.llm_response_converters import ConverseResponseConverter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Bedrock Converse()\n",
    "Now that we have our types and converters, we can simplify the Bedrock calls substantially by just passing in the request object and getting the response object back. The rest of the code base doesn't need to know any specifics of the API itself because it's all abstracted away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to call Bedrock. Passing around JSON is messy and error prone.\n",
    "from typing import Dict, Any\n",
    "def call_bedrock(request: LLMRequest) -> LLMResponse:\n",
    "    kwargs: Dict[str, Any] = ConverseRequestConverter.convert_llm_request(request)\n",
    "    # Call Bedrock\n",
    "    converse_response: Dict[str, Any] = bedrock.converse(**kwargs)\n",
    "    # Get the model's text response\n",
    "    return ConverseResponseConverter.to_llm_response(converse_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Memory Client\n",
    "Lastly, we need to create a MemoryClient that can vend us conversations. In this implementation we'll be doing this locally. In a production environment you would want to swap out the memory client with one that calls a production grade database like DynamoDB or anyting else that works well with key/value pairs. \n",
    "\n",
    "Going back to the importance of abstraction, if your MemoryClient takes in a conversationId and returns a Conversation object, the rest of your code doesn't care what database, vendor, etc.. you're using. The important thing is to own your own types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryClient:\n",
    "    \"\"\"Manages conversations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.conversations: Dict[str, SessionContext] = {}\n",
    "\n",
    "    def upsert_conversation(self, conversation: SessionContext) -> bool:\n",
    "        self.conversations[conversation.session_id] = conversation\n",
    "\n",
    "        print(f'Conversation upserted: {self.conversations[conversation.session_id]}')\n",
    "\n",
    "    def get_or_create_conversation(self, conversation_id: str=None) -> SessionContext:\n",
    "        return self.conversations.get(conversation_id, SessionContext()) if conversation_id else SessionContext()\n",
    "    \n",
    "memory_client: MemoryClient = MemoryClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Chat Bot With Memory\n",
    "This is a very simple chat bot and there's no need for complex orchestration using LangGraph.\n",
    "\n",
    "For this implementation, we'll keep it simple with vanilla Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the same base prompt we've been using in the previous labs.\n",
    "from agentic_platform.core.models.prompt_models import BasePrompt\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# WE have two more agent types we'll incorporate from our platform. \n",
    "from agentic_platform.core.models.api_models import AgenticRequest, AgenticResponse\n",
    "\n",
    "class MemoryAgentPrompt(BasePrompt):\n",
    "    system_prompt: str = \"You are a helpful assistant. Respond to the user as best as you can.\"\n",
    "    user_prompt: str = \"{user_message}\"\n",
    "\n",
    "class MemoryAgent:\n",
    "\n",
    "    def __init__(self, prompt: BasePrompt):\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def call_llm(self, context: SessionContext) -> LLMResponse:\n",
    "        # Create LLM request\n",
    "        request: LLMRequest = LLMRequest(\n",
    "            system_prompt=self.prompt.system_prompt,\n",
    "            messages=context.get_messages(),\n",
    "            model_id=self.prompt.model_id,\n",
    "            hyperparams=self.prompt.hyperparams\n",
    "        )\n",
    "\n",
    "        response: LLMResponse = call_bedrock(request)\n",
    "        # Return the response.\n",
    "        return response\n",
    "\n",
    "    def invoke(self, request: AgenticRequest) -> AgenticRequest:\n",
    "        # Get or create conversation\n",
    "        context = memory_client.get_or_create_conversation(request.session_id)\n",
    "        # Add user message to conversation. Using a convenience frunction from_text()\n",
    "        # To create a message object with a content list of one text object.\n",
    "        context.add_message(request.message)\n",
    "        # Call the LLM.\n",
    "        response: LLMResponse = self.call_llm(context)\n",
    "        # Append the llms response to the conversation.\n",
    "        response_msg: Message = Message.from_text(role=\"assistant\", text=response.text)\n",
    "        context.add_message(response_msg)\n",
    "        # Save updated conversation\n",
    "        memory_client.upsert_conversation(context)\n",
    "\n",
    "        return AgenticResponse(\n",
    "            message=response_msg,\n",
    "            session_id=context.session_id\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Chat Bot\n",
    "Now that we have a chat bot, lets invoke it. Reiterating the importance of abstraction, we'll create our own Message type and convert it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to construct request\n",
    "def construct_request(user_message: str, conversation_id: str=None) -> AgenticRequest:\n",
    "    return AgenticRequest.from_text(\n",
    "        text=user_message, \n",
    "        **{'session_id': conversation_id}\n",
    "    )\n",
    "\n",
    "agent: MemoryAgent = MemoryAgent(MemoryAgentPrompt()) \n",
    "\n",
    "# Invoke the agent\n",
    "request: AgenticRequest = construct_request(\"Hello!\")\n",
    "print(request)\n",
    "agent.invoke(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test it with a multi-turn conversation. The first response will return a conversationId. In the second turn, we'll pass in that conversation Id to let our code know it needs to send in the past conversation as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent: MemoryAgent = MemoryAgent(MemoryAgentPrompt()) \n",
    "\n",
    "user_message: str = \"Hello, can you quickly tell me why the sky is blue? One sentence is fine.\"\n",
    "request: AgenticRequest = construct_request(user_message)\n",
    "\n",
    "response: AgenticRequest = agent.invoke(request)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have a response and now a new conversation_id as well. Lets check out what the conversation history looks like in our memory \"store\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation: SessionContext = memory_client.get_or_create_conversation(response.session_id)\n",
    "from pydantic_core import to_jsonable_python\n",
    "\n",
    "# Use the pydantic model_dump_json. The content list uses a base class so we need to serialize as any to see specific subclass attributes.\n",
    "print(conversation.model_dump_json(indent=2, serialize_as_any=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test with multi turn. We'll pass in the conversationId from the response object and ask the model what we were talking about about previously. The response should tell us that we were asking why the sky is blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_id: str = response.session_id\n",
    "\n",
    "# Construct a new request with the conversationId\n",
    "request: AgenticRequest = construct_request('What were we talking about again?', conversation_id)\n",
    "\n",
    "# Invoke the agent\n",
    "response: AgenticRequest = agent.invoke(request)\n",
    "print('--- Models response ---')\n",
    "print(response.model_dump_json(indent=2, serialize_as_any=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What did we just do?\n",
    "Congrats! We successfully added short term memory to our agent! This allows conversations to flow more freely and give us our first LLM augmented component on our path to building an agent!\n",
    "\n",
    "Next, we'll explore how to add tool use to this agent ðŸš€\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-program-technical-assets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
