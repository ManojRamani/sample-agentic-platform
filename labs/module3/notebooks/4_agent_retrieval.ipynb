{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Building Autonomous Agents: Adding Retrieval\n",
    "\n",
    "Welcome to the next step in building autonomous agents! In this notebook, we'll focus on expanding an LLM's knowledge by allowing it to query a knowledge base filled with OpenSearch's Documentation.\n",
    "\n",
    "In a typical Retrieval Augmented Generation (RAG) system, you programmatically query the knowledge base and pass it in as context into the LLM. We'll be doing something very similar here with one cavaet, the model is deciding whether to query the knowledge base through the use of a function call. \n",
    "\n",
    "## Objectives:\n",
    "- Extend the existing agent we've built to query our chromaDB instance we've been using through this lab. \n",
    "- Register the retrieval tool so that the agent can decide to query the knowledge base\n",
    "- Observe how the agent can use retrieval to solve problems\n",
    "\n",
    "At the end of this module, we'll have a fully fledged agent complete with memory, retrieval, and tools\n",
    "\n",
    "Let's begin by setting up our environment and importing all the types, clients, and converters we've built so far. ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets import all the types, clients, and converters we've used so far.\n",
    "\n",
    "from agentic_platform.core.models.prompt_models import BasePrompt\n",
    "from agentic_platform.core.models.memory_models import Message, ToolResult, ToolCall\n",
    "from agentic_platform.core.models.llm_models import LLMResponse, LLMRequest\n",
    "from agentic_platform.core.models.tool_models import ToolSpec\n",
    "\n",
    "# Import our decorator for tools like we discussed in the previous lab. \n",
    "from agentic_platform.core.decorator.toolspec_decorator import tool_spec\n",
    "\n",
    "# import our sample tools from the tool lab.\n",
    "from agentic_platform.core.tool.sample_tools import weather_report, handle_calculation\n",
    "\n",
    "# Import our chroma client we've been using throughout all the modules.\n",
    "# If you have not gone through the setup.ipynb file, please do so before continuing \n",
    "from utils.retrieval_client import get_chroma_os_docs_collection, RetrievalResult, ChromaDBRetrievalClient\n",
    "\n",
    "\n",
    "# Import Converse API converter to convert the raw JSON into our own types.\n",
    "from agentic_platform.core.converter.llm_request_converters import ConverseRequestConverter\n",
    "from agentic_platform.core.converter.llm_response_converters import ConverseResponseConverter\n",
    "from agentic_platform.core.models.llm_models import LLMRequest, LLMResponse\n",
    "from typing import Dict, Any\n",
    "import boto3\n",
    "\n",
    "bedrock = boto3.client('bedrock-runtime')\n",
    "# Helper function to call Bedrock. Passing around JSON is messy and error prone.\n",
    "def call_bedrock(request: LLMRequest) -> LLMResponse:\n",
    "    kwargs: Dict[str, Any] = ConverseRequestConverter.convert_llm_request(request)\n",
    "    # Call Bedrock\n",
    "    converse_response: Dict[str, Any] = bedrock.converse(**kwargs)\n",
    "    # Get the model's text response\n",
    "    return ConverseResponseConverter.to_llm_response(converse_response)\n",
    "\n",
    "chroma_client = get_chroma_os_docs_collection()\n",
    "print(\"Make sure our chroma client is working\")\n",
    "print(len(chroma_client.retrieve(query_text=\"How do I install OpenSearch on AWS?\", n_results=1)) == 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Knowledge Base Client wrapper\n",
    "Lets create some types around the knowledge base retrieval and implement our client. We're using the VectorSearchRequest and Response from the agentic platform here. Even though it's geared towards a more complex search, we can still use it for our simple ChromaDB implementation. We're using an abstraction layer between the actual db and the rest of the code so swapping different DBs becomes trivial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "from agentic_platform.core.models.vectordb_models import VectorSearchRequest, VectorSearchResponse, VectorSearchResult, FilterCondition\n",
    "\n",
    "# Lets see what the request looks like. \n",
    "VectorSearchRequest??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple VectorSearch client that abstracts away which vector DB we're using.\n",
    "class VectorSearchClient:\n",
    "    \"\"\"Client for vector search.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.client: ChromaDBRetrievalClient = get_chroma_os_docs_collection()\n",
    "\n",
    "    def search(self, request: VectorSearchRequest) -> VectorSearchResult:\n",
    "        response: List[RetrievalResult] = self.client.retrieve(request.query, n_results=1)\n",
    "        # Create a VectorSearchResult for each retrieval result\n",
    "\n",
    "        results: List[VectorSearchResult] = []\n",
    "        for result in response:\n",
    "            results.append(VectorSearchResult(text=result.document, score=result.distance, metadata=result.metadata))\n",
    "        \n",
    "        \n",
    "        return VectorSearchResponse(results=results)\n",
    "    \n",
    "vector_search_client: VectorSearchClient = VectorSearchClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Retrieval Tool. \n",
    "There are a couple ways to implement this, but the simplest is to build out retrieval as a tool that essentially just does RAG. You can pass the retrieval results directly back into the model but it generally works better over multi turn conversations to make a 1 off call to an LLM to summarize the results. It keeps the conversation token usage down on multi-turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a RAG bot. You are given a query and context. Your job is to answer the query using ONLY the context provided.\"\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "For the users query:\n",
    "<query>\n",
    "{user_message}\n",
    "</query>\n",
    "\n",
    "And context below\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the query using ONLY the context provided. Avoid saying \"according to the context provided\" or anything similar.\n",
    "Be very direct and straight forward with your answer. It's returning to an agent, not a human.\n",
    "\"\"\"    \n",
    "\n",
    "class RAGPrompt(BasePrompt):\n",
    "    system_prompt: str = SYSTEM_PROMPT\n",
    "    user_prompt: str = USER_PROMPT\n",
    "\n",
    "class RagInput(BaseModel):\n",
    "    query_text: str\n",
    "\n",
    "def retrieve_and_answer(input: RagInput) -> str:\n",
    "    \"\"\"Search the knowledge base for relevant information based on a query.\"\"\"\n",
    "    # Perform the search\n",
    "    response: VectorSearchResponse = vector_search_client.search(VectorSearchRequest(query=input.query_text))\n",
    "    # Aggregate the results into a single string\n",
    "    context: str = \"\\n\".join([result.text for result in response.results])\n",
    "    # Create the RAG prompt\n",
    "    rag_prompt: RAGPrompt = RAGPrompt(inputs={\"user_message\": input.query_text, \"context\": context})\n",
    "    # Build out the LLMRequest.\n",
    "    llm_request: LLMRequest = LLMRequest(\n",
    "        system_prompt=rag_prompt.system_prompt,\n",
    "        messages=[Message(role=\"user\", text=rag_prompt.user_prompt)],\n",
    "        model_id=rag_prompt.model_id,\n",
    "        hyperparams=rag_prompt.hyperparams\n",
    "    )\n",
    "    # Call Bedrock through the client.\n",
    "    rag_response: LLMResponse = call_bedrock(llm_request)  \n",
    "    # Return the results \n",
    "    return rag_response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tool_result: str = retrieve_and_answer(RagInput(query_text='How does the aggregate function work in OpenSearch?'))\n",
    "\n",
    "test_tool_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now that we have our RAG tool, let's add it to our agent. \n",
    "\n",
    "We can actually reuse the same agent from our previous workshop and just import it / add the extra tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse from the previous lab.\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Import our agent request and response types.\n",
    "from agentic_platform.core.models.api_models import AgenticRequest, AgenticResponse\n",
    "from agentic_platform.core.models.memory_models import TextContent\n",
    "from agentic_platform.core.models.memory_models import SessionContext\n",
    "\n",
    "\n",
    "# Clients.\n",
    "class MemoryClient:\n",
    "    \"\"\"Manages conversations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.conversations: Dict[str, SessionContext] = {}\n",
    "\n",
    "    def upsert_conversation(self, conversation: SessionContext) -> bool:\n",
    "        self.conversations[conversation.session_id] = conversation\n",
    "\n",
    "    def get_or_create_conversation(self, conversation_id: str=None) -> SessionContext:\n",
    "        return self.conversations.get(conversation_id, SessionContext()) if conversation_id else SessionContext()\n",
    "\n",
    "# Lets reuse this from the previous lab.\n",
    "memory_client: MemoryClient = MemoryClient()\n",
    "\n",
    "# Create a prompt with our system and user messages.\n",
    "class AgentPrompt(BasePrompt):\n",
    "    system_prompt: str = \"You are a helpful assistant. You are given tools to help you accomplish your task. You can choose to use them or not.\"\n",
    "    user_prompt: str = \"{user_message}\"\n",
    "\n",
    "\n",
    "class ToolCallingAgent:\n",
    "    # This is new, we're adding tools in the constructor to bind them to the agent.\n",
    "    # Don't get too attached to this idea, it'll change as we get into MCP.\n",
    "    def __init__(self, tools: List[ToolSpec], prompt: BasePrompt):\n",
    "        self.tools: List[ToolSpec] = tools\n",
    "        self.conversation: SessionContext = SessionContext()\n",
    "        self.prompt: BasePrompt = prompt\n",
    "\n",
    "    def call_llm(self) -> LLMResponse:\n",
    "        # Create LLM request\n",
    "        request: LLMRequest = LLMRequest(\n",
    "            system_prompt=self.prompt.system_prompt,\n",
    "            messages=self.conversation.get_messages(),\n",
    "            model_id=self.prompt.model_id,\n",
    "            hyperparams=self.prompt.hyperparams,\n",
    "            tools=self.tools\n",
    "        )\n",
    "\n",
    "        # Call the LLM.\n",
    "        response: LLMResponse = call_bedrock(request)\n",
    "        # Append the llms response to the conversation.\n",
    "        self.conversation.add_message(Message(\n",
    "            role=\"assistant\",\n",
    "            text=response.text,\n",
    "            tool_calls=response.tool_calls\n",
    "        ))\n",
    "        # Return the response.\n",
    "        return response\n",
    "    \n",
    "    def execute_tools(self, llm_response: LLMResponse) -> List[ToolResult]:\n",
    "        \"\"\"Call tools and return the results.\"\"\"\n",
    "        # It's possible that the model will call multiple tools.\n",
    "        tool_results: List[ToolResult] = []\n",
    "        # Iterate over the tool calls and call the tool.\n",
    "        for tool_invocation in llm_response.tool_calls:\n",
    "            # Get the tool spec for the tool call.\n",
    "            tool: ToolSpec = next((t for t in self.tools if t.name == tool_invocation.name), None)\n",
    "            # Call the tool.\n",
    "            input_data: BaseModel = tool.model.model_validate(tool_invocation.arguments)\n",
    "            function_result: str = str(tool.function(input_data))\n",
    "            tool_response: ToolResult = ToolResult(\n",
    "                id=tool_invocation.id,\n",
    "                content=[TextContent(text=function_result)],\n",
    "                isError=False\n",
    "            )\n",
    "\n",
    "            print(f\"Tool response: {tool_response}\")\n",
    "\n",
    "            # Add the tool result to the list.\n",
    "            tool_results.append(tool_response)\n",
    "\n",
    "        # Add the tool results to the conversation\n",
    "        message: Message = Message(role=\"user\", tool_results=tool_results)\n",
    "        self.conversation.add_message(message)\n",
    "        \n",
    "        # Return the tool results even though we don't use it.\n",
    "        return tool_results\n",
    "        \n",
    "    def invoke(self, request: AgenticRequest) -> AgenticResponse:\n",
    "        # Get or create conversation\n",
    "        self.conversation = memory_client.get_or_create_conversation(request.session_id)\n",
    "        # Add user message to conversation\n",
    "        self.conversation.add_message(request.message)\n",
    "\n",
    "        # Keep calling LLM until we get a final response\n",
    "        while True:\n",
    "            # Call the LLM\n",
    "            response: LLMResponse = self.call_llm()\n",
    "            \n",
    "            # If the model wants to use tools\n",
    "            if response.stop_reason == \"tool_use\":\n",
    "                # Execute the tools\n",
    "                self.execute_tools(response)\n",
    "                # Continue the loop to get final response\n",
    "                continue\n",
    "            \n",
    "            # If we get here, it's a final response \n",
    "            break\n",
    "\n",
    "        # Save updated conversation\n",
    "        memory_client.upsert_conversation(self.conversation)\n",
    "\n",
    "        # Return our own type.\n",
    "        return AgenticResponse(\n",
    "            message=self.conversation.messages[-1],\n",
    "            session_id=self.conversation.session_id\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add retrieval to our agent\n",
    "We'll create a new instantiation of our agent and bind the retrieval tool to it. We'll ask a question related to open search and see if the model routes correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_platform.core.tool.sample_tools import weather_report, handle_calculation, WeatherReportInput, Calculator\n",
    "# Helper to construct request\n",
    "def construct_request(user_message: str, conversation_id: str=None) -> AgenticRequest:\n",
    "    return AgenticRequest.from_text(\n",
    "        text=user_message, \n",
    "        **{'session_id': conversation_id}\n",
    "    )\n",
    "tools = [\n",
    "    ToolSpec(\n",
    "        name=\"WeatherReport\",\n",
    "        description=\"Useful for getting the weather in a given location\",\n",
    "        function=weather_report,\n",
    "        model=WeatherReportInput\n",
    "    ),\n",
    "    ToolSpec(\n",
    "        name=\"Calculator\",\n",
    "        description=\"Useful for calculating the result of a mathematical operation\",\n",
    "        function=handle_calculation,\n",
    "        model=Calculator\n",
    "    ),\n",
    "    ToolSpec(\n",
    "        name=\"RetrieveAndAnswer\",\n",
    "        description=\"Useful for retrieving and answering questions about the OpenSearch documentation\",\n",
    "        function=retrieve_and_answer,\n",
    "        model=RagInput\n",
    "    )\n",
    "]\n",
    "\n",
    "agent: ToolCallingAgent = ToolCallingAgent(\n",
    "    tools=tools,\n",
    "    prompt=AgentPrompt()\n",
    ")\n",
    "\n",
    "# Invoke the agent\n",
    "user_message: str = \"What is 7+7?\"\n",
    "request: AgenticRequest = construct_request(user_message)\n",
    "response: AgenticResponse = agent.invoke(request)\n",
    "\n",
    "# Print the response\n",
    "print(response.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What did we just do?\n",
    "We essentially built a ReACT based agent using tools in a while loop augmenting the LLM with retrieval, memory, and tools.\n",
    "\n",
    "While this was a great excercise. Building a ReACT agent is kind of undifferentiated at this point. There's lots of great frameworks out there that can do it for you. With the right level of abstraction you just use an existing framework and let it do the heavy lifting for you. \n",
    "\n",
    "Next, we'll explore using agent frameworks to do similar things ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-program-technical-assets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
