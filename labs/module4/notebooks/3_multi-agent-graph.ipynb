{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Systems: Orchestrating Agents Through Graphs\n",
    "\n",
    "Welcome to this multi-agent lab! Here we'll explore how to orchestrate multiple agents using a graph-based approach, allowing for more flexible and controlled agent interactions.\n",
    "\n",
    "## Why Use Graph-Based Orchestration?\n",
    "While individual specialized agents excel at specific tasks, the way they communicate and collaborate significantly impacts system performance. Graph-based orchestration provides precise control over agent interactions, allowing for more complex workflows and better resource management.\n",
    "\n",
    "## What We'll Build\n",
    "We'll create the same deep research agent system as before, but with an improved orchestration approach:\n",
    "\n",
    "* Our supervisor will become an evaluator\n",
    "* A Researcher agent that has access to the internet and can research topics\n",
    "* A Writer agent that can take the information and create a report\n",
    "\n",
    "This time, we'll enhance our implementation with:\n",
    "* Reference passing between agents - allowing efficient sharing of information\n",
    "* Structured controls to manage agent interactions - limiting how often agents can call each other\n",
    "* Graph-based workflow patterns that enable more complex execution paths\n",
    "\n",
    "At the end of this module, you'll have a more flexible \"Deep Research\" agentic system with better orchestration capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Tavily MCP Server\n",
    "For the sake of variety, we'll use the Tavily MCP on our research agent to see how these pieces come together. We need the API key in order for it to work so lets import that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "# Check if you created the .env file before running this notebook.\n",
    "print('Does the .env file exist?', os.path.exists('.env'))\n",
    "# from dotenv import load_dotenv\n",
    "load_dotenv('.env')\n",
    "\n",
    "TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we wrap our own MCP server in a MCPServerStdio object\n",
    "from pydantic_ai.mcp import MCPServerStdio as PyAIMCPServerStdio\n",
    "\n",
    "# You'll need to have NPM installed to run this command.\n",
    "tavily_mcp_server = PyAIMCPServerStdio(  \n",
    "    command = 'npx',\n",
    "    args=['-y', 'tavily-mcp@0.1.4'],\n",
    "    env= {\n",
    "        'TAVILY_API_KEY': TAVILY_API_KEY\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next lets create our researcher agent. \n",
    "from pydantic_ai import Agent as PyAIAgent\n",
    "from agentic_platform.core.models.prompt_models import BasePrompt\n",
    "\n",
    "RESEARCHER_SYSTEM_PROMPT = \"\"\"\n",
    "You are a specialized Research Agent with web search capabilities. Your role is to:\n",
    "\n",
    "1. Analyze user queries and construct a question to query the internet with. \n",
    "2. Organize findings into comprehensive, well-sourced research briefs\n",
    "3. Return the research brief in a well structured format that a writer can use to write a report.\n",
    "4. Make sure to cite your sources with links in markdown format at the bottom of the research brief.\n",
    "\n",
    "Provide only the research based of your web search results in a format that a writer can use to write a report.\n",
    "Make sure to cite your sources with links in markdown format at the bottom of the research brief.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "Here is the user query:\n",
    "{user_query}\n",
    "\n",
    "Please provide a research brief based on the user query.\n",
    "\"\"\"\n",
    "\n",
    "class ResearcherPrompt(BasePrompt):\n",
    "    system_prompt: str = RESEARCHER_SYSTEM_PROMPT\n",
    "    user_prompt: str = USER_PROMPT\n",
    "\n",
    "\n",
    "# Add the MCP Server params to the agent.\n",
    "researcher_agent: PyAIAgent = PyAIAgent(\n",
    "    'bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0',\n",
    "    system_prompt=ResearcherPrompt().system_prompt,\n",
    "    mcp_servers=[tavily_mcp_server]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "# We need to apply the nest_asyncio to run the agent.\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the researcher agent. \n",
    "async def run_pydantic_ai_mcp_client(user_msg: str,agent: PyAIAgent):\n",
    "    async with agent.run_mcp_servers():\n",
    "        result = await agent.run(user_msg)\n",
    "    return result\n",
    "\n",
    "# Run the agent with a user message.\n",
    "user_msg = 'Can you show me any weather alerts for California?'\n",
    "researcher_results = asyncio.run(run_pydantic_ai_mcp_client(user_msg, researcher_agent))\n",
    "print(researcher_results.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Our Writer Agent\n",
    "Next lets recreate our writer agent just like we did in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt using our BasePrompt class. \n",
    "WRITER_SYSTEM_PROMPT = \"\"\"\n",
    "You are a specialized Writer Agent responsible for crafting polished, cohesive reports from research provided by the Research Agent. Your role is to:\n",
    "\n",
    "1. Transform the short research brief into a comprehensive report.\n",
    "2. Organize information logically with appropriate sections and flow\n",
    "3. Maintain a professional, authoritative tone appropriate for the subject matter\n",
    "4. Ensure clarity, conciseness, and readability for the target audience\n",
    "\n",
    "You will be provided with comprehensive research materials that include facts, figures, and sourced information. Your job is to synthesize this information without altering facts or adding unsupported claims.\n",
    "\n",
    "Please use complete sentenences and paragraphs. No bullet points. Break up the report into section with headers with the following format:\n",
    "Title:\n",
    "[Title of the report]\n",
    "\n",
    "Section 1:\n",
    "[Section 1 of the report]\n",
    "\n",
    "Conclusion:\n",
    "[Conclusion of the report]\n",
    "\"\"\"\n",
    "\n",
    "WRITER_USER_PROMPT = \"\"\"\n",
    "Here is the research brief:\n",
    "{research_brief}\n",
    "\n",
    "Please write a report based on the research brief.\n",
    "\"\"\"\n",
    "\n",
    "class WriterPrompt(BasePrompt):\n",
    "    system_prompt: str = WRITER_SYSTEM_PROMPT\n",
    "    user_prompt: str = WRITER_USER_PROMPT\n",
    "\n",
    "# Lets use a really fast model for the writer agent.\n",
    "writer_agent: PyAIAgent = PyAIAgent(\n",
    "    'bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0',\n",
    "    system_prompt=WriterPrompt().system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Our Evaluator(s)\n",
    "Lastly we need to modify the supervisor agent to be more of an \"evaluator\". Because it's not entirely orchestrating these agents, it doesn't really need to be an agent. It can be simple evaluation prompt calling Bedrock to evaluate the research and then evaluate the writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prompt for evaluating research quality with chain of thought\n",
    "RESEARCH_EVALUATOR_SYSTEM_PROMPT = \"\"\"\n",
    "You are a Research Evaluator responsible for assessing the quality and completeness of research results. Your role is to:\n",
    "\n",
    "1. Analyze user queries to understand their information needs\n",
    "2. Review research outputs for accuracy, thoroughness, and relevance\n",
    "3. Ensure sources are properly documented and credible\n",
    "4. Provide a clear assessment with a final decision: APPROVE or REVISE\n",
    "\n",
    "First, think through your evaluation step by step within <thinking> tags. Consider these criteria:\n",
    "- Accuracy: Does the research contain factual errors or misinterpretations?\n",
    "- Completeness: Does it fully address all aspects of the user's query?\n",
    "- Relevance: Is the information directly relevant to the query?\n",
    "- Sources: Are sources properly documented and credible?\n",
    "\n",
    "After your analysis, provide your final decision within <evaluation> tags, which must be ONLY either APPROVE or REVISE (no other text inside these tags).\n",
    "\n",
    "If APPROVE, follow the tags with a brief justification.\n",
    "If REVISE, follow the tags with specific improvement suggestions for the Research Agent.\n",
    "\n",
    "Format your response as:\n",
    "<thinking>\n",
    "Your step-by-step analysis of the research...\n",
    "</thinking>\n",
    "\n",
    "<evaluation>APPROVE</evaluation> or <evaluation>REVISE</evaluation>\n",
    "\n",
    "Followed by your justification or improvement suggestions.\n",
    "\n",
    "Limit evaluation cycles to prevent excessive iterations - accept reasonable quality research rather than pursuing perfection.\n",
    "\"\"\"\n",
    "\n",
    "RESEARCH_EVALUATOR_USER_PROMPT = \"\"\"\n",
    "Please analyze the following research results for the query: {query}\n",
    "\n",
    "RESEARCH CONTENT:\n",
    "{research_content}\n",
    "\n",
    "Provide your step-by-step evaluation using the thinking and evaluation tags.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt for evaluating the final report with chain of thought\n",
    "REPORT_EVALUATOR_SYSTEM_PROMPT = \"\"\"\n",
    "You are a Report Evaluator responsible for assessing the quality and effectiveness of final reports. Your role is to:\n",
    "\n",
    "1. Compare the final report against the original research to ensure accuracy\n",
    "2. Evaluate the report's structure, coherence, and readability\n",
    "3. Assess whether the report effectively communicates the key findings\n",
    "4. Provide a clear assessment with a final decision: APPROVE or REVISE\n",
    "\n",
    "First, think through your evaluation step by step within <thinking> tags. Consider these criteria:\n",
    "- Accuracy: Does the report faithfully represent the research without distortion?\n",
    "- Completeness: Does it include all key information from the research?\n",
    "- Coherence: Is the information organized logically with good flow?\n",
    "- Readability: Is it written in clear, professional language appropriate for the audience?\n",
    "\n",
    "After your analysis, provide your final decision within <evaluation> tags, which must be ONLY either APPROVE or REVISE (no other text inside these tags).\n",
    "\n",
    "If APPROVE, follow the tags with a brief justification.\n",
    "If REVISE, follow the tags with specific improvement suggestions for the Writer Agent.\n",
    "\n",
    "Format your response as:\n",
    "<thinking>\n",
    "Your step-by-step analysis of the report compared to the research...\n",
    "</thinking>\n",
    "\n",
    "<evaluation>APPROVE</evaluation> or <evaluation>REVISE</evaluation>\n",
    "\n",
    "Followed by your justification or improvement suggestions.\n",
    "\n",
    "Limit evaluation cycles to prevent excessive iterations - accept reasonable quality reports rather than pursuing perfection.\n",
    "\"\"\"\n",
    "\n",
    "REPORT_EVALUATOR_USER_PROMPT = \"\"\"\n",
    "Please analyze the following report for the query: {query}\n",
    "\n",
    "ORIGINAL RESEARCH:\n",
    "{research_content}\n",
    "\n",
    "FINAL REPORT:\n",
    "{report_content}\n",
    "\n",
    "Provide your step-by-step evaluation using the thinking and evaluation tags.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ResearchEvaluatorPrompt(BasePrompt):\n",
    "    system_prompt: str = RESEARCH_EVALUATOR_SYSTEM_PROMPT\n",
    "    user_prompt: str = RESEARCH_EVALUATOR_USER_PROMPT\n",
    "\n",
    "class ReportEvaluatorPrompt(BasePrompt):\n",
    "    system_prompt: str = REPORT_EVALUATOR_SYSTEM_PROMPT\n",
    "    user_prompt: str = REPORT_EVALUATOR_USER_PROMPT\n",
    "\n",
    "# Create the prompt instances\n",
    "research_evaluator_prompt: BasePrompt = ResearchEvaluatorPrompt()\n",
    "report_evaluator_prompt: BasePrompt = ReportEvaluatorPrompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Helper Function\n",
    "Lets write a helper function for our Graph that can generate structured from our prompts for our graph. This will give us our evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_platform.core.models.memory_models import Message, ToolCall\n",
    "from agentic_platform.core.models.tool_models import ToolSpec\n",
    "from agentic_platform.core.converter.llm_request_converters import ConverseRequestConverter\n",
    "from agentic_platform.core.converter.llm_response_converters import ConverseResponseConverter\n",
    "from agentic_platform.core.models.llm_models import LLMRequest, LLMResponse\n",
    "from typing import Dict, Any\n",
    "import boto3\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "# Helper function to call Bedrock. Passing around JSON is messy and error prone.\n",
    "bedrock = boto3.client('bedrock-runtime')\n",
    "def call_bedrock(request: LLMRequest) -> LLMResponse:\n",
    "    kwargs: Dict[str, Any] = ConverseRequestConverter.convert_llm_request(request)\n",
    "    # Call Bedrock\n",
    "    converse_response: Dict[str, Any] = bedrock.converse(**kwargs)\n",
    "    # Get the model's text response\n",
    "    return ConverseResponseConverter.to_llm_response(converse_response)\n",
    "\n",
    "class EvalResults(BaseModel):\n",
    "    thinking: str\n",
    "    evaluation: Literal[\"APPROVE\", \"REVISE\"]\n",
    "\n",
    "# This will force a tool call allowing us to get structured output.\n",
    "def run_evaluation(prompt: BasePrompt) -> EvalResults:\n",
    "    structured_output = ToolSpec(\n",
    "        name=\"evaluate\",\n",
    "        description=\"Useful for thinking about and evaluating research brifs or research documents\",\n",
    "        model=EvalResults\n",
    "    )\n",
    "\n",
    "    request: LLMRequest = LLMRequest(\n",
    "        system_prompt=prompt.system_prompt,\n",
    "        messages=[Message(role=\"user\", text=prompt.user_prompt)],\n",
    "        model_id=prompt.model_id,\n",
    "        hyperparams=prompt.hyperparams,\n",
    "        tools=[structured_output],\n",
    "        force_tool=structured_output.name\n",
    "    )\n",
    "\n",
    "    response: LLMResponse = call_bedrock(request)\n",
    "    tool_invocation: ToolCall = response.tool_calls[0]\n",
    "    return EvalResults.model_validate(tool_invocation.arguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "# Test the evaluator\n",
    "inputs: Dict[str, str] = {\n",
    "    \"research_content\": researcher_results.data,\n",
    "    \"query\": user_msg\n",
    "}\n",
    "prompt: BasePrompt = ResearchEvaluatorPrompt(inputs=inputs)\n",
    "run_evaluation(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Our Graph\n",
    "If you remember from module2, we built a series of graphs for our workflow agents. We will be doing something similar here as well but with pydantic-graph. Pydantic Graph has no dependencies on PydanticAI and can be run standalone. It offers similar functionality as LangGraph. \n",
    "\n",
    "The benefit of Pydantic graphs is that they're type safe. The downside is that they're a bit more complicated.\n",
    "\n",
    "## Pydantic Graph Crash Course / tl;dr\n",
    "Pydantic-graph creates workflow graphs where nodes are Python dataclasses that inherit from BaseNode. Each node implements a run method that returns another node or an End object, creating a directed flow. The GraphRunContext maintains state throughout execution and can inject dependencies. Nodes use Python type hints to define valid transitions between states, making the workflow type-safe. The library supports visualizing graphs with Mermaid diagrams, can persist state between runs (allowing workflows to be paused and resumed), and works well with PydanticAI for GenAI applications, though it can be used independently. \n",
    "\n",
    "The full documentation can be found [here](https://ai.pydantic.dev/graph/#graph). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "\n",
    "from pydantic_graph import BaseNode, End, Graph, GraphRunContext, Edge\n",
    "\n",
    "# State class to track the flow\n",
    "@dataclass\n",
    "class ResearchFlowState:\n",
    "    user_query: str\n",
    "    research_results: str = \"\"\n",
    "    research_feedback: str = \"\"\n",
    "    final_report: str = \"\"\n",
    "    report_feedback: str = \"\"\n",
    "    attempts: int = 0\n",
    "    max_attempts: int = 3\n",
    "\n",
    "# Graph nodes\n",
    "@dataclass\n",
    "class ResearchQuery(BaseNode[ResearchFlowState]):\n",
    "    \"\"\"Processes user query through Research Agent\"\"\"\n",
    "    async def run(self, ctx: GraphRunContext[ResearchFlowState]) -> EvaluateResearch:\n",
    "        print(\"Running Research Query\")\n",
    "        query = ctx.state.user_query\n",
    "        if ctx.state.research_feedback:\n",
    "            query = f\"{query}\\n\\nPrevious feedback: {ctx.state.research_feedback}\"\n",
    "            \n",
    "        # Run researcher agent\n",
    "        async with researcher_agent.run_mcp_servers():\n",
    "            result = await researcher_agent.run(query)\n",
    "        \n",
    "        ctx.state.research_results = result.data\n",
    "        return EvaluateResearch()\n",
    "\n",
    "@dataclass\n",
    "class EvaluateResearch(BaseNode[ResearchFlowState]):\n",
    "    \"\"\"Evaluates research quality using direct LLM call\"\"\"\n",
    "    async def run(self, ctx: GraphRunContext[ResearchFlowState]) -> WriteReport | ResearchQuery:\n",
    "        print(\"Evaluating Research\")\n",
    "        ctx.state.attempts += 1\n",
    "\n",
    "        inputs: Dict[str, str] = {\n",
    "            \"research_content\": ctx.state.research_results,\n",
    "            \"query\": ctx.state.user_query\n",
    "        }\n",
    "\n",
    "        # Run the evaluation\n",
    "        prompt: BasePrompt = ResearchEvaluatorPrompt(inputs=inputs)\n",
    "        feedback: EvalResults = run_evaluation(prompt)\n",
    "        \n",
    "        ctx.state.research_feedback = feedback.thinking\n",
    "        \n",
    "        # Simple decision logic\n",
    "        if feedback.evaluation == \"APPROVE\" or ctx.state.attempts >= ctx.state.max_attempts:\n",
    "            ctx.state.attempts = 0  # Reset for next phase\n",
    "            return WriteReport()\n",
    "        else:\n",
    "            return ResearchQuery()\n",
    "\n",
    "@dataclass\n",
    "class WriteReport(BaseNode[ResearchFlowState]):\n",
    "    \"\"\"Creates report from research\"\"\"\n",
    "    async def run(self, ctx: GraphRunContext[ResearchFlowState]) -> EvaluateReport:\n",
    "        print(\"Writing Report\")\n",
    "        write_input = ctx.state.research_results\n",
    "        if ctx.state.report_feedback:\n",
    "            write_input = f\"{write_input}\\n\\nPrevious feedback: {ctx.state.report_feedback}\"\n",
    "            \n",
    "        # Call writer agent\n",
    "        result = await writer_agent.run(write_input)\n",
    "        ctx.state.final_report = result.data\n",
    "        return EvaluateReport()\n",
    "\n",
    "@dataclass\n",
    "class EvaluateReport(BaseNode[ResearchFlowState]):\n",
    "    \"\"\"Evaluates final report using direct LLM call\"\"\"\n",
    "    async def run(self, ctx: GraphRunContext[ResearchFlowState]) -> End[str] | WriteReport:\n",
    "        print(\"Evaluating Report\")\n",
    "        ctx.state.attempts += 1\n",
    "        \n",
    "        inputs: Dict[str, str] = {\n",
    "            \"research_content\": ctx.state.research_results,\n",
    "            \"query\": ctx.state.user_query,\n",
    "            \"report_content\": ctx.state.final_report\n",
    "        }\n",
    "        \n",
    "        # Run the evaluation\n",
    "        prompt: BasePrompt = ReportEvaluatorPrompt(inputs=inputs)\n",
    "        feedback: EvalResults = run_evaluation(prompt)\n",
    "        \n",
    "        ctx.state.report_feedback = feedback.thinking\n",
    "        \n",
    "        # Simple decision logic\n",
    "        if feedback.evaluation == \"APPROVE\" or ctx.state.attempts >= ctx.state.max_attempts:\n",
    "            return End(ctx.state.final_report)\n",
    "        else:\n",
    "            return WriteReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the graph\n",
    "research_graph = Graph(nodes=[ResearchQuery, EvaluateResearch, WriteReport, EvaluateReport])\n",
    "\n",
    "# Function to run the graph\n",
    "async def process_query(user_query, max_attempts=3):\n",
    "    state = ResearchFlowState(user_query=user_query, max_attempts=max_attempts)\n",
    "    result = await research_graph.run(ResearchQuery(), state=state)\n",
    "    return result.output\n",
    "\n",
    "# Sync wrapper\n",
    "def run_query(user_query, max_attempts=3):\n",
    "    return asyncio.run(process_query(user_query, max_attempts))\n",
    "\n",
    "# Example usage\n",
    "final_report = run_query(\"What are the differences between the US and EU around pasturization of milk? \")\n",
    "print(final_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Graph\n",
    "Like LangGraph, we can visualize the graph using Mermaid diagrams. Lets take a look at what our graph looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(research_graph.mermaid_image(start_node=ResearchQuery, direction=\"LR\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this workshop we built a deep research agentic system composed of 2 agents and 2 structured output prompts using constructed using Pydantic graph. \n",
    "\n",
    "## Takeaways:\n",
    "1. Graph frameworks like Pydantic or LangGraph are model / framework agnostic. They're just graphs. You can put any model from any provider in the graph so long as it returns the right type and context to the next node. \n",
    "2. Not everything is an agent problem. The evaluation prompts didn't need to be agents and it just created unecessary overhead. \n",
    "3. Graphs are great, but we also could have implemented this with simple python control flow. Graphs start to make sense when control flow get complicated and your code starts to look like spaghetti code.\n",
    "\n",
    "This concludes the advanced agent concept workshop. In the next module we will start to focus more on running these agents in a production environment at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
