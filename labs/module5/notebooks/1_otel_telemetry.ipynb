{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operationalizing Agent Platform: Telemetry\n",
    "Welcome to the telemetry lab. In this lab, we'll instrument our code to output traces, logs, and metrics using OpenTelemetry (OTEL), with outputs initially directed to the console.\n",
    "\n",
    "We'll cover:\n",
    "* The importance of using instrumentation facades to create \"two-way door\" decisions that allow you to change your telemetry backends without refactoring code\n",
    "* How auto instrumenters and our own custom telemetry data play together\n",
    "* Common patterns for logging, metric emission, and distributed tracing in agent systems\n",
    "* Practical examples of instrumenting different components of an agent platform\n",
    "* How to configure and use various OTEL exporters\n",
    "* Examining how our telemetry pipeline works in the EKS cluster we've already deployed\n",
    "\n",
    "By the end of this lab, you'll understand how to effectively monitor and debug agent systems in production using industry-standard observability practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Trace Context\n",
    "The core concept here is to use libraries that are OTLP (open telemetry protocol) compatible. The trick is to use OTELs context propagation to work between different instrumentation libraries. Instrumentation libraries are useful because they patch common packages to provide auto-instrumentation of things like LLMCalls, Agent frameworks, etc.. Otherwise you'll be writing a LOT of custom instrumentation code which will ruin your day. It also creates a 2-way door decision if you decide you want to use a different instrumentation library. As long as it's OTLP compatible, you're in business.\n",
    "\n",
    "## Where to send my telemetry data? \n",
    "Avoid sending your telemetry directly to a provider. Instead send it to an OTEL collector running somewhere. Configure the OTEL collectors to send the telemetry information wherever you want them to go. Want to send it to LangFuse? Great! Want to send it to LogFire or Phoenix? Also great! Anything that is OTEL compatible is fair game. Your code shouldn't care where the telemetry data is going.\n",
    "\n",
    "## What about custom spans, metrics, and logs? \n",
    "This is where you use a Facade. it's common practice to abstract your metrics, logs, and custom trace logic to an abstraction layer. If you change your mind and want to change the underlying logging/metric/trace configuration, you just need to update the facade.\n",
    "\n",
    "To get started, lets build our facade and OTEL provider. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a facade\n",
    "Firstly, we need to create a facade that we can call to interface with the collectors. What you don't want is to be in a situation where you're using an SDK or tool directly throughout your code. If you decide to switch or pick something different, it will be a painful refactor. Keep vendor specific logic in the collector, not your code. Lockin comes from using framework specific enrichments that might be difficult to reverse.\n",
    "\n",
    "There are two components to the face\n",
    "1. The actual facade itself\n",
    "2. A provider that teh facade writes to (aka provider) \n",
    "\n",
    "First lets define an interface for our provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from opentelemetry.trace import Tracer\n",
    "from opentelemetry.metrics import Meter\n",
    "from logging import Logger\n",
    "\n",
    "class BaseObservabilityProvider(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for observability providers.\n",
    "    Implementations can switch out the underlying tooling.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def get_tracer(self, name: str) -> Tracer:\n",
    "        \"\"\"Get a tracer for creating spans.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_meter(self, name: str) -> Meter:\n",
    "        \"\"\"Get a meter for recording metrics.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_logger(self, name: str) -> Logger:\n",
    "        \"\"\"Get a logger for recording logs.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our interface, lets define an otel provider that would work by calling the otel endpoint in a cluster. This is pretty undifferentiated work so we'll just use the version in our common/ directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_platform.core.observability.provider.otel_provider import OpenTelemetryProvider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Testing\n",
    "This otel collector requires an OTEL endpoint setup to send telemetry to. For the initial part of this notebook, we want to emit the metrics to our jupyter notebook. The setup is a bit different to output to the console. Let's subclass the setup methods of the OpenTelemetryProvider and replace them with Console exporters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter\n",
    "from opentelemetry.sdk.metrics.export import ConsoleMetricExporter\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.metrics import MeterProvider\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "# OpenTelemetry imports for traces\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "\n",
    "# OpenTelemetry imports for metrics\n",
    "from opentelemetry import metrics\n",
    "from opentelemetry.sdk.metrics import MeterProvider\n",
    "from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader\n",
    "\n",
    "import logging\n",
    "\n",
    "from typing import Optional, Dict\n",
    "\n",
    "class ConsoleOpenTelemetryProvider(OpenTelemetryProvider):\n",
    "    \"\"\"\n",
    "    OpenTelemetry provider that outputs to console for Jupyter notebooks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        service_name: str,\n",
    "        service_version: str = \"0.1.0\",\n",
    "        additional_attributes: Optional[Dict[str, str]] = None\n",
    "    ):\n",
    "        # Don't call parent's __init__ to avoid running its setup methods\n",
    "        # Instead, copy the initialization code but skip the setup calls\n",
    "        self.service_name = service_name\n",
    "        self.service_version = service_version\n",
    "        \n",
    "        # Create base resource with service info\n",
    "        resource_attributes = {\n",
    "            \"service.name\": service_name,\n",
    "            \"service.version\": service_version,\n",
    "        }\n",
    "        \n",
    "        # Add any additional attributes\n",
    "        if additional_attributes:\n",
    "            resource_attributes.update(additional_attributes)\n",
    "        \n",
    "        self.resource = Resource.create(resource_attributes)\n",
    "        \n",
    "        # Initialize providers\n",
    "        self.tracer_provider = None\n",
    "        self.meter_provider = None\n",
    "        self.logger_provider = None\n",
    "        \n",
    "        # Call our own setup methods\n",
    "        self._setup_tracing()\n",
    "        self._setup_metrics()\n",
    "        self._setup_logging()\n",
    "\n",
    "        # Track which loggers were configured. For the console logger, we don't really need this\n",
    "        # But the subclass will.\n",
    "        self._configured_loggers = set()\n",
    "    \n",
    "    def _setup_tracing(self) -> None:\n",
    "        \"\"\"Override to use console exporter instead of OTLP.\"\"\"\n",
    "        # Create and set global tracer provider only if not already set\n",
    "        self.tracer_provider = TracerProvider(resource=self.resource)\n",
    "        console_exporter = ConsoleSpanExporter()\n",
    "        self.tracer_provider.add_span_processor(SimpleSpanProcessor(console_exporter))\n",
    "        trace.set_tracer_provider(self.tracer_provider)\n",
    "    \n",
    "    def _setup_metrics(self) -> None:\n",
    "        \"\"\"Override to use console exporter instead of OTLP.\"\"\"\n",
    "        console_exporter = ConsoleMetricExporter()\n",
    "        reader = PeriodicExportingMetricReader(console_exporter, export_interval_millis=1000)\n",
    "        self.meter_provider = MeterProvider(resource=self.resource, metric_readers=[reader])\n",
    "        metrics.set_meter_provider(self.meter_provider)\n",
    "    \n",
    "    def _setup_logging(self) -> None:\n",
    "        \"\"\"Override to use basic console logging instead of OTLP.\"\"\"\n",
    "        # Configure basic Python logging to console\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')\n",
    "\n",
    "    # Override the get logger method with basic logging to stdout. \n",
    "    # The OTEL logger adds OTEL context to the log record that requires\n",
    "    # the actual Meter and Tracer to be available vs. the console versions.\n",
    "    def get_logger(self, name: str) -> Logger:\n",
    "        \"\"\"Get a logger that writes to stdout with trace context.\"\"\"\n",
    "        logger = logging.getLogger(name)\n",
    "        return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to create our Facade. This will consume an observability provider and act as an interface between the rest of our code and the underlying provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional\n",
    "from opentelemetry.trace import Tracer\n",
    "from opentelemetry.metrics import Meter\n",
    "from logging import Logger\n",
    "\n",
    "class ObservabilityFacade:\n",
    "    \"\"\"\n",
    "    A unified service to handle all observability concerns.\n",
    "    This facade class encapsulates all three telemetry signals.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, service_name: str, provider: BaseObservabilityProvider):\n",
    "        \"\"\"\n",
    "        Initialize the observability service.\n",
    "        \n",
    "        Args:\n",
    "            service_name: Name of the service\n",
    "            provider: pre-configured observability provider\n",
    "        \"\"\"\n",
    "        self.service_name = service_name\n",
    "        \n",
    "        # Use provided provider.\n",
    "        self.provider = provider\n",
    "        \n",
    "        # Get the components for use\n",
    "        self.tracer = self.provider.get_tracer(service_name)\n",
    "        self.meter = self.provider.get_meter(service_name)\n",
    "        self.logger = self.provider.get_logger(service_name)\n",
    "        \n",
    "        # Create commonly used meters\n",
    "        self.counter_metrics = {}\n",
    "        self.gauge_metrics = {}\n",
    "        self.histogram_metrics = {}\n",
    "    \n",
    "    def create_counter(self, name: str, description: str, unit: str = \"1\") -> None:\n",
    "        \"\"\"Create a counter metric.\"\"\"\n",
    "        self.counter_metrics[name] = self.meter.create_counter(\n",
    "            name=name,\n",
    "            description=description,\n",
    "            unit=unit\n",
    "        )\n",
    "    \n",
    "    def create_gauge(self, name: str, description: str, unit: str = \"1\") -> None:\n",
    "        \"\"\"Create a gauge metric.\"\"\"\n",
    "        self.gauge_metrics[name] = self.meter.create_observable_gauge(\n",
    "            name=name,\n",
    "            description=description,\n",
    "            unit=unit\n",
    "        )\n",
    "    \n",
    "    def create_histogram(self, name: str, description: str, unit: str = \"1\") -> None:\n",
    "        \"\"\"Create a histogram metric.\"\"\"\n",
    "        self.histogram_metrics[name] = self.meter.create_histogram(\n",
    "            name=name,\n",
    "            description=description,\n",
    "            unit=unit\n",
    "        )\n",
    "    \n",
    "    def increment_counter(self, name: str, value: int = 1, attributes: Optional[Dict[str, Any]] = None) -> None:\n",
    "        \"\"\"Increment a counter metric.\"\"\"\n",
    "        if name not in self.counter_metrics:\n",
    "            self.create_counter(name, f\"Counter for {name}\")\n",
    "        self.counter_metrics[name].add(value, attributes)\n",
    "    \n",
    "    def record_histogram(self, name: str, value: float, attributes: Optional[Dict[str, Any]] = None) -> None:\n",
    "        \"\"\"Record a value to a histogram metric.\"\"\"\n",
    "        if name not in self.histogram_metrics:\n",
    "            self.create_histogram(name, f\"Histogram for {name}\")\n",
    "        self.histogram_metrics[name].record(value, attributes)\n",
    "    \n",
    "    def start_span(self, name: str, attributes: Optional[Dict[str, Any]] = None, kind=None):\n",
    "        \"\"\"Start a new trace span.\"\"\"\n",
    "        return self.tracer.start_as_current_span(name, attributes=attributes, kind=kind)\n",
    "    \n",
    "    def log(self, level: int, message: str, **kwargs) -> None:\n",
    "        \"\"\"Log a message at the specified level.\"\"\"\n",
    "        self.logger.log(level, message, **kwargs)\n",
    "    \n",
    "    def debug(self, message: str, **kwargs) -> None:\n",
    "        \"\"\"Log a debug message.\"\"\"\n",
    "        self.logger.debug(message, **kwargs)\n",
    "    \n",
    "    def info(self, message: str, **kwargs) -> None:\n",
    "        \"\"\"Log an info message.\"\"\"\n",
    "        self.logger.info(message, **kwargs)\n",
    "    \n",
    "    def warning(self, message: str, **kwargs) -> None:\n",
    "        \"\"\"Log a warning message.\"\"\"\n",
    "        self.logger.warning(message, **kwargs)\n",
    "    \n",
    "    def error(self, message: str, **kwargs) -> None:\n",
    "        \"\"\"Log an error message.\"\"\"\n",
    "        self.logger.error(message, **kwargs)\n",
    "    \n",
    "    def critical(self, message: str, **kwargs) -> None:\n",
    "        \"\"\"Log a critical message.\"\"\"\n",
    "        self.logger.critical(message, **kwargs)\n",
    "\n",
    "    # Direct accessor methods:\n",
    "    def get_tracer(self) -> Tracer:\n",
    "        return self.tracer\n",
    "        \n",
    "    def get_meter(self) -> Meter:\n",
    "        return self.meter\n",
    "        \n",
    "    def get_logger(self) -> Logger:\n",
    "        return self.logger\n",
    "\n",
    "\n",
    "# Global instance\n",
    "_instance: Optional[ObservabilityFacade] = None\n",
    "\n",
    "def configure_facade(service_name: str, provider: BaseObservabilityProvider) -> 'ObservabilityFacade':\n",
    "    \"\"\"\n",
    "    Configure and set the global ObservabilityFacade instance.\n",
    "    \"\"\"\n",
    "    global _instance\n",
    "    _instance = ObservabilityFacade(service_name, provider)\n",
    "    return _instance\n",
    "\n",
    "def get_facade() -> Optional['ObservabilityFacade']:\n",
    "    \"\"\"\n",
    "    Get the global ObservabilityFacade instance if configured.\n",
    "    \"\"\"\n",
    "    return _instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure The Facade\n",
    "Great! We now have everything we need to start instrumenting our system. For the purpose of this notebook, we'll be outputting the OTEL telemetry data to our jupyter notebook output. Later on in the notebook, we will swap out the underlying provider for our OTEL provider in the EKS cluster running and see it in action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the console provider\n",
    "console_provider = ConsoleOpenTelemetryProvider(\n",
    "    service_name=\"jupyter-notebook-test\",\n",
    "    service_version=\"0.1.0\",\n",
    "    additional_attributes={\"environment\": \"notebook\"}\n",
    ")\n",
    "\n",
    "telemetry: ObservabilityFacade = configure_facade(\"jupyter-notebook-test\", console_provider)\n",
    "\n",
    "# Create a counter instrument\n",
    "telemetry.create_counter(\"sample_counter\", \"Counts sample events\", \"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out our spans\n",
    "We'll start a span called batch-operations and use our telemetry object to tie them all together with the span context. If you run this multiple times, you'll see that the parentId is the same for the child spans but is different every time you run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create an outer parent span\n",
    "with telemetry.start_span(\"batch-operations\"):\n",
    "    telemetry.info(\"Starting batch of operations\")  # This log is tied to the parent span\n",
    "    \n",
    "    # Test metric collection\n",
    "    for i in range(3):\n",
    "        telemetry.increment_counter(\"sample_counter\", value=1, attributes={\"action\": \"test\"})\n",
    "        print(f\"Added metric {i+1}\")\n",
    "        \n",
    "        # Create child spans - these will automatically be children of the outer span\n",
    "        with telemetry.start_span(f\"operation-{i}\"):\n",
    "            print(f\"Performing operation {i}\")\n",
    "            telemetry.info(f\"Processing operation {i}\")  # This log is tied to the child span\n",
    "            time.sleep(0.5)\n",
    "    \n",
    "    telemetry.info(\"Completed all operations\")  # This log is tied to the parent span again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Instrumenters\n",
    "To date (4/6/2024) there's no OTLP compatible auto instrumenter that is comprehensive so we need to patch them together using the the global context of the OTEL SDK as the glue. \n",
    "\n",
    "## OpenLLMetry\n",
    "OpenLLMetry is fairly comprehensive and has instrumentation for most of the major LLM providers including Bedrock, SageMaker. It also has instrumentation for LangChain, LlamaIndex, CrewAI, and LiteLLM, etc.. What it doesn't have is auto instrumentation for PydanticAI or other newer and emerging frameworks. \n",
    "\n",
    "## LogFire\n",
    "To suppliment this, we can use LogFire's free SDK which is also OTLP compatible.\n",
    "\n",
    "\n",
    "# Example Instrumentation\n",
    "Lets auto instrument a sample FastAPI application composed of a couple classes that call a LangChain agent, a PydanticAI agent, and Bedrock directly to see it in action. This is representative of what a sample Agent application server might look like in the agent platform. \n",
    "\n",
    "**Note** Because this is going to be very long, we won't add our normal abstraction layers to the agents to keep the implementation easier to understand in a Jupyter notebook context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent as PydanticAIAgent\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool as lc_tool\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "from agentic_platform.core.models.llm_models import LLMRequest, LLMResponse\n",
    "from agentic_platform.core.models.memory_models import Message\n",
    "from agentic_platform.core.client.llm_gateway.bedrock_gateway_client import BedrockGatewayClient\n",
    "\n",
    "########################################################\n",
    "# Define PyAIAgent\n",
    "########################################################\n",
    "\n",
    "example_facade: ObservabilityFacade = get_facade()\n",
    "\n",
    "logger: Logger = example_facade.get_logger()\n",
    "\n",
    "class PyAIAgent:\n",
    "    def __init__(self):\n",
    "        self.agent: PyAIAgent = PydanticAIAgent(\n",
    "            'bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0',\n",
    "            system_prompt='You are a helpful PydanticAI agent / assistant.'\n",
    "        )\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        logger.info(\"Invoking PydanticAI agent\")\n",
    "        # Demonstrate a metric getting emited.\n",
    "        example_facade.create_counter(\"pyai_invocations\", \"Counts PydanticAI invocations\", \"1\")\n",
    "        response = self.agent.run_sync(prompt)\n",
    "        return response.data\n",
    "    \n",
    "########################################################\n",
    "# Define LangChainAgent\n",
    "########################################################\n",
    "\n",
    "@lc_tool\n",
    "def echo(x: str) -> str:\n",
    "    \"\"\"Echo the input\"\"\"\n",
    "    return x\n",
    "\n",
    "class LangChainAgent:\n",
    "    def __init__(self):\n",
    "        llm = ChatBedrockConverse(\n",
    "            model=\"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "            temperature=.5\n",
    "        )\n",
    "\n",
    "        # Create react agent requires tools in the definition? \n",
    "        tools = [echo]\n",
    "        self.agent = create_react_agent(model=llm, tools=tools)\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        logger.info(\"Invoking LangChain agent\")\n",
    "        inputs = {\"messages\": [(\"system\", \"You are a helpful LangChain agent / assistant.\"), (\"user\", prompt)]}\n",
    "        response = self.agent.invoke(inputs)\n",
    "        return response['messages'][-1].content\n",
    "    \n",
    "########################################################\n",
    "# Call Bedrock directly with our abstraction types\n",
    "\n",
    "class CallBedrockDirectly:\n",
    "    @staticmethod\n",
    "    def call(prompt: str) -> str:\n",
    "        logger.info(\"Calling Bedrock directly\")\n",
    "        request: LLMRequest = LLMRequest(\n",
    "            system_prompt=\"You are a helpful direct Bedrock calling assistant\",\n",
    "            model_id=\"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "            messages=[Message(role=\"user\", text=prompt)],\n",
    "            hyperparams={\"temperature\": .5}\n",
    "        )\n",
    "        response: LLMResponse = BedrockGatewayClient.chat_invoke(request)\n",
    "        return response.message\n",
    "    \n",
    "########################################################\n",
    "# Write some middle piece of code that calls everything.\n",
    "########################################################\n",
    "\n",
    "class AgentCoordinator:\n",
    "    @staticmethod\n",
    "    def call_everything(prompt: str) -> str:\n",
    "        pyai: PyAIAgent = PyAIAgent()\n",
    "        langchain: LangChainAgent = LangChainAgent()\n",
    "\n",
    "        pyai_response: str = pyai.invoke(prompt)\n",
    "        langchain_response: str = langchain.invoke(prompt)\n",
    "\n",
    "        bedrock_response: str = CallBedrockDirectly.call(prompt)\n",
    "\n",
    "        response: str = f\"PyAI: {pyai_response}\\n\\nLangChain: {langchain_response}\\n\\nBedrock: {bedrock_response}\"\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Middlware to set the global context\n",
    "Middleware is a core component of most web frameworks. We can create the global context and spans for each http request using middlware and just plug it into our FastAPI server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from starlette.middleware.base import BaseHTTPMiddleware\n",
    "from fastapi import Request, Response\n",
    "from starlette.types import ASGIApp\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TelemetryMiddleware(BaseHTTPMiddleware):\n",
    "    \"\"\"\n",
    "    Middleware that sets up OpenTelemetry instrumentation.\n",
    "    Configures the telemetry facade and enables auto-instrumentation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        app: ASGIApp, \n",
    "        service_name: str = \"fastapi-app\",\n",
    "        service_version: str = \"0.1.0\",\n",
    "        excluded_paths: List[str] = None,\n",
    "    ):\n",
    "        super().__init__(app)\n",
    "        self.excluded_paths = excluded_paths or [\"/health\", \"/metrics\"]\n",
    "\n",
    "        \n",
    "        # Set up telemetry provider\n",
    "        try:\n",
    "            provider: BaseObservabilityProvider = ConsoleOpenTelemetryProvider(\n",
    "                service_name=service_name,\n",
    "                service_version=service_version\n",
    "            )\n",
    "            \n",
    "            # Configure the global telemetry facade\n",
    "            self.telemetry = configure_facade(service_name, provider)\n",
    "            self.telemetry.info(f\"Telemetry configured for {service_name} v{service_version}\")\n",
    "\n",
    "            # BedrockInstrumentor().instrument()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing telemetry: {e}\")\n",
    "            self.telemetry = None\n",
    "    \n",
    "    async def dispatch(self, request: Request, call_next):\n",
    "        # Skip telemetry for excluded paths\n",
    "        path = request.scope[\"path\"]\n",
    "        for excluded_path in self.excluded_paths:\n",
    "            if path == excluded_path or path.startswith(excluded_path):\n",
    "                return await call_next(request)\n",
    "        \n",
    "        # Handle the request - auto-instrumentation creates spans automatically\n",
    "        response = await call_next(request)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Setup Nest Asyncio\n",
    "########################################################\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "#################################################################\n",
    "# Import auto instrumenters\n",
    "#################################################################\n",
    "from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor\n",
    "from opentelemetry.instrumentation.bedrock import BedrockInstrumentor\n",
    "from opentelemetry.instrumentation.langchain import LangchainInstrumentor\n",
    "\n",
    "########################################################\n",
    "# Define FastAPI Server\n",
    "########################################################\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Add the middware here which sets our global tracer context.\n",
    "app.add_middleware(\n",
    "    TelemetryMiddleware,\n",
    "    service_name=\"agent-api\",\n",
    "    service_version=\"1.0.0\",\n",
    "    excluded_paths=[\"/health\", \"/metrics\"]  # Skip instrumentation for these paths\n",
    ")\n",
    "\n",
    "# Auto instrments the FastAPI app. It adds a middleware to the app.\n",
    "# Secondly since the global span context is set, it'll use that context.\n",
    "FastAPIInstrumentor.instrument_app(app)\n",
    "\n",
    "# This patches the boto3 client to record Bedrock invocation calls.\n",
    "BedrockInstrumentor().instrument()\n",
    "\n",
    "# Instruments LangChain. Note: This will be a bit noisy with the Bedrock instrumenter.\n",
    "LangchainInstrumentor().instrument()\n",
    "\n",
    "\n",
    "\n",
    "class AgentRequest(BaseModel):\n",
    "    prompt: str\n",
    "\n",
    "class AgentResponse(BaseModel):\n",
    "    response: str\n",
    "\n",
    "@app.post(\"/agent\")\n",
    "async def agent_endpoint(request: AgentRequest):\n",
    "    response: str = AgentCoordinator.call_everything(request.prompt)\n",
    "    return AgentResponse(response=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi.testclient import TestClient\n",
    "\n",
    "client = TestClient(app)\n",
    "# Now make requests to your API\n",
    "print(\"Sending request to the API...\")\n",
    "response = client.post(\n",
    "    \"/agent\",\n",
    "    json={\"prompt\": \"Tell me about OpenTelemetry\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this lab we dove into telemetry for our agent applications using OTEL and OpenLLMetry auto instrumenters. We encourage you to make some calls to the agent base existing agents and check out the traces, logs, and metrics in action that are pushed to LangFuse and OpenSearch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
