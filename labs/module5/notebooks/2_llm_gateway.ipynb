{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Platform: LLM Gateway\n",
    "\n",
    "This lab introduces the concept of an LLM Gateway. LLM Gateways let you track and throttle requests in a multi-tenant environment. Your tenancy could be by department, workload, customer organization, or even individual users on your platform.\n",
    "\n",
    "There are many options for this from open source projects, private offerings, or DIY. In this lab we'll explore a DIY approach. As the single point of failure for your entire agent platform, it's generally something customers want to own and control themselves. The three main patterns we see are \n",
    "* Completely DIY \n",
    "* Using an SDK like LiteLLM and building your own infrastructure around it\n",
    "* Using a 3P offering\n",
    "\n",
    "The 3P providers guard a lot of the most useful features (security, team management, etc..) behind enterprise licenses. The open source versions of their proxies do deploy resources in your account, but generally, it's not suitable for scale and lacks the enterprise security features needed by most enterprises like oAuth support & easy integration into your existing identity services.\n",
    "\n",
    "To get started, let's build a rate limiter using Redis. In the platform we use ElastiCache (redis) but fo this lab we'll be using redis in a docker container spun up using the docker compose file provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components of an LLM Gateway\n",
    "An LLM Gateway generally does 3 things\n",
    "1. Unifies the types across different APIs and model providers \n",
    "2. Provides usage metrics\n",
    "3. Enables rate limiting based on your defined tenancy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage Keys\n",
    "To experiment with Redis, we've provided a docker-compose file in the main project. You can spin up the docker compose to get a local instance of Redis and a local DB instance. In the agent stack Redis is in a private subnet making testing harder to test. You can port foward through the jump box if you'd like to hit it directly, but for this lab we'll be hitting our local docker instance.\n",
    "\n",
    "You should create a .env file in this directory and have these values once docker compose is stood up. We've also provided a local DynamoDB container in the compose file which you can connect to to run and debug locally.\n",
    "\n",
    "```bash\n",
    "REDIS_HOST=localhost\n",
    "REDIS_PASSWORD=redis\n",
    "REDIS_PORT=6379\n",
    "DYNAMODB_USAGE_PLANS_TABLE=<yourDDBTable>\n",
    "DYNAMODB_USAGE_LOGS_TABLE=<yourDDBTable>\n",
    "\n",
    "# You need to set the env to local so that Redis doesn't expect SSL\n",
    "ENVIRONMENT=local\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create our Usage Plan Keys\n",
    "Usage plans are effectively rate limits. Compared to traditional rate limiting, LLM rate limiting holds a new set of requirements. Rate limits can be set at\n",
    "* The global level, \n",
    "* Model level, \n",
    "* Requests per minute (RPM), \n",
    "* Tokens per minute (TPM) on the input and output tokens. \n",
    "\n",
    "For tenancy, it can be set at the individual level, team level, organization level, or at the service level meaning a background process (or agent level).\n",
    "\n",
    "This is potentially a high throughput gateway so we need to consider scale as we launch more agentic systems. Additionally we need to optimize for flexibility. Because of these two requirements, using a NoSQL DB with a flexible identifier is key. We'll define our usage plan as follows:\n",
    "* primary key is <entity id> where entity is a user, team, organization, or service. \n",
    "* secondary key is <entity type>. \n",
    "\n",
    "We place the entity id as the primary key to more evenly distribute the keys to prevent hot spotting (common issue in NoSQL tables) where common primary key types are grouped together on a shard and the load is unevenly distributed.\n",
    "\n",
    "Secondly, we'll store the rate limits for input, output, rpm, and model specific limits as an attribute of the item in the NoSQL table. This allows us to rate limit based on an arbitrary lookup key and still maintain flexibility.\n",
    "\n",
    "Find the key definition below\n",
    "\n",
    "\n",
    "```bash\n",
    "class UsagePlanEntityType(str, Enum):\n",
    "    USER = \"USER\"\n",
    "    SERVICE = \"SERVICE\"\n",
    "    API_KEY = \"API_KEY\"\n",
    "    DEPARTMENT = \"DEPARTMENT\"\n",
    "    PROJECT = \"PROJECT\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.value\n",
    "\n",
    "class RateLimits(BaseModel):\n",
    "    \"\"\"Rate limits configuration\"\"\"\n",
    "    input_tpm: int = Field(default=40000, description=\"Input tokens per minute limit\")\n",
    "    output_tpm: int = Field(default=10000, description=\"Output tokens per minute limit\")\n",
    "    rpm: int = Field(default=60, description=\"Requests per minute limit\")\n",
    "\n",
    "class UsagePlan(BaseModel):\n",
    "    \"\"\"Usage plan with rate limits\"\"\"\n",
    "    entity_id: str\n",
    "    entity_type: UsagePlanEntityType\n",
    "    tenant_id: str = 'SYSTEM' # By default, we assume no tenancy\n",
    "    budget_id: Optional[str] = None # Placeholder for future use.\n",
    "    model_permissions: List[str]\n",
    "    active: bool = Field(default=True)\n",
    "    default_limits: RateLimits = Field(default_factory=RateLimits)\n",
    "    model_limits: Dict[str, RateLimits] = Field(default_factory=dict)\n",
    "    metadata: Optional[Dict] = Field(default_factory=dict)\n",
    "    created_at: int = Field(default_factory=lambda: int(time.time()))\n",
    "\n",
    "    def get_limits_for_model(self, model_id: str) -> RateLimits:\n",
    "        \"\"\"Get limits for a specific model, falling back to defaults\"\"\"\n",
    "        return self.model_limits.get(model_id, self.default_limits)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our key type from the platform.\n",
    "from agentic_platform.service.llm_gateway.models.usage_types import UsagePlanEntityType,  UsagePlan, RateLimits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "# Create our usage plan for a user.\n",
    "usage_plan: UsagePlan = UsagePlan(\n",
    "    entity_id=\"123\",\n",
    "    entity_type=UsagePlanEntityType.USER,\n",
    "    tenant_id=\"SYSTEM\",\n",
    "    model_permissions=[\"*\"],\n",
    "    active=True,\n",
    "    default_limits=RateLimits(input_tpm=40000, output_tpm=10000, rpm=60),\n",
    "    model_limits={}\n",
    ")\n",
    "\n",
    "print(usage_plan.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a global usage limit of 40000/10000 input and output token limits respectively. We'll also limit all model usage to 60 requests per minute. The pk and sk will look like 123:USER Great!\n",
    "\n",
    "Now we need to upload it to dynamoDB. We can use agentic platform APIs to upload it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_platform.service.llm_gateway.api.create_usage_plan_controller import CreateUsagePlanController\n",
    "from agentic_platform.service.llm_gateway.models.gateway_api_types import CreateUsagePlanRequest, CreateUsagePlanResponse\n",
    "\n",
    "# Create our usage plan.\n",
    "create_usage_plan_request: CreateUsagePlanRequest = CreateUsagePlanRequest(\n",
    "    entity_type=UsagePlanEntityType.USER,\n",
    "    entity_id=\"123\",\n",
    "    tenant_id=\"SYSTEM\",\n",
    "    model_permissions=[\"*\"],\n",
    "    default_limits=RateLimits(input_tpm=40000, output_tpm=10000, rpm=60),\n",
    "    model_limits={},\n",
    "    metadata={}\n",
    ")\n",
    "\n",
    "response: CreateUsagePlanResponse = CreateUsagePlanController.create(create_usage_plan_request)\n",
    "print(response.plan.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Rate Limiter\n",
    "Now tha twe have our API keys, lets build basic rate limiting logic in a rate limiter class that uses a sliding window. It's efficient but could be optimized further by handling edge cases where someone burns through the rate limit at the end of 1 minute and then again at second minute. We're going for the lowest latency approach so our rate limit implementation should be fine for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_platform.service.llm_gateway.client.cache_client import RateLimiter\n",
    "from agentic_platform.service.llm_gateway.models.usage_types import RateLimitResult\n",
    "\n",
    "# Our rate limiter is async, so we need to use nest_asyncio to run it in a synchronous context.\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response: RateLimitResult = await RateLimiter.check_limit(plan=usage_plan, model_id=\"nova-micro\", est_input=100, est_output=100)\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test throttling\n",
    "Lets call it a bunch of times to see if it'll throttle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    response = await RateLimiter.check_limit(plan=usage_plan, model_id=\"nova-micro\", est_input=5500, est_output=500)\n",
    "    print(f\"Request {i}: {response}\")\n",
    "\n",
    "    # Update rate limits.\n",
    "    await RateLimiter.record_usage(plan=usage_plan, model_id=\"nova-micro\", input_tokens=5500, output_tokens=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Our Gateway\n",
    "There's two approaches to a gateway\n",
    "1) Make a passthrough API for the model provider you're using\n",
    "2) Normalize all the model APIs to a common format. \n",
    "\n",
    "ChatCompletion format is probably the most common format overall. In 2025, new protocols are emerging and it's unclear if ChatCompletion will remain the standard long term. Different model providers have different modalities and features. You miss out on some of those features if you're limited to one providers specific format. Therefore it's important to provide passthrough endpoints as well to allow users to access model provider specific features.\n",
    "\n",
    "It's also still important to own your own types so even though you're getting all the model results in ChatCompletion format. You still want to convert it to your own type object (even if that type is identical to ChatCompletion). This creates 2 way doors if you decide to use a different gateway or return results in a different format down the road. \n",
    "\n",
    "We've provided a sample FastAPI server below and will use the TestClient to execute http requests to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request, HTTPException\n",
    "import asyncio\n",
    "from litellm import completion\n",
    "import uvicorn\n",
    "from threading import Thread\n",
    "import boto3\n",
    "import random\n",
    "from multiprocessing import Process\n",
    "\n",
    "from agentic_platform.service.llm_gateway.models.gateway_api_types import (\n",
    "    ChatCompletionRequest, \n",
    "    ChatCompletionResponse,\n",
    "    ConverseRequest, \n",
    "    ConverseResponse,\n",
    ")\n",
    "\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Mimic what our rate limiter would look for\n",
    "def rate_limit_allowed(request: Request) -> bool:\n",
    "    identifier = request.headers.get('X-IDENTIFIER')\n",
    "    print(f'Identifier is allowed')\n",
    "    return True\n",
    "\n",
    "# Define your API endpoints\n",
    "@app.post(\"/chat/completions\")\n",
    "async def chat_completions(request: Request) -> ChatCompletionResponse:\n",
    "\n",
    "    # Useful for seeing our auth headers\n",
    "    rate_limit_allowed(request)\n",
    "\n",
    "    payload = await request.json()\n",
    "    response = completion(**payload)\n",
    "    return ChatCompletionResponse(**response.model_dump())\n",
    "\n",
    "@app.post(\"/model/{model_id}/converse\")\n",
    "async def converse(model_id: str, request: Request) -> ConverseResponse:\n",
    "\n",
    "    # Useful for seeing our auth headers\n",
    "    rate_limit_allowed(request)\n",
    "\n",
    "    request_body = await request.json()\n",
    "    request_body[\"modelId\"] = model_id\n",
    "    response = bedrock_client.converse(**request_body)\n",
    "    return ConverseResponse(**response)\n",
    "\n",
    "# Lets create a test client for our API server.\n",
    "from fastapi.testclient import TestClient\n",
    "\n",
    "test_client = TestClient(app)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call our API\n",
    "Now we can call our API. Using a gateway, we can pass in authentication headers just like a normal endpoint. We can then use those headers to identify the correct usage plan and rate limit based off it. \n",
    "\n",
    "For lab purposes, we'll pass in an API key. In practice, this should use oAuth. API keys are essentially long lived secrets that aren't rotated. We don't recommend authentication this way, but understand that many API providers use API keys. With a gateway approach you can apply least priviledged access to the API key secret to just the gateway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metering_identifier: str = 'whatever your identifier is'\n",
    "\n",
    "async def test_chat_completions():\n",
    "    completion_compatible_payload = {\n",
    "        \"model\": \"bedrock/anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        \"messages\": [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
    "    }\n",
    "\n",
    "    return test_client.post(\n",
    "        \"/chat/completions\", \n",
    "        json=completion_compatible_payload,\n",
    "        headers={\n",
    "            'X-IDENTIFIER': f'{metering_identifier}'\n",
    "        }\n",
    "    )\n",
    "\n",
    "async def test_br_passthrough():\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "    br_payload = {\n",
    "        \"messages\": [{ \"content\": [{'text': 'hello how are you?'}], \"role\": \"user\"}]\n",
    "    }\n",
    "\n",
    "    return test_client.post(\n",
    "        f\"/model/{model_id}/converse\", \n",
    "        json=br_payload,\n",
    "        headers={\n",
    "            'X-IDENTIFIER': f'{metering_identifier}'\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "result = asyncio.run(test_chat_completions())\n",
    "print(f'Chat Completion compatible response: {result.json()}')\n",
    "\n",
    "print('\\n--------------------------------\\n')\n",
    "\n",
    "result = asyncio.run(test_br_passthrough())\n",
    "print(f'Bedrock passthrough response: {result.json()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring it all together\n",
    "\n",
    "\n",
    "<div style=\"background-color: #FEEFB3; color: #9F6000; padding: 15px; border-radius: 5px; border-left: 6px solid #9F6000; margin-bottom: 15px;\">\n",
    "<strong>⚠️ WARNING:</strong> You will need to have the platform stack deployed for these next steps and the necessary permissions to grab secrets from secrets manager \n",
    "</div>\n",
    "\n",
    "\n",
    "Now that we understand what our LLM Gateway needs to look like, we can start calling it from our code. During the deployment of the platform, we configured a load balancer that points to our LLM gateway. The gateway is fronted by Cognito for authentication. To invoke our gateway, we'll grab the load balancers url & pull our secret to construct a request to it. \n",
    "\n",
    "The first step is to grab the load balancers name. You can find it in the console or use boto3 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from typing import List, Dict\n",
    "\n",
    "# Initialize the client\n",
    "elbv2 = boto3.client('elbv2')\n",
    "\n",
    "# List all load balancers\n",
    "load_balancers: List[Dict] = elbv2.describe_load_balancers()['LoadBalancers']\n",
    "\n",
    "# Get the load balancer name. It should be prefixed by k8s-platform\n",
    "dns_name: str = [lb['DNSName'] for lb in load_balancers if 'k8s-platform' in lb['LoadBalancerName']][0]\n",
    "dns_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get our Secret for Auth\n",
    "Now we need to get our secret containing our machine 2 machine client auth token. In the deployment script we've set up two client applications in cognito. The first one is for users and teh second one is for machine 2 machine oAuth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our Secret for Auth\n",
    "import json\n",
    "# The name should be prefixed by whatever you named your stack prefix followed by -m2m-credentials\n",
    "secret_name: str = 'agent-platform-123-m2m-credentials'\n",
    "secret = boto3.client('secretsmanager').get_secret_value(SecretId=secret_name)\n",
    "secret_value: str = secret['SecretString']\n",
    "\n",
    "# Parse the secret value\n",
    "secret_value_dict: Dict = json.loads(secret_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call API\n",
    "There are two main ways you interface with the proxy. (1) direct https requests and (2) through the SDKs. Lets start with the direct. \n",
    "\n",
    "At this point, our gateway is just a normal https endpont. We can query it using httpx or the requests package. The API is authenticated using oAuth so we need to pass in the Authentication header with the value 'Bearer <token>'. \n",
    "\n",
    "Lets get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_token():\n",
    "    client_id = secret_value_dict.get('client_id')\n",
    "    client_secret = secret_value_dict.get('client_secret')\n",
    "    token_url = secret_value_dict.get('token_url')\n",
    "    scopes = secret_value_dict.get('scopes')\n",
    "\n",
    "    data={\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': client_id,\n",
    "        'client_secret': client_secret,\n",
    "        'scope': scopes\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        token_url,\n",
    "        headers={'Content-Type': 'application/x-www-form-urlencoded'},\n",
    "        data=data\n",
    "    )\n",
    "\n",
    "    token_data = response.json()\n",
    "    # Extract the access token\n",
    "    token = token_data['access_token']\n",
    "    return token\n",
    "\n",
    "def construct_auth_header(token: str) -> str:\n",
    "    return f'Bearer {token}'\n",
    "\n",
    "m2m_token = get_token()\n",
    "auth_header = construct_auth_header(m2m_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a request.\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "bedrock_payload = {\n",
    "    \"messages\": [{ \"content\": [{'text': 'hello how are you?'}], \"role\": \"user\"}]\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f'http://{dns_name}/llm-gateway/model/{model_id}/converse',\n",
    "    headers={ 'Authorization': auth_header },\n",
    "    json=bedrock_payload,\n",
    "    timeout=5\n",
    ")\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! We just called our LLM Gateway! We can make this even better by wrapping the request in our own type so we can get back structured output using the converters from our agent platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_platform.core.converter.llm_request_converters import ConverseRequestConverter\n",
    "from agentic_platform.core.converter.llm_response_converters import ConverseResponseConverter\n",
    "from agentic_platform.core.models.llm_models import LLMRequest, LLMResponse, Message\n",
    "\n",
    "def call_gateway(request: LLMRequest) -> LLMResponse:\n",
    "    # Convert the request to the gateway format\n",
    "    gateway_request = ConverseRequestConverter.convert_llm_request(request)\n",
    "\n",
    "    # Drop the model_id because it's pulled from the request url.\n",
    "    model_id = gateway_request.pop('modelId')\n",
    "\n",
    "    # Call the gateway\n",
    "    response = requests.post(\n",
    "        f'http://{dns_name}/llm-gateway/model/{model_id}/converse',\n",
    "        headers={'Authorization': auth_header},\n",
    "        json=gateway_request,\n",
    "        timeout=5\n",
    "    )\n",
    "\n",
    "    # Convert the response to our own type\n",
    "    return ConverseResponseConverter.to_llm_response(response.json())\n",
    "\n",
    "request: LLMRequest = LLMRequest(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    hyperparams={'temperature': 0.5},\n",
    "    messages=[ Message(role=\"user\", text=\"Hello, how are you?\") ],\n",
    "    system_prompt=\"You are a helpful assistant.\"\n",
    ")\n",
    "\n",
    "response: LLMResponse = call_gateway(request)\n",
    "\n",
    "print(response.model_dump_json(indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call through the SDK\n",
    "There's lots of reasons to call our gateway through the SDK. Many frameworks like LangChain and Pydantic accept a configured boto3 client as input for one. Secondly, it can be nice to just use the SDK. To get the boto3 client to work with our proxy we need to make a couple configuration changes to it.\n",
    "\n",
    "## Configuration changes\n",
    "**Signing Requests**: \n",
    "\n",
    "Botocore uses your credentials to sign the request using sigV4. However, we want to use our oAuth credentials so we'll need to configure our client not to sign requests. If you do, the client won't respect our authentication headers when we pass them in. \n",
    "\n",
    "**Configure endpoint**\n",
    "\n",
    "Boto3 allows you to specify a proxy url by passing in endpoint_url. We'll be using our gateway endpoint for that url\n",
    "\n",
    "**Register Event**\n",
    "\n",
    "Boto3 allows you to register \"events\" before requests get sent out. We can use this to pass in our auth token as an Authentication header before boto3 sends the request to our proxy. For this lab, we'll just use our bearer token. In practice you'll want to use contextvars (python package) to store user's oAuth tokens for retrieval. This way we can propagate the users identity cleanly through our system and only access the token when we need it (like calling our gateway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "import botocore\n",
    "from contextvars import ContextVar\n",
    "from functools import partial\n",
    "\n",
    "# Set up our contextvar to store our token\n",
    "token_var: ContextVar = ContextVar('token')\n",
    "\n",
    "# This would normally be done by a middleware right after authentication.\n",
    "token_var.set(m2m_token)\n",
    "\n",
    "# Configure a function to add our auth token to the request.\n",
    "def _add_headers(request, **kwargs):\n",
    "    # Get the token from the contextvar\n",
    "    token: str = token_var.get()\n",
    "    request.headers['Authorization'] = f\"Bearer {token}\"\n",
    "\n",
    "# Keep the request unsigned. Our agent doesn't have an IAM role to sign the request so this will fail without this.\n",
    "config = Config(\n",
    "    retries={'max_attempts': 1},\n",
    "    signature_version=botocore.UNSIGNED\n",
    ")\n",
    "\n",
    "# Create our client and specify the endpoint url\n",
    "endpoint_url = f'http://{dns_name}/llm-gateway'\n",
    "client = boto3.client(\n",
    "    'bedrock-runtime',\n",
    "    endpoint_url=endpoint_url,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Add API key header to requests using partial\n",
    "client.meta.events.register_first(\n",
    "    'before-send.bedrock-runtime.Converse',\n",
    "    partial(_add_headers)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call our gateway through the configured client.\n",
    "bedrock_payload = {\n",
    "    \"modelId\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    \"messages\": [{ \"content\": [{'text': 'hello how are you?'}], \"role\": \"user\"}]\n",
    "}\n",
    "\n",
    "response = client.converse(**bedrock_payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have a proxy!\n",
    "Nice we have a proxy that works with both regular requests and our boto3 client! Lastly, lets see how this works using frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.providers.bedrock import BedrockProvider\n",
    "from pydantic_ai.models.bedrock import BedrockConverseModel\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "# Configure the provider to use our boto3 client.\n",
    "model: BedrockConverseModel = BedrockConverseModel(\n",
    "    model_name=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    provider=BedrockProvider(bedrock_client=client)\n",
    ")\n",
    "\n",
    "# model.request()\n",
    "\n",
    "# Create an agent with the model.\n",
    "agent = Agent(system_prompt='You are a helpful assistant.', model=model)\n",
    "\n",
    "response = agent.run_sync(\"Hello, how are you?\")\n",
    "\n",
    "print(response.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use LangChain\n",
    "Next we'll show you how to use our gateway with LangChain. It's as simple as passing in our prebuild client and running the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_core.messages import AIMessage, BaseMessage\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    temperature=0,\n",
    "    client=client\n",
    ")\n",
    "\n",
    "# Invoke the llm\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"Hello! How are you today?\"),\n",
    "]\n",
    "\n",
    "response: AIMessage =llm.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this lab, we went through the concept of an LLM gateway, constructed one locally and then called our LLM gateway deployed in our agent platform! We successfully converted the gateway responses to our types and also showed how the gateway integrates into existing frameworks. \n",
    "\n",
    "In the next lab, we'll be discussing distributed long term memory implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
