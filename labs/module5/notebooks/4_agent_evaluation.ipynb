{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Platform: Agent Evaluation\n",
    "\n",
    "This lab introduces the concept of Agent Evaluation. Effective evaluation frameworks are essential for measuring, comparing and improving agent performance across different implementations and configurations.\n",
    "\n",
    "There are several approaches to evaluating agentic systems, ranging from simple to complex. In this lab we'll focus on practical, implementation-oriented metrics. The three main evaluation patterns we see are:\n",
    "* Basic success rate measurement\n",
    "* Step-based procedural analysis\n",
    "* Comprehensive multi-dimensional evaluation frameworks\n",
    "\n",
    "The most sophisticated evaluation systems incorporate human feedback loops and nuanced assessments of agent behavior. While powerful, these systems often require significant infrastructure and human resources to implement effectively.\n",
    "\n",
    "To get started, we'll explore a couple approaches to an evaluation framework focused on two key metrics:\n",
    "1. Task success rate - measuring whether agents complete their assigned tasks correctly\n",
    "2. Steps to completion - analyzing the efficiency of agents by tracking the number of steps required to achieve success\n",
    "\n",
    "Additionally we'll be applying an approach using assertions. Assertions are essentially test cases. Sometimes we're looking for exact answers (What's the capital of France? -> Paris). Sometimes the evaluation criteria is qualitative data (did the agent provide a recipe that was gluten free?)\n",
    "\n",
    "These metrics provide a straightforward but powerful foundation for comparing agent performance across different models, prompting strategies, and tool configurations.\n",
    "\n",
    "To date (4/19/25) there's no comprehensive framework that does exactly what we need it to do so a little bit of custom work goes into it. \n",
    "\n",
    "First lets modify a couple agents we built in the previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic import BaseModel\n",
    "from tavily import TavilyClient\n",
    "from typing import Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from agentic_platform.core.models.memory_models import ToolResult\n",
    "from mcp import ListToolsResult\n",
    "from mcp.server import FastMCP\n",
    "from typing import List, Any\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# First we wrap our own MCP server in a MCPServerStdio object\n",
    "from pydantic_ai.mcp import MCPServerStdio as PyAIMCPServerStdio\n",
    "\n",
    "# Load our API key from the environment variable\n",
    "TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')\n",
    "\n",
    "# Now lets create our research tools\n",
    "class WebSearch(BaseModel):\n",
    "    query: str\n",
    "\n",
    "server = FastMCP()\n",
    "\n",
    "def search_web(query: WebSearch) -> List[Dict[str, Any]]:\n",
    "    '''Search the web to get back a list of results and content.'''\n",
    "    client: TavilyClient = TavilyClient(os.getenv(\"TAVILY_KEY\"))\n",
    "    response: Dict[str, any] = client.search(query=query.query)\n",
    "    return response['results']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next lets create our researcher agent. \n",
    "from pydantic_ai import Agent as PyAIAgent\n",
    "from common.models.prompt_models import BasePrompt\n",
    "from pydantic import BaseModel\n",
    "\n",
    "RESEARCHER_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful research agent with web search capabilities. Your job is to:\n",
    "\n",
    "1. Search for accurate, up-to-date information about any topic\n",
    "2. Provide clear and conscise answers to the users question. Provide a source annotation at the end of your response that maps to the source links below.\n",
    "3. Cite your sources with numbered links at the end of your response.\n",
    "\n",
    "Always be factual and objective in your research. Be clear and concise in your responses.\n",
    "Only answer the immediate question, you do not need to provide any additional context or commentary.\n",
    "If someone asks who the CEO of a company is, you should just say their name for example.\n",
    "\"\"\"\n",
    "\n",
    "# Define the expected output structure\n",
    "class ResearchResponse(BaseModel):\n",
    "    content: str\n",
    "    sources: list[str]\n",
    "\n",
    "# Create the agent with the simplified prompt\n",
    "researcher_agent = PyAIAgent(\n",
    "    'bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0',\n",
    "    system_prompt=RESEARCHER_SYSTEM_PROMPT,\n",
    "    output_type=ResearchResponse,\n",
    ")\n",
    "\n",
    "researcher_agent.tool_plain(search_web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "test_case = \"Who is the CEO of Amazon?\"\n",
    "\n",
    "result = researcher_agent.run_sync(test_case)\n",
    "\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the conversation history to see where the results came from. \n",
    "result.all_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a researcher agent using Tavily websearch, lets evaluate how well it does. Do do this we'll use Pydantic evals. There are other options, but this is the cleanest for assertion based testing. pydantic evals is doesn't have any dependencies on other frameworks. You could replace the perform_search() function with a LangChain agent and the framework would still work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pydantic_evals import Case, Dataset\n",
    "from pydantic_evals.evaluators import IsInstance, LLMJudge\n",
    "from typing import Any\n",
    "import re\n",
    "from pydantic_ai.agent import AgentRunResult\n",
    "\n",
    "# Define the input type for the researcher agent\n",
    "class ResearchQuery(BaseModel):\n",
    "    user_query: str\n",
    "\n",
    "# Function to run the researcher agent\n",
    "async def perform_research(query: ResearchQuery) -> ResearchResponse:\n",
    "    \n",
    "    # Run the agent with MCP servers using the pattern you provided\n",
    "    async with researcher_agent.run_mcp_servers():\n",
    "        result: AgentRunResult[ResearchResponse] = researcher_agent.run_sync(query.user_query)\n",
    "    \n",
    "    content: ResearchResponse = result.output\n",
    "    return content\n",
    "    \n",
    "    \n",
    "\n",
    "# Create the dataset with the test cases\n",
    "researcher_dataset = Dataset[ResearchQuery, ResearchResponse, Any](\n",
    "    cases=[\n",
    "        Case(\n",
    "            name='amazon_ceo_query',\n",
    "            inputs=ResearchQuery(\n",
    "                user_query=\"Who is the current CEO of Amazon?\"\n",
    "            ),\n",
    "            expected_output=None,\n",
    "            metadata={'topic': 'factual', 'complexity': 'low'},\n",
    "            evaluators=(\n",
    "                LLMJudge(\n",
    "                    rubric=\"\"\"\n",
    "                    Evaluate the research brief based on:\n",
    "                    1. Accuracy of identifying the current Amazon CEO\n",
    "                    2. Including when they took the position\n",
    "                    3. Mentioning any relevant background about the CEO\n",
    "                    4. Citation of at least 1 credible source\n",
    "                    5. Conciseness and clarity of the information\n",
    "                    \"\"\",\n",
    "                    model='bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0',\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "        Case(\n",
    "            name='harry_styles_girlfriend_query',\n",
    "            inputs=ResearchQuery(\n",
    "                user_query=\"How old is Harry Styles' girlfriend?\"\n",
    "            ),\n",
    "            expected_output=None,\n",
    "            metadata={'topic': 'celebrity', 'complexity': 'medium'},\n",
    "            evaluators=(\n",
    "                LLMJudge(\n",
    "                    rubric=\"\"\"\n",
    "                    Evaluate the research brief based on:\n",
    "                    1. Identification of Harry Styles' current relationship status\n",
    "                    2. If in a relationship, correctly identifying his girlfriend's name and age\n",
    "                    3. Including relevant context about the relationship\n",
    "                    4. Handling of potentially outdated or uncertain information appropriately\n",
    "                    5. Citation of at least 2 recent entertainment news sources\n",
    "                    6. Clarity about the time-sensitive nature of this information\n",
    "                    \"\"\",\n",
    "                    model='bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0',\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    evaluators=[\n",
    "        IsInstance(type_name='ResearchResponse'),\n",
    "        LLMJudge(\n",
    "            rubric=\"\"\"\n",
    "            Evaluate all research briefs on:\n",
    "            1. Accuracy and timeliness of the information provided\n",
    "            2. Appropriate handling of the query (factual vs. potentially changing information)\n",
    "            3. Clear and concise presentation of findings\n",
    "            4. Quality and recency of cited sources\n",
    "            5. Overall usefulness in answering the query\n",
    "            \"\"\",\n",
    "            include_input=True,\n",
    "            model='bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0',\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "report = researcher_dataset.evaluate_sync(perform_research)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we got our results back. Evals are finicky. They almost never pass 100%, but they do fail. Running them multiple times can often result in different answers by the nature of LLMs being non-determinstic. What consistitutes as passing is more on the owner of the change. One metric might go up while another goes down. \n",
    "\n",
    "Lastly, writing all your cases in code is not ideal. Let's export this dataset to a json file so it's more reusable across different platforms and approaches to evaluation. Optionally you can save the files as yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "research_dataset_path = Path('data/test_eval.json') \n",
    "researcher_dataset.to_file(research_dataset_path)\n",
    "\n",
    "# Now we can load it back in and use it in a different notebook or framework. \n",
    "print(research_dataset_path.read_text())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lets go a bit deeper and evaluate tool use. All of the evaluation / frameworks have different types. If you plan to mix and match, we suggest you write converters and store the evals in your own format. Writing your own evaluation harness is also about as difficult as it is to convert into all these types For this next part, we'll just write our own to create logical units of tool calls and measure steps to complete.\n",
    "\n",
    "First lets get Pydantics types into our own types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_platform.core.models.memory_models import Message, SessionContext\n",
    "import ragas\n",
    "\n",
    "test_runs: Dict[str, Any] = {}\n",
    "\n",
    "# Lets convert our test runner to keep the messages from each message run.\n",
    "async def perform_research_2(query: ResearchQuery) -> ResearchResponse:\n",
    "    \n",
    "    # Run the agent with MCP servers using the pattern you provided\n",
    "    async with researcher_agent.run_mcp_servers():\n",
    "        result: AgentRunResult[ResearchResponse] = researcher_agent.run_sync(query.user_query)\n",
    "    \n",
    "    content: ResearchResponse = result.output\n",
    "    test_runs[query.user_query] = result.all_messages()\n",
    "    return content\n",
    "\n",
    "# Rerun the evaluation\n",
    "report = researcher_dataset.evaluate_sync(perform_research_2)\n",
    "print(report)\n",
    "\n",
    "print('-' * 100)\n",
    "\n",
    "for k, v in test_runs.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    print('-' * 100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets convert our test runs into our format so we can run our own evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets convert the message to our types using our converters we've already build. \n",
    "from typing import List\n",
    "from agentic_platform.core.converter.pydanticai_converters import PydanticAIMessageConverter\n",
    "\n",
    "# Convert the messages to our types.\n",
    "test_runs_formatted: Dict[str, List[Message]] = {}\n",
    "for k, v in test_runs.items():\n",
    "    print(v)\n",
    "    test_runs_formatted[k] = PydanticAIMessageConverter.convert_messages(v)\n",
    "\n",
    "for k, v in test_runs_formatted.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    print('-' * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, lets write our evaluation harness. To do this we need our test cases. Steps to complete is performed by creating logical groupings of steps the agent takes to complete a task. Because they're autonomous, they might take different paths to get to the same answer. This is why logical groupings are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Callable\n",
    "from pydantic import Field\n",
    "import json\n",
    "from typing import Tuple\n",
    "from pydantic_core import to_jsonable_python\n",
    "from agentic_platform.core.models.llm_models import LLMRequest, LLMResponse\n",
    "from agentic_platform.core.models.prompt_models import BasePrompt\n",
    "from agentic_platform.core.models.tool_models import ToolSpec\n",
    "\n",
    "from agentic_platform.core.converter.llm_request_converters import ConverseRequestConverter\n",
    "from agentic_platform.core.converter.llm_response_converters import ConverseResponseConverter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import boto3\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert evaluator of agentic systems. You are given a list of steps the agent took to complete a task and a list of expected steps. \n",
    "You need to evaluate the agent's performance based on the expected steps.\n",
    "\n",
    "You should take the steps inputted and create \"logical groupings\" of those steps. Those grouping names should come from the expected steps if similar. \n",
    "If the agent took a different path, you should create a new grouping name so we can evaluate the agent's performance.\n",
    "\n",
    "When creating logical groupings, group things together. Instead of saying search the web, gather results. Group them into one step even if the message history shows them as separate steps.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "Here are the steps the agent took to complete the task:\n",
    "{steps}\n",
    "\n",
    "Here are the expected steps:\n",
    "{expected_steps}\n",
    "\n",
    "Here is the task success criteria:\n",
    "{success_criteria}\n",
    "\n",
    "Before answering, think about the steps and how the agent took them to complete the task.\n",
    "\"\"\"\n",
    "\n",
    "# Types for the evaluation.\n",
    "\n",
    "class AgentEvalPrompt(BasePrompt):\n",
    "    system_prompt: str = SYSTEM_PROMPT\n",
    "    user_prompt: str = USER_PROMPT\n",
    "\n",
    "class AgentEvalResult(BaseModel):\n",
    "    '''Evaluation results for the agent.'''\n",
    "    thoughts: str = Field(\n",
    "        description=\"The evaluators thoughts on the agents performance.\"\n",
    "    )\n",
    "    \n",
    "    steps: List[str] = Field(\n",
    "        description=\"The logical groupings of actions taken by the agent to complete the task, typically representing tool calls or major decision points.\"\n",
    "    )\n",
    "    \n",
    "    task_success: bool = Field(\n",
    "        description=\"Whether the agent successfully completed the task according to the defined success criteria. True indicates success, False indicates failure.\"\n",
    "    )\n",
    "\n",
    "class AgentEvalSample(BaseModel):\n",
    "    user_input: str\n",
    "    expected_steps: List[str]\n",
    "    expected_output: Any\n",
    "    success_criteria: Literal['Got Corect Answer', '< 1 step from gold standard']\n",
    "\n",
    "\n",
    "client = boto3.client('bedrock-runtime')\n",
    "\n",
    "# Helper function to call the LLM.\n",
    "def call_bedrock(request: LLMRequest) -> LLMResponse:\n",
    "    kwargs: Dict[str, Any] = ConverseRequestConverter.convert_llm_request(request)\n",
    "    converse_response: Dict[str, Any] = client.converse(**kwargs)\n",
    "    return ConverseResponseConverter.to_llm_response(converse_response)\n",
    "\n",
    "\n",
    "def eval_function(sample: AgentEvalSample, context: List[Message]) -> AgentEvalResult:\n",
    "    # Create the input for the prompt.\n",
    "    inputs={\n",
    "        'steps': json.dumps(to_jsonable_python(context)),\n",
    "        'expected_steps': '\\n'.join(sample.expected_steps),\n",
    "        'success_criteria': sample.success_criteria\n",
    "    }\n",
    "\n",
    "    # Format the prompt.\n",
    "    prompt: BasePrompt = AgentEvalPrompt(inputs=inputs)\n",
    "\n",
    "    # Create a tool spec for the structured output.\n",
    "    tool_spec: ToolSpec = ToolSpec(\n",
    "        name='AgentEvalResult',\n",
    "        description='Structured output for an agents performance.',\n",
    "        model=AgentEvalResult,\n",
    "    )\n",
    "\n",
    "    # Create the LLM request and forces structured output through a tool call.\n",
    "    llm_request: LLMRequest = LLMRequest(\n",
    "        system_prompt=prompt.system_prompt,\n",
    "        model_id='us.anthropic.claude-3-5-haiku-20241022-v1:0',\n",
    "        messages=[Message(role='user', text=prompt.user_prompt)],\n",
    "        hyperparams={\"temperature\": 0.2},\n",
    "        tools=[tool_spec],\n",
    "        force_tool=tool_spec.name\n",
    "    )\n",
    "\n",
    "    # Call the LLM and get results.\n",
    "    llm_response: LLMResponse = call_bedrock(llm_request)\n",
    "    return AgentEvalResult(**llm_response.tool_calls[0].arguments)\n",
    "\n",
    "def run_function(sample: AgentEvalSample) -> List[Message]:\n",
    "    result: AgentRunResult[ResearchResponse] = researcher_agent.run_sync(sample.user_input)\n",
    "    messages: List[Message] = result.all_messages()\n",
    "    return PydanticAIMessageConverter.convert_messages(messages)\n",
    "\n",
    "class AgentEvalHarness:\n",
    "\n",
    "    def __init__(self, \n",
    "                 samples: List[AgentEvalSample], \n",
    "                 eval_function: Callable[[AgentEvalSample, List[Message]], AgentEvalResult],\n",
    "                 run_function: Callable[[AgentEvalSample], List[Message]]):\n",
    "        \n",
    "        self.samples = samples\n",
    "        self.eval_function = eval_function\n",
    "        self.run_function = run_function\n",
    "\n",
    "    def evaluate_sample(self, sample: AgentEvalSample) -> Tuple[AgentEvalSample, AgentEvalResult]:\n",
    "        context: List[Message] = self.run_function(sample)\n",
    "        result: AgentEvalResult = self.eval_function(sample, context)\n",
    "        return sample, result\n",
    "    \n",
    "    \n",
    "    def evaluate_threaded(self, num_workers: int = 2) -> List[AgentEvalSample]:\n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            results: List[Tuple[AgentEvalSample, AgentEvalResult]] = list(executor.map(self.evaluate_sample, self.samples))\n",
    "        return results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    AgentEvalSample(\n",
    "        user_input='Who is the current CEO of Amazon?',\n",
    "        expected_steps=['Search the web for the current CEO of Amazon', \"Verify the information is correct\"],\n",
    "        expected_output='Andy Jassy',\n",
    "        success_criteria='Got Corect Answer'\n",
    "    ),\n",
    "    AgentEvalSample(\n",
    "        user_input='What is the population of the capital of France?',\n",
    "        expected_steps=[\n",
    "            'Search the web for the capital of France',\n",
    "            'Search the web for the population of Paris',\n",
    "            'Format results into a response'\n",
    "        ],\n",
    "        expected_output='Roughly 2.1 million',\n",
    "        success_criteria='Got Corect Answer'\n",
    "    )\n",
    "]\n",
    "\n",
    "harness = AgentEvalHarness(samples, eval_function, run_function)\n",
    "evaluation_results = harness.evaluate_threaded()\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great, but going through each result individually would be painful. We can create a summary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_summary(eval_pairs: List[Tuple[AgentEvalSample, AgentEvalResult]]) -> str:\n",
    "    total_samples = len(eval_pairs)\n",
    "    success_count = 0\n",
    "    total_step_delta = 0\n",
    "    \n",
    "    for sample, result in eval_pairs:\n",
    "        # Calculate step delta\n",
    "        expected_step_count = len(sample.expected_steps)\n",
    "        actual_step_count = len(result.steps)\n",
    "        step_delta = actual_step_count - expected_step_count\n",
    "        \n",
    "        # Update totals\n",
    "        if result.task_success:\n",
    "            success_count += 1\n",
    "        total_step_delta += step_delta\n",
    "    \n",
    "    # Calculate averages\n",
    "    success_rate = success_count / total_samples * 100\n",
    "    avg_step_delta = total_step_delta / total_samples\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = f\"\"\"\n",
    "    AGENT EVALUATION SUMMARY\n",
    "    ========================\n",
    "    Total samples evaluated: {total_samples}\n",
    "    Success rate: {success_rate:.1f}%\n",
    "    Average step delta: {avg_step_delta:.2f}\n",
    "\n",
    "    Step delta interpretation:\n",
    "    - Negative: Agent used fewer steps than expected (more efficient)\n",
    "    - Zero: Agent used exactly the expected number of steps\n",
    "    - Positive: Agent used more steps than expected (less efficient)\n",
    "    \"\"\"\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_evaluation_summary(eval_pairs=evaluation_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly, lets convert the results to a pandas dataframe so we can dive into the results a bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "def create_evaluation_dataframe(eval_pairs: List[Tuple]):\n",
    "    \"\"\"Convert evaluation pairs to a structured pandas DataFrame.\"\"\"\n",
    "    \n",
    "    data = []\n",
    "    for sample, result in eval_pairs:\n",
    "        # Calculate step delta\n",
    "        expected_step_count = len(sample.expected_steps)\n",
    "        actual_step_count = len(result.steps)\n",
    "        step_delta = actual_step_count - expected_step_count\n",
    "        \n",
    "        # Create row\n",
    "        row = {\n",
    "            'user_input': sample.user_input,\n",
    "            'expected_steps': sample.expected_steps,\n",
    "            'expected_output': sample.expected_output,\n",
    "            'success_criteria': sample.success_criteria,\n",
    "            'actual_steps': result.steps,\n",
    "            'step_count': actual_step_count,\n",
    "            'expected_step_count': expected_step_count,\n",
    "            'step_delta': step_delta,\n",
    "            'task_success': result.task_success,\n",
    "            'thoughts': result.thoughts\n",
    "        }\n",
    "        data.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Calculate summary statistics and add to dataframe attributes\n",
    "    df.attrs['total_samples'] = len(eval_pairs)\n",
    "    df.attrs['success_rate'] = df['task_success'].mean() * 100\n",
    "    df.attrs['avg_step_delta'] = df['step_delta'].mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "df = create_evaluation_dataframe(evaluation_results)\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Evaluation results for {df.attrs['total_samples']} samples:\")\n",
    "print(f\"Success rate: {df.attrs['success_rate']:.1f}%\")\n",
    "print(f\"Average step delta: {df.attrs['avg_step_delta']:.2f}\")\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nDetailed results:\")\n",
    "print(df[['user_input', 'step_delta', 'task_success']])\n",
    "\n",
    "# Output summary statistics\n",
    "print(\"\\nSummary by success status:\")\n",
    "print(df.groupby('task_success')['step_delta'].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this lab we went through the basics of agent evaluation. This is very much an open research area. For starting out, we suggest using the assertion based approach that Pydantic provides. The most important thing in evaluation is to identify what metrics to select and what make sense for your usecase. If a framework provides it, great. If not, you often times have to build your own test harness. \n",
    "\n",
    "In future labs, we'll discuss how these fit into a CI/CD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
