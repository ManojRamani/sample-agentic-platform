{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Responses in Agentic Platform\n",
    "\n",
    "## Overview\n",
    "Streaming responses send data incrementally as it becomes available, rather than waiting for the complete response. This is ideal for AI applications where we want to show progress in real-time.\n",
    "\n",
    "## Event Types\n",
    "Our platform streams these events via Server-Sent Events (SSE):\n",
    "- `text_delta`: A chunk of text content\n",
    "- `text_done`: Text content is complete\n",
    "- `thinking`: Agent's internal reasoning\n",
    "- `tool_call`: Tool being called\n",
    "- `tool_result`: Result from tool\n",
    "- `error`: Error information\n",
    "- `done`: Stream completion\n",
    "\n",
    "Each event includes a unique ID, conversation ID, timestamp, usage statistics (for LLM responses), and additional metadata.\n",
    "\n",
    "## Example Flow\n",
    "```python\n",
    "# Server streams events like:\n",
    "{\n",
    "    \"session_id\": \"123\",\n",
    "    \"type\": \"text_delta\",\n",
    "    \"text\": \"Based on my search\n",
    "}\n",
    "```\n",
    "\n",
    "## Benefits\n",
    "Streaming provides real-time response display. It's not uncommon to see double digit latency within Agentic systems. Streaming intermediate steps keeps the users engaged and lets them know whats going on which in a way, masks the latency from the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, lets take a look at our streaming types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_platform.core.models.streaming_models import ToolCallEvent, ToolResultEvent, ErrorEvent, DoneEvent, TextDeltaEvent, TextDoneEvent, ThinkingEvent\n",
    "\n",
    "\n",
    "TextDoneEvent??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextDeltaEvent??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ToolCallEvent??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the events above, you can see they all follow a similar flow. Text events have a text field, tool call and tool results have outputted results, etc.. Importantly, each event has a \"type\" attribute which makes it easier to piece together agent responses on a frontend.\n",
    "\n",
    "Next lets build a simple agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next lets create our researcher agent. \n",
    "from pydantic_ai import Agent as PyAIAgent\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "# Create a basic agent with a specialized system prompt\n",
    "agent = PyAIAgent(\n",
    "    'bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0',\n",
    "    system_prompt=\"You are a helpful assistant.\"\n",
    ")\n",
    "\n",
    "# The response will be automatically printed by the Agent class\n",
    "ABSTRACTION_QUESTION = \"Explain the concept of abstractions in programming in one paragraph.\"\n",
    "response = agent.run_sync(ABSTRACTION_QUESTION)\n",
    "response.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent above is a simple agent that doesn't do much. one thing you might have noticed in all the agents we've built up to this point is that they take a while to return. End users are often impatient and don't want to wait 10-15 seconds for a response. A way to make the latency less noticable is by streaming results back to the user. Most frameworks support streaming.\n",
    "\n",
    "In the example below we'll stream the text results back from the agent output it as it comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async with agent.run_stream(ABSTRACTION_QUESTION) as result:\n",
    "    async for message in result.stream_text(delta=True):  \n",
    "            print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using streaming, the latency is much less noticable. However, what if we want to stream intermediate actions back? \n",
    "\n",
    "To do that, we'll need to return the structured output as well. Lets add a simple tool to our agent and stream the intermediate tool results back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str) -> str:\n",
    "    '''Useful for getting the local weather'''\n",
    "    return f'The weather in {location} is Sunny and 70 degrees.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.tool_plain(get_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_core import to_jsonable_python\n",
    "\n",
    "nodes = []\n",
    "async with agent.iter('What is the weather in SF?') as result:\n",
    "    async for message in result:   \n",
    "        nodes.append(to_jsonable_python(message))\n",
    "\n",
    "for n in nodes:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we're now getting intermediate results out of our pydantic agent. However, we want to convert this into our streaming types to decouple the rest of our code from the specific framework. We've created a converter for this which we'll import below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_platform.core.converter.pydanticai_converters import PydanticAIStreamingEventConverter\n",
    "from agentic_platform.core.models.streaming_models import StreamEvent\n",
    "from typing import List\n",
    "\n",
    "for node in nodes:\n",
    "    events: List[StreamEvent] = PydanticAIStreamingEventConverter.convert_event(node, session_id='abc123')\n",
    "    if events:\n",
    "        for event in events:\n",
    "            print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. Now that we have our streaming events, it's time to bring it all together. Lets re-run our agent but this tiime lets convert into our message types and stream them back. As part of SSE, data is expected to be prefixed with \"data: [YOUR DATA HERE]\" folowed by two new lines. \n",
    "\n",
    "To stream results back, we'll use the yield. Yield allows allows for real-time streaming without buffering all the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import json\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from pydantic_core import to_jsonable_python\n",
    "from typing import AsyncGenerator\n",
    "import uuid\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Import your converter\n",
    "from agentic_platform.core.models.api_models import AgenticRequestStream\n",
    "\n",
    "class AgentRequest(BaseModel):\n",
    "    prompt: str\n",
    "    conversation_id: str = \"default\"\n",
    "\n",
    "async def generate_agent_events(request: AgenticRequestStream) -> AsyncGenerator[str, None]:\n",
    "    \"\"\"Generate Server-Sent Events from the agent stream\"\"\"\n",
    "\n",
    "    session_id: str = request.session_id if request.session_id else str(uuid.uuid4())\n",
    "\n",
    "    async with agent.iter(request.message.text) as result:\n",
    "        async for message in result:   \n",
    "            json_message = to_jsonable_python(message)\n",
    "            events: List[StreamEvent] = PydanticAIStreamingEventConverter.convert_event(json_message, session_id)\n",
    "            for event in events:\n",
    "                sse_data = f\"data: {json.dumps(event.model_dump_json(serialize_as_any=True))}\\n\\n\"\n",
    "                yield sse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for sse_data in generate_agent_events(AgenticRequestStream.from_text(text=\"What is the weather in SF?\", **{'session_id':\"abc123\"})):\n",
    "    print(sse_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-program-technical-assets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
