{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning, this notebook is a WIP and will eventually replace 2_llm_gateway. target July 14th 2025.\n",
    "\n",
    "# Agentic Platform: LLM Gateway\n",
    "\n",
    "This lab introduces the concept of an LLM Gateway. LLM Gateways let you track and throttle requests in a multi-tenant environment. Your tenancy could be by department, workload, customer organization, or even individual users on your platform.\n",
    "\n",
    "There are many options for this from open source projects, private offerings, or DIY. In this lab we'll focus on using LiteLLM as our proxy. There are many options available (Portkey, Kong, Envoy, etc..). We opted to use LiteLLM because of it's existing integrations in the ecosystem. \n",
    "\n",
    "Lets get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a call to Bedrock\n",
    "LiteLLM has two ways to interact with it:\n",
    "1. As an SDK\n",
    "2. As a proxy server\n",
    "\n",
    "LiteLLM in both cases normalizes LLM provider APIs to OpenAIs ChatCompletion format. This is not only useful for integrating with other LLM APIs, but also works with models hosted in SageMaker, locally (through Ollama), etc.. Going back to our previous labs, if we're calling the API directly, we convert into our own types regardless. This makes it easy to switch out LiteLLM for another gateway down the road if needed.\n",
    "\n",
    "We'll start with the SDK. Lets make a call to Bedrock. In the example below we'll call Bedrock but we'll use the ChatCompletion format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "from litellm import ModelResponse\n",
    "import os\n",
    "\n",
    "# Simple LiteLLM call to Bedrock\n",
    "response: ModelResponse = litellm.completion(\n",
    "    model=\"bedrock/anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello! Can you tell me a fun fact about AI?\"}\n",
    "    ],\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print('-----------------')\n",
    "\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract into our own types\n",
    "Great! We now have a standard API output format. However we want to normalize it into our own types to better future proof our system. To do that, we follow a similar pattern where we write converts into the types we've been using throughout the labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_platform.core.converter.litellm_converters import LiteLLMRequestConverter, LiteLLMResponseConverter\n",
    "from agentic_platform.core.models.llm_models import LLMRequest, LLMResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_litellm(request: LLMRequest) -> LLMResponse:\n",
    "    \"\"\"\n",
    "    Call LiteLLM directly using the SDK approach.\n",
    "    Converts our internal LLMRequest format to LiteLLM and back to LLMResponse.\n",
    "    \"\"\"\n",
    "    # Convert internal request to LiteLLM format\n",
    "    litellm_payload = LiteLLMRequestConverter.convert_llm_request(request)\n",
    "    \n",
    "    # Make the call using LiteLLM SDK\n",
    "    litellm_response = litellm.completion(**litellm_payload)\n",
    "    \n",
    "    # Convert LiteLLM response back to our internal format\n",
    "    return LiteLLMResponseConverter.to_llm_response(litellm_response.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the function\n",
    "Now let's test our `call_litellm` function with our internal types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentic_platform.core.models.memory_models import Message, TextContent\n",
    "\n",
    "# Create a test LLMRequest using our internal types\n",
    "test_request: LLMRequest = LLMRequest(\n",
    "    system_prompt=\"You are a helpful AI assistant.\",\n",
    "    messages=[\n",
    "        Message(\n",
    "            role=\"user\",\n",
    "            content=[TextContent(type=\"text\", text=\"What is the capital of France?\")]\n",
    "        )\n",
    "    ],\n",
    "    model_id=\"bedrock/anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    hyperparams={\"max_tokens\": 100}\n",
    ")\n",
    "\n",
    "# Call our function\n",
    "response: LLMResponse = call_litellm(test_request)\n",
    "\n",
    "# Display the results\n",
    "print(\"Response text:\", response.text)\n",
    "print(\"Response type:\", type(response))\n",
    "print(\"Usage:\", response.usage)\n",
    "print(\"Stop reason:\", response.stop_reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate LiteLLM With Frameworks\n",
    "Now that we have the basics down, lets see how to integrate litellm with different frameworks. Lets start with Strands which has a native LiteLLM integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent as StrandsAgent\n",
    "from strands.models.litellm import LiteLLMModel as StrandsLiteLLMModel\n",
    "from strands_tools import calculator\n",
    "\n",
    "model = StrandsLiteLLMModel(\n",
    "    model_id=\"bedrock/anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    params={\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.7,\n",
    "    }\n",
    ")\n",
    "\n",
    "agent = StrandsAgent(model=model, tools=[calculator])\n",
    "response = agent(\"What is 2+2\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate with LangGraph\n",
    "Next lets see how we can integrate litellm into the other two frameworks. Lets do LangChain / LangGraph next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_litellm import ChatLiteLLM\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool as lc_tool\n",
    "\n",
    "llm = ChatLiteLLM(model=\"bedrock/anthropic.claude-3-haiku-20240307-v1:0\", temperature=0.1)\n",
    "\n",
    "@lc_tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get weather for a location.\"\"\"\n",
    "    return f\"The weather in {location} is sunny and 75°F\"\n",
    "\n",
    "@lc_tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Calculate a mathematical expression.\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return str(result)\n",
    "    except:\n",
    "        return \"Error in calculation\"\n",
    "\n",
    "# Create the React agent - works exactly like with any other LangChain LLM\n",
    "agent = create_react_agent(llm, [get_weather, calculate])\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [(\"user\", \"What's the weather in NYC and what's 25 * 4?\")]\n",
    "})\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FEEFB3; color: #9F6000; padding: 15px; border-radius: 5px; border-left: 6px solid #9F6000; margin-bottom: 15px;\">\n",
    "<strong>⚠️ WARNING:</strong> You will need to have the platform stack deployed for these next steps and will also need to be running this on the Code Server in the bastion host. If you do not wish to deploy the stack, you can continue to the 3_agent_evaluation notebook onwards which does not require access to aws resources.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proxy Server\n",
    "So far we've been using LiteLLM locally as an SDK. While that's useful, we need to call Bedrock through the proxy server so we can get all the benefits of an LLM Gateway. For this next section we'll need to deploy litellm into our EKS cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy LiteLLM\n",
    "\n",
    "Now that we understand what our LLM Gateway needs to look like, we can start calling it from our code. Before we do that, we need to deploy the gateway and aws application load balancer controller into kubernetes. Open up the terminal window if it's not already open and run the following commands in it. \n",
    "\n",
    "First test that you can hit kubernetes. You should see a couple nodes pop up. If not, please reach out to your facilitar. \n",
    "```bash\n",
    "kubectl get nodes\n",
    "```\n",
    "\n",
    "**Note:** If your helm installs and docker builds are failing in the code server environment, you may want to ssm into the host directly and execute the commands as the ubuntu user.\n",
    "\n",
    "Next lets install all the cluster essentials (like our LB controller). make sure your in /home/ubuntu/sample-agentic-platform in your terminal. Run the following command\n",
    "\n",
    "```bash\n",
    ". ./bootstrap/eks-bootstrap.sh\n",
    "```\n",
    "Once completed run this command to see if the controller is deployed\n",
    "```bash\n",
    "kubectl get pods -n kube-system\n",
    "```\n",
    "\n",
    "You should see a bunch of pods. Two of them should be prefixed with \"lb-controller-aws-load-balancer-controller\". Great! We have our load balancer controller!\n",
    "\n",
    "Next we need to deploy our llm gateway. Run the following command in that same terminal window\n",
    "```bash\n",
    ". ./deploy/deploy-litellm.sh --build\n",
    "```\n",
    "\n",
    "After completing, you should see the llm gateway by running the command below\n",
    "```bash\n",
    "kubectl get pods\n",
    "```\n",
    "\n",
    "### Check Your Load balancer\n",
    "The load balancer controller detects that we've set an ingress rule on our llm gateway. When we deployed the gateway, kubernetes will automatically deploy the load balancer but this can take a couple minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the deployment of the platform, we configured a load balancer that points to our LLM gateway. In the free OSS version of LiteLLM, it only supports API key auth. We'll need to grab the load balancers name and grab the API key from secrets manager.\n",
    "\n",
    "The first step is to grab the load balancers name. You can find it in the console or use boto3 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from typing import List, Dict\n",
    "\n",
    "# Initialize the client\n",
    "elbv2 = boto3.client('elbv2')\n",
    "\n",
    "# List all load balancers\n",
    "load_balancers: List[Dict] = elbv2.describe_load_balancers()['LoadBalancers']\n",
    "\n",
    "# Get the load balancer name. It should be prefixed by k8s-platform\n",
    "dns_name: str = [lb['DNSName'] for lb in load_balancers if 'k8s-platform' in lb['LoadBalancerName']][0]\n",
    "dns_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get our Secret for Auth\n",
    "Now we need to get our secret containing our machine 2 machine client auth token. In the deployment script we've set up two client applications in cognito. The first one is for users and teh second one is for machine 2 machine oAuth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our Secret for Auth\n",
    "import json\n",
    "\n",
    "# Get the parameter\n",
    "response = ssm_client.get_parameter(\n",
    "    Name='/agentic-platform/config/dev',\n",
    "    WithDecryption=True\n",
    ")\n",
    "\n",
    "# Parse the JSON\n",
    "json_value = response['Parameter']['Value']\n",
    "config = json.loads(json_value)\n",
    "\n",
    "secret_arn= config['LITELLM_CONFIG_SECRET_ARN']\n",
    "\n",
    "\n",
    "secret = boto3.client('secretsmanager').get_secret_value(SecretId=secret_arn)\n",
    "secret_value: str = secret['SecretString']\n",
    "\n",
    "# Parse the secret value\n",
    "secret_value_dict: Dict = json.loads(secret_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call LiteLLM from the proxy\n",
    "Now that we have our api key & proxy deployed, we can call it from the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
